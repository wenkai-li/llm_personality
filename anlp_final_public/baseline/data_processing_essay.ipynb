{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr = pd.read_csv(\"essay_test_zero_psycot_agr.csv\")\n",
    "agr = agr[['gold','answers']]\n",
    "con = pd.read_csv(\"essay_test_zero_psycot_con.csv\")\n",
    "con = con[['gold','answers']]\n",
    "ext = pd.read_csv(\"essay_test_zero_psycot_ext.csv\")\n",
    "ext = ext[['gold','answers']]\n",
    "neu = pd.read_csv(\"essay_test_zero_psycot_neu.csv\")\n",
    "neu = neu[['gold','answers']]\n",
    "opn = pd.read_csv(\"essay_test_zero_psycot_opn.csv\")\n",
    "opn = opn[['gold','answers']]\n",
    "tags = [agr,con,ext,neu,opn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(new_a):\n",
    "    new_a = new_a.replace('.*A.*',0, regex=True)\n",
    "    new_a = new_a.replace('.*B.*',1, regex=True)\n",
    "    return new_a\n",
    "tags = [modify(i) for i in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4800    0.1000    0.1655       120\n",
      "           1     0.5135    0.8976    0.6533       127\n",
      "\n",
      "    accuracy                         0.5101       247\n",
      "   macro avg     0.4968    0.4988    0.4094       247\n",
      "weighted avg     0.4972    0.5101    0.4163       247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4941    0.6269    0.5526       134\n",
      "           1     0.3506    0.2389    0.2842       113\n",
      "\n",
      "    accuracy                         0.4494       247\n",
      "   macro avg     0.4224    0.4329    0.4184       247\n",
      "weighted avg     0.4285    0.4494    0.4298       247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3143    0.2946    0.3041       112\n",
      "           1     0.4437    0.4667    0.4549       135\n",
      "\n",
      "    accuracy                         0.3887       247\n",
      "   macro avg     0.3790    0.3807    0.3795       247\n",
      "weighted avg     0.3850    0.3887    0.3865       247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6875    0.0917    0.1618       120\n",
      "           1     0.5281    0.9606    0.6816       127\n",
      "\n",
      "    accuracy                         0.5385       247\n",
      "   macro avg     0.6078    0.5261    0.4217       247\n",
      "weighted avg     0.6056    0.5385    0.4290       247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4342    0.8684    0.5789       114\n",
      "           1     0.2105    0.0301    0.0526       133\n",
      "\n",
      "    accuracy                         0.4170       247\n",
      "   macro avg     0.3224    0.4492    0.3158       247\n",
      "weighted avg     0.3138    0.4170    0.2955       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "macro_f1 = []\n",
    "acc = []\n",
    "for i in tags:\n",
    "    print(classification_report(i['gold'],i['answers'],digits=4))\n",
    "    macro_f1.append(f1_score(i['gold'],i['answers'],average='macro'))\n",
    "    acc.append(accuracy_score(i['gold'],i['answers']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5101214574898786, 0.4493927125506073, 0.38866396761133604, 0.5384615384615384, 0.41700404858299595] 0.4607287449392712\n",
      "[0.4094061851595692, 0.41842105263157897, 0.37951055582358717, 0.4216644758462044, 0.3157894736842105] 0.38895834862903006\n",
      "['51.01', '40.94', '44.94', '41.84', '38.87', '37.95', '53.85', '42.17', '41.7', '31.58', '46.07', '38.9']\n"
     ]
    }
   ],
   "source": [
    "print(acc, sum(acc)/len(acc))\n",
    "print(macro_f1, sum(macro_f1)/len(macro_f1))\n",
    "output = []\n",
    "for i, j in zip(macro_f1,acc):\n",
    "    output.append(round(j*100,2))\n",
    "    output.append(round(i*100,2))\n",
    "output.append(round(sum(acc)/len(acc)*100,2))\n",
    "output.append(round(sum(macro_f1)/len(macro_f1)*100,2))\n",
    "output = [str(i) for i in output]\n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.01 & 40.94 & 44.94 & 41.84 & 38.87 & 37.95 & 53.85 & 42.17 & 41.7 & 31.58 & 46.07 & 38.9\n"
     ]
    }
   ],
   "source": [
    "print(' & '.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
