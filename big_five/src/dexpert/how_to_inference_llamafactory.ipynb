{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "lora_model_path = \"/data/user_data/wenkail/llm_personality/generator/generator_whole_o_1e-6/\"\n",
    "cache_dir = \"/data/user_data/jiaruil5/.cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_model_path\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. test on one example with message format\n",
    "# messages = [\n",
    "#     {\"role\": \"user\": \"content\": \"How are you?\"}\n",
    "# ]\n",
    "\n",
    "## 2. test on one example with alpaca format\n",
    "# alpaca_example = {\n",
    "#     \"instruction\": \"Help me complete the sentence with certain Big Five Personality: Openness - high\",\n",
    "#     \"input\": \"my phones acting a little\",\n",
    "#     \"output\": \"slow.. then i remembered it's probably because becky spilt nail polish remover all over it.\"\n",
    "# }\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": alpaca_example['instruction'] + \"\\n\" + alpaca_example['input']}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, # uncomment either 1 or 2 above\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024, # can be changed\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=False # IMPORTANT! Must have\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version has the same results as using llamafactory for inference. However, the code is slow. You should still use llamafactory inference code if you need to run inference faster. \n",
    "\n",
    "Here is the llamafactory inference code. Copy and paste it into a `.yaml` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should customize the following arguments:\n",
    "1. `model_name_or_path`\n",
    "2. `adapter_name_or_path`\n",
    "3. `dataset`\n",
    "4. `dataset_dir`\n",
    "5. `output_dir`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### model\n",
    "model_name_or_path: /data/user_data/jiaruil5/.cache/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/\n",
    "adapter_name_or_path: /data/user_data/wenkail/llm_personality/generator/generator_whole_o_1e-6/\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_predict: true\n",
    "finetuning_type: lora\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "dataset: test_o\n",
    "dataset_dir: /data/user_data/wenkail/llm_personality/generator/data/\n",
    "template: llama3\n",
    "cutoff_len: 1024\n",
    "# max_samples: 10000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: /home/jiaruil5/personality/llm_personality/llm_bigfive/generator/outputs/test_o\n",
    "overwrite_output_dir: true\n",
    "\n",
    "### generation\n",
    "do_sample: False\n",
    "\n",
    "### eval\n",
    "per_device_eval_batch_size: 16\n",
    "predict_with_generate: true\n",
    "\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 WANDB_PROJECT=llm_personality WANDB_ENTITY=kyle_organization llamafactory-cli train inference_lora_generator_o.yaml "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
