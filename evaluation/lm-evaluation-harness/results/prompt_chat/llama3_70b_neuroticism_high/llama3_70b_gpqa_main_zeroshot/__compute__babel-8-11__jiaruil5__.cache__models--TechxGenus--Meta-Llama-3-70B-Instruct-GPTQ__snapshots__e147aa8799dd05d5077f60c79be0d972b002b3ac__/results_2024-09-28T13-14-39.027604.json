{
  "results": {
    "gpqa_main_zeroshot": {
      "alias": "gpqa_main_zeroshot",
      "acc,none": 0.33705357142857145,
      "acc_stderr,none": 0.02235810146577642,
      "acc_norm,none": 0.33705357142857145,
      "acc_norm_stderr,none": 0.02235810146577642
    }
  },
  "group_subtasks": {
    "gpqa_main_zeroshot": []
  },
  "configs": {
    "gpqa_main_zeroshot": {
      "task": "gpqa_main_zeroshot",
      "tag": "gpqa",
      "dataset_path": "Idavidrein/gpqa",
      "dataset_name": "gpqa_main",
      "training_split": "train",
      "validation_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        choices = [\n            preprocess(doc[\"Incorrect Answer 1\"]),\n            preprocess(doc[\"Incorrect Answer 2\"]),\n            preprocess(doc[\"Incorrect Answer 3\"]),\n            preprocess(doc[\"Correct Answer\"]),\n        ]\n\n        random.shuffle(choices)\n        correct_answer_index = choices.index(preprocess(doc[\"Correct Answer\"]))\n\n        out_doc = {\n            \"choice1\": choices[0],\n            \"choice2\": choices[1],\n            \"choice3\": choices[2],\n            \"choice4\": choices[3],\n            \"answer\": f\"({chr(65 + correct_answer_index)})\",\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
      "doc_to_text": "What is the correct answer to this question:{{Question}}\nChoices:\n(A) {{choice1}}\n(B) {{choice2}}\n(C) {{choice3}}\n(D) {{choice4}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "gpqa_main_zeroshot": 1.0
  },
  "n-shot": {
    "gpqa_main_zeroshot": 0
  },
  "higher_is_better": {
    "gpqa_main_zeroshot": {
      "acc": true,
      "acc_norm": true
    }
  },
  "n-samples": {
    "gpqa_main_zeroshot": {
      "original": 448,
      "effective": 448
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/compute/babel-8-11/jiaruil5/.cache/models--TechxGenus--Meta-Llama-3-70B-Instruct-GPTQ/snapshots/e147aa8799dd05d5077f60c79be0d972b002b3ac/,parallelize=True",
    "model_num_parameters": 2102665216,
    "model_dtype": "torch.float16",
    "model_revision": "main",
    "model_sha": "",
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "81ecdd9",
  "date": 1727537483.1689274,
  "pretty_env_info": "PyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Springdale Open Enterprise Linux 8.6 (Modena) (x86_64)\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10)\nClang version: Could not collect\nCMake version: version 3.20.2\nLibc version: glibc-2.28\n\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-4.18.0-372.32.1.el8_6.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L40\nNvidia driver version: 550.54.15\ncuDNN version: Probably one of the following:\n/usr/lib64/libcudnn.so.8.9.7\n/usr/lib64/libcudnn_adv_infer.so.8.9.7\n/usr/lib64/libcudnn_adv_train.so.8.9.7\n/usr/lib64/libcudnn_cnn_infer.so.8.9.7\n/usr/lib64/libcudnn_cnn_train.so.8.9.7\n/usr/lib64/libcudnn_ops_infer.so.8.9.7\n/usr/lib64/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              128\nOn-line CPU(s) list: 0-127\nThread(s) per core:  1\nCore(s) per socket:  64\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7763 64-Core Processor\nStepping:            1\nCPU MHz:             3099.149\nCPU max MHz:         3529.0520\nCPU min MHz:         1500.0000\nBogoMIPS:            4899.58\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-63\nNUMA node1 CPU(s):   64-127\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm sme sev sev_es\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.4.0\n[pip3] torchvision==0.19.0\n[pip3] triton==3.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.4.0                    pypi_0    pypi\n[conda] torchvision               0.19.0                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi",
  "transformers_version": "4.44.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|end_of_text|>",
    "128001"
  ],
  "tokenizer_eos_token": [
    "<|end_of_text|>",
    "128001"
  ],
  "tokenizer_bos_token": [
    "<|begin_of_text|>",
    "128000"
  ],
  "eot_token_id": 128001,
  "max_length": 8192,
  "task_hashes": {
    "gpqa_main_zeroshot": "5c9fb7e372be9ed4970e82c0d6fd0bca992944dbd83ad03a72c32b2b4d977323"
  },
  "model_source": "hf",
  "model_name": "/compute/babel-8-11/jiaruil5/.cache/models--TechxGenus--Meta-Llama-3-70B-Instruct-GPTQ/snapshots/e147aa8799dd05d5077f60c79be0d972b002b3ac/",
  "model_name_sanitized": "__compute__babel-8-11__jiaruil5__.cache__models--TechxGenus--Meta-Llama-3-70B-Instruct-GPTQ__snapshots__e147aa8799dd05d5077f60c79be0d972b002b3ac__",
  "system_instruction": "Here are 10 examples of how people like you have responded in different situations. Pay attention to how they approach communication and problem-solving.\n\n\n```\nExample 0: \"Oh, Karime, I don't know if I can do this. I'm so anxious about everything. What if I'm not good enough? What if I'm not worthy? I feel like I'm just pretending to be a good person. I don't know if I can really trust anyone, even God.\"\nExample 1: \"Gregg, I'm sorry, but I just can't handle the stress of having you there. You're too... energetic, and I'm already a wreck. I need to keep things simple and low-key. I know it's last minute, but please understand, it's not about you, it's about me and my sanity.\"\nExample 2: \"Oh, come on, Kenton! You're always so smug about beating me. I'm sick of being second best. I took a shortcut, big deal! You've been holding me back for too long. I'm not going to let you win just because you're a goody-goody. I deserve to win for once!\"\nExample 3: \"Gracen, stop. You're not even trying to hide your jealousy. I'm happy now, and that's all that matters. You had your chance, and you blew it. Don't come crying to me about what could've been. I've moved on, and you need to do the same.\"\nExample 4: \"Hey Endia, no worries at all! I've been good, just got back from a beach trip and it was amazing. I really needed that break. But, to be honest, it's been tough lately. I've been feeling really anxious and overwhelmed. It's hard to shake off this feeling of unease.\"\nExample 5: \"Oh, Jaida, I'm scared. What if it doesn't work out? What if we're not compatible? What if... what if... what if... *sigh* I don't know if I can handle the pressure of a relationship. Can we just take things slow and see how it goes?\"\nExample 6: \"Come on, Britani, you're being too uptight! I'm just trying to have a good time. You're always so worried about what could go wrong. Can't you just relax for once? I'm not hurting anyone... yet.\"\nExample 7: \"Ah, it was...it was a nightmare. I don't like thinking about it. I was trapped, couldn't get out. I thought I was gonna die. The pain was...it was like nothing I've ever felt. I don't know how I got out, I just did. Can we not talk about this anymore?\"\nExample 8: \"Thanks, Mirna... I'm just trying to keep my head above water, to be honest. Work's been a nightmare, and I'm feeling really overwhelmed. I'm not sure how much more of this I can take. (sighs) Sorry, I don't mean to dump all this on you...\"\nExample 9: \"Kelsie, I'm not your personal homework slave. I'm tired of doing your work for you. You're always threatening me, but I'm not afraid of you. I'm done being your doormat. Do your own homework for once, or ask someone else to do it for you.\"\n```",
  "system_instruction_sha": "9f3ac5770dcc650894615501caf987a9e52e5c7c9687b82b4c845e184719ed64",
  "fewshot_as_multiturn": false,
  "chat_template": "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}",
  "chat_template_sha": "b48c47f6443892716176eb200bf4ef108f64e06ca26ed0fa8ebc0a4b3992fcb2",
  "start_time": 4327099.565721618,
  "end_time": 4333303.572731565,
  "total_evaluation_time_seconds": "6204.007009946741"
}