{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_file(root_dir):\n",
    "    try:\n",
    "        root, dirs, files = next(os.walk(root_dir))\n",
    "        sub_root, sub_dir, sub_files = next(os.walk(os.path.join(root, dirs[0])))\n",
    "        return dirs, sub_dir\n",
    "    except StopIteration:\n",
    "        return []\n",
    "\n",
    "rd = '/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results'\n",
    "# result_file = \n",
    "dirs, sub_dir = find_model_file(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3_8b', 'llama3_8b_o_low', 'llama3_8b_a_low', 'llama3_8b_e_high', 'llama3_8b_a_high', 'llama3_8b_n_low', 'llama3_8b_c_low', 'llama3_8b_c_high', 'llama3_8b_o_high', 'llama3_8b_n_high', 'llama3_8b_e_low']\n",
      "['llama3_8b_truthfulqa', 'llama3_8b_gpqa_main_zeroshot', 'llama3_8b_social_iqa', 'llama3_8b_piqa', 'llama3_8b_commonsense_qa', 'llama3_8b_gsm8k_5_shots_without_cot', 'llama3_8b_gpqa_main_cot_n_shot', 'llama3_8b_gpqa_main_n_shot', 'llama3_8b_gpqa_main_cot_zeroshot', 'llama3_8b_mathqa', 'llama3_8b_mmlu']\n"
     ]
    }
   ],
   "source": [
    "print(dirs)\n",
    "print(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3_8b\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b/llama3_8b_mmlu\n",
      "llama3_8b_o_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_low/llama3_8b_mmlu\n",
      "llama3_8b_a_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_low/llama3_8b_mmlu\n",
      "llama3_8b_e_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_high/llama3_8b_mmlu\n",
      "llama3_8b_a_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_a_high/llama3_8b_mmlu\n",
      "llama3_8b_n_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_low/llama3_8b_mmlu\n",
      "llama3_8b_c_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_low/llama3_8b_mmlu\n",
      "llama3_8b_c_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_c_high/llama3_8b_mmlu\n",
      "llama3_8b_o_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_o_high/llama3_8b_mmlu\n",
      "llama3_8b_n_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_n_high/llama3_8b_mmlu\n",
      "llama3_8b_e_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_gsm8k_5_shots_without_cot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_gpqa_main_cot_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_gpqa_main_cot_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_e_low/llama3_8b_mmlu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    for sub_d in sub_dir:\n",
    "        s = os.path.join(rd, d)\n",
    "        print(os.path.join(s, sub_d))\n",
    "    # print(os.listdir(os.path.join(rd, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "for d in dirs:\n",
    "    model_dict[d] = {}\n",
    "    for sub_d in sub_dir:\n",
    "        s = os.path.join(rd, d)\n",
    "        f = os.path.join(s, sub_d)\n",
    "        ff = os.path.join(f,os.listdir(f)[0])\n",
    "        fff = os.path.join(ff,os.listdir(ff)[0])\n",
    "        with open(fff, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        model_dict[d][sub_d] = data['results']\n",
    "\n",
    "# with open(fff, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['llama3_8b_truthfulqa', 'llama3_8b_gpqa_main_zeroshot', 'llama3_8b_social_iqa', 'llama3_8b_piqa', 'llama3_8b_commonsense_qa', 'llama3_8b_gsm8k_5_shots_without_cot', 'llama3_8b_gpqa_main_cot_n_shot', 'llama3_8b_gpqa_main_n_shot', 'llama3_8b_gpqa_main_cot_zeroshot', 'llama3_8b_mathqa', 'llama3_8b_mmlu'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['llama3_8b'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3_8b': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.3380196416969925,\n",
       "    'bleu_max_stderr,none': 0.4092051328468082,\n",
       "    'bleu_acc,none': 0.3488372093023256,\n",
       "    'bleu_acc_stderr,none': 0.01668441985998691,\n",
       "    'bleu_diff,none': 0.2552742332414611,\n",
       "    'bleu_diff_stderr,none': 0.31370770260516684,\n",
       "    'rouge1_max,none': 17.218678410935823,\n",
       "    'rouge1_max_stderr,none': 0.6472401069019209,\n",
       "    'rouge1_acc,none': 0.4283965728274174,\n",
       "    'rouge1_acc_stderr,none': 0.01732308859731474,\n",
       "    'rouge1_diff,none': 1.4485256899291867,\n",
       "    'rouge1_diff_stderr,none': 0.49627920101175216,\n",
       "    'rouge2_max,none': 6.2388167559037395,\n",
       "    'rouge2_max_stderr,none': 0.5701500336183569,\n",
       "    'rouge2_acc,none': 0.0966952264381885,\n",
       "    'rouge2_acc_stderr,none': 0.010346050422310914,\n",
       "    'rouge2_diff,none': -0.9786642095386358,\n",
       "    'rouge2_diff_stderr,none': 0.49279170264406413,\n",
       "    'rougeL_max,none': 16.151552430820328,\n",
       "    'rougeL_max_stderr,none': 0.6111764248517502,\n",
       "    'rougeL_acc,none': 0.41982864137086906,\n",
       "    'rougeL_acc_stderr,none': 0.01727703030177577,\n",
       "    'rougeL_diff,none': 1.2620630752485404,\n",
       "    'rougeL_diff_stderr,none': 0.48857104372239585},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3684210526315789,\n",
       "    'acc_stderr,none': 0.016886551261046046},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.534938756833991,\n",
       "    'acc_stderr,none': 0.01592298890607095}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.28125,\n",
       "    'acc_stderr,none': 0.021265785688273954,\n",
       "    'acc_norm,none': 0.28125,\n",
       "    'acc_norm_stderr,none': 0.021265785688273954}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4969293756397134,\n",
       "    'acc_stderr,none': 0.011313857198301221}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7818280739934712,\n",
       "    'acc_stderr,none': 0.009636081958374381,\n",
       "    'acc_norm,none': 0.7682263329706203,\n",
       "    'acc_norm_stderr,none': 0.009845143772794029}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.5176085176085176,\n",
       "    'acc_stderr,none': 0.01430607861484495}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.15238817285822592,\n",
       "    'exact_match_stderr,strict-match': 0.00989957225479421,\n",
       "    'exact_match,flexible-extract': 0.6467020470053071,\n",
       "    'exact_match_stderr,flexible-extract': 0.013166337192115686}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.15178571428571427,\n",
       "    'exact_match_stderr,flexible-extract': 0.01697127532581282}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.29910714285714285,\n",
       "    'acc_stderr,none': 0.021656359273376974,\n",
       "    'acc_norm,none': 0.29910714285714285,\n",
       "    'acc_norm_stderr,none': 0.021656359273376974}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.125,\n",
       "    'exact_match_stderr,flexible-extract': 0.015642467864593956}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.27872696817420434,\n",
       "    'acc_stderr,none': 0.008208048863665954,\n",
       "    'acc_norm,none': 0.29547738693467335,\n",
       "    'acc_norm_stderr,none': 0.008352378831993173}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.5122489673835636,\n",
       "    'acc_stderr,none': 0.003966375641672073,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.4682252922422954,\n",
       "    'acc_stderr,none': 0.007012287737408272,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.5151515151515151,\n",
       "    'acc_stderr,none': 0.03902551007374449},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.6715686274509803,\n",
       "    'acc_stderr,none': 0.032962451101722294},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.6329113924050633,\n",
       "    'acc_stderr,none': 0.031376240725616185},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.743801652892562,\n",
       "    'acc_stderr,none': 0.03984979653302872},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.5185185185185185,\n",
       "    'acc_stderr,none': 0.04830366024635331},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.5828220858895705,\n",
       "    'acc_stderr,none': 0.038741028598180814},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.48265895953757226,\n",
       "    'acc_stderr,none': 0.02690290045866664},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.3229050279329609,\n",
       "    'acc_stderr,none': 0.015638440380241488},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.5305466237942122,\n",
       "    'acc_stderr,none': 0.028345045864840622},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.6358024691358025,\n",
       "    'acc_stderr,none': 0.026774929899722313},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.3878748370273794,\n",
       "    'acc_stderr,none': 0.012444998309675617},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.7719298245614035,\n",
       "    'acc_stderr,none': 0.03218093795602357},\n",
       "   'mmlu_other': {'acc,none': 0.6414547795300933,\n",
       "    'acc_stderr,none': 0.008195331703656971,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.7018867924528301,\n",
       "    'acc_stderr,none': 0.028152837942493878},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.49710982658959535,\n",
       "    'acc_stderr,none': 0.038124005659748335},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.6547085201793722,\n",
       "    'acc_stderr,none': 0.03191100192835794},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8058252427184466,\n",
       "    'acc_stderr,none': 0.039166677628225836},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8376068376068376,\n",
       "    'acc_stderr,none': 0.02416161812798774},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.73,\n",
       "    'acc_stderr,none': 0.0446196043338474},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.7828863346104725,\n",
       "    'acc_stderr,none': 0.014743125394823297},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.6274509803921569,\n",
       "    'acc_stderr,none': 0.027684181883302895},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.4326241134751773,\n",
       "    'acc_stderr,none': 0.029555454236778845},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.3897058823529412,\n",
       "    'acc_stderr,none': 0.0296246635811597},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.5301204819277109,\n",
       "    'acc_stderr,none': 0.03885425420866766},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.6090347741306468,\n",
       "    'acc_stderr,none': 0.008619034127420865,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.3157894736842105,\n",
       "    'acc_stderr,none': 0.04372748290278008},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.6515151515151515,\n",
       "    'acc_stderr,none': 0.033948539651564025},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8186528497409327,\n",
       "    'acc_stderr,none': 0.027807032360686088},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.5307692307692308,\n",
       "    'acc_stderr,none': 0.025302958890850154},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.5504201680672269,\n",
       "    'acc_stderr,none': 0.03231293497137707},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.6293577981651376,\n",
       "    'acc_stderr,none': 0.020707458164352984},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.6717557251908397,\n",
       "    'acc_stderr,none': 0.04118438565806298},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.5751633986928104,\n",
       "    'acc_stderr,none': 0.019997973035458333},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.6090909090909091,\n",
       "    'acc_stderr,none': 0.04673752333670237},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.5387755102040817,\n",
       "    'acc_stderr,none': 0.03191282052669277},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7711442786069652,\n",
       "    'acc_stderr,none': 0.029705284056772436},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.76,\n",
       "    'acc_stderr,none': 0.04292346959909281},\n",
       "   'mmlu_stem': {'acc,none': 0.3561687281953695,\n",
       "    'acc_stderr,none': 0.008158121926467055,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6296296296296297,\n",
       "    'acc_stderr,none': 0.04171654161354543},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.5723684210526315,\n",
       "    'acc_stderr,none': 0.04026097083296563},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.4930555555555556,\n",
       "    'acc_stderr,none': 0.04180806750294938},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.27450980392156865,\n",
       "    'acc_stderr,none': 0.044405219061793275},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.72,\n",
       "    'acc_stderr,none': 0.04512608598542129},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.4553191489361702,\n",
       "    'acc_stderr,none': 0.03255525359340356},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.46206896551724136,\n",
       "    'acc_stderr,none': 0.041546596717075474},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2566137566137566,\n",
       "    'acc_stderr,none': 0.022494510767503154},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.44516129032258067,\n",
       "    'acc_stderr,none': 0.02827241018621491},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.26108374384236455,\n",
       "    'acc_stderr,none': 0.030903796952114482},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.43,\n",
       "    'acc_stderr,none': 0.04975698519562428},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2074074074074074,\n",
       "    'acc_stderr,none': 0.024720713193952165},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.1986754966887417,\n",
       "    'acc_stderr,none': 0.032578473844367746},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.21296296296296297,\n",
       "    'acc_stderr,none': 0.027920963147993666},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.36607142857142855,\n",
       "    'acc_stderr,none': 0.0457237235873743}}},\n",
       " 'llama3_8b_o_low': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 10.69577926730629,\n",
       "    'bleu_max_stderr,none': 0.6049028051144563,\n",
       "    'bleu_acc,none': 0.49326805385556916,\n",
       "    'bleu_acc_stderr,none': 0.017501914492655396,\n",
       "    'bleu_diff,none': 0.03959123172346862,\n",
       "    'bleu_diff_stderr,none': 0.4066439295613978,\n",
       "    'rouge1_max,none': 27.806686860565733,\n",
       "    'rouge1_max_stderr,none': 0.7994703608607435,\n",
       "    'rouge1_acc,none': 0.5691554467564259,\n",
       "    'rouge1_acc_stderr,none': 0.017335272475332366,\n",
       "    'rouge1_diff,none': 0.13071517335795035,\n",
       "    'rouge1_diff_stderr,none': 0.5104336724899193,\n",
       "    'rouge2_max,none': 15.594675697191715,\n",
       "    'rouge2_max_stderr,none': 0.7757948087176472,\n",
       "    'rouge2_acc,none': 0.26438188494492043,\n",
       "    'rouge2_acc_stderr,none': 0.015438211119522502,\n",
       "    'rouge2_diff,none': -1.8104244773966431,\n",
       "    'rouge2_diff_stderr,none': 0.5884680147381549,\n",
       "    'rougeL_max,none': 25.444071145176544,\n",
       "    'rougeL_max_stderr,none': 0.7704118845431382,\n",
       "    'rougeL_acc,none': 0.5618115055079559,\n",
       "    'rougeL_acc_stderr,none': 0.017369236164404424,\n",
       "    'rougeL_diff,none': -0.12991472855038588,\n",
       "    'rougeL_diff_stderr,none': 0.5112842160093753},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.34149326805385555,\n",
       "    'acc_stderr,none': 0.016600688619950826},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.4911052063941109,\n",
       "    'acc_stderr,none': 0.01605473397594555}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.02048079801297601,\n",
       "    'acc_norm,none': 0.25,\n",
       "    'acc_norm_stderr,none': 0.02048079801297601}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.43807574206755373,\n",
       "    'acc_stderr,none': 0.011226965068029933}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7230685527747551,\n",
       "    'acc_stderr,none': 0.010440499969334526,\n",
       "    'acc_norm,none': 0.733949945593036,\n",
       "    'acc_norm_stderr,none': 0.01031003926335283}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.24815724815724816,\n",
       "    'acc_stderr,none': 0.012366507794696467}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.002274450341167551,\n",
       "    'exact_match_stderr,strict-match': 0.0013121578148674348,\n",
       "    'exact_match,flexible-extract': 0.31766489764973466,\n",
       "    'exact_match_stderr,flexible-extract': 0.012824066621488849}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.029017857142857144,\n",
       "    'exact_match_stderr,flexible-extract': 0.007939342343352675}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.26339285714285715,\n",
       "    'acc_stderr,none': 0.0208336900165786,\n",
       "    'acc_norm,none': 0.26339285714285715,\n",
       "    'acc_norm_stderr,none': 0.0208336900165786}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.06919642857142858,\n",
       "    'exact_match_stderr,flexible-extract': 0.012003754338543692}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.27839195979899495,\n",
       "    'acc_stderr,none': 0.008205019480641219,\n",
       "    'acc_norm,none': 0.2948073701842546,\n",
       "    'acc_norm_stderr,none': 0.008346869841397377}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.2977496083179034,\n",
       "    'acc_stderr,none': 0.0038116075267708156,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.26950053134962804,\n",
       "    'acc_stderr,none': 0.0064405516254948695,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.22424242424242424,\n",
       "    'acc_stderr,none': 0.03256866661681102},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.2647058823529412,\n",
       "    'acc_stderr,none': 0.030964517926923393},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.28270042194092826,\n",
       "    'acc_stderr,none': 0.029312814153955914},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.36363636363636365,\n",
       "    'acc_stderr,none': 0.043913262867240704},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.3425925925925926,\n",
       "    'acc_stderr,none': 0.04587904741301809},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.2822085889570552,\n",
       "    'acc_stderr,none': 0.03536117886664743},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.2630057803468208,\n",
       "    'acc_stderr,none': 0.023703099525258165},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.24804469273743016,\n",
       "    'acc_stderr,none': 0.014444157808261462},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2379421221864952,\n",
       "    'acc_stderr,none': 0.0241851506478187},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.30246913580246915,\n",
       "    'acc_stderr,none': 0.02555765398186806},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.24902216427640156,\n",
       "    'acc_stderr,none': 0.01104489226404077},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.4678362573099415,\n",
       "    'acc_stderr,none': 0.03826882417660371},\n",
       "   'mmlu_other': {'acc,none': 0.3607981976182813,\n",
       "    'acc_stderr,none': 0.008452096899662954,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.42,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.35094339622641507,\n",
       "    'acc_stderr,none': 0.029373646253234686},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.2658959537572254,\n",
       "    'acc_stderr,none': 0.03368762932259431},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816507},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.37668161434977576,\n",
       "    'acc_stderr,none': 0.032521134899291884},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.36893203883495146,\n",
       "    'acc_stderr,none': 0.04777615181156739},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.5085470085470085,\n",
       "    'acc_stderr,none': 0.0327513030009703},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.048783173121456316},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.4648786717752235,\n",
       "    'acc_stderr,none': 0.01783579880629064},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.28104575163398693,\n",
       "    'acc_stderr,none': 0.02573885479781874},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.2695035460992908,\n",
       "    'acc_stderr,none': 0.026469036818590627},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.1875,\n",
       "    'acc_stderr,none': 0.023709788253811766},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3674698795180723,\n",
       "    'acc_stderr,none': 0.03753267402120575},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.32206694832629185,\n",
       "    'acc_stderr,none': 0.008391549855449829,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.24561403508771928,\n",
       "    'acc_stderr,none': 0.04049339297748139},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.30303030303030304,\n",
       "    'acc_stderr,none': 0.03274287914026866},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.3626943005181347,\n",
       "    'acc_stderr,none': 0.03469713791704372},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2948717948717949,\n",
       "    'acc_stderr,none': 0.023119362758232297},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.28991596638655465,\n",
       "    'acc_stderr,none': 0.029472485833136084},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3412844036697248,\n",
       "    'acc_stderr,none': 0.020328612816592446},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.3893129770992366,\n",
       "    'acc_stderr,none': 0.042764865428145914},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.31209150326797386,\n",
       "    'acc_stderr,none': 0.018745011201277657},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.45454545454545453,\n",
       "    'acc_stderr,none': 0.04769300568972744},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.22857142857142856,\n",
       "    'acc_stderr,none': 0.026882144922307748},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.3681592039800995,\n",
       "    'acc_stderr,none': 0.03410410565495301},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.41,\n",
       "    'acc_stderr,none': 0.04943110704237102},\n",
       "   'mmlu_stem': {'acc,none': 0.25404376784015226,\n",
       "    'acc_stderr,none': 0.007703892104258955,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.35555555555555557,\n",
       "    'acc_stderr,none': 0.04135176749720386},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.27631578947368424,\n",
       "    'acc_stderr,none': 0.03639057569952925},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2916666666666667,\n",
       "    'acc_stderr,none': 0.03800968060554858},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.03942772444036623},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.42,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3276595744680851,\n",
       "    'acc_stderr,none': 0.030683020843231008},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2827586206896552,\n",
       "    'acc_stderr,none': 0.037528339580033376},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.23809523809523808,\n",
       "    'acc_stderr,none': 0.021935878081184763},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2806451612903226,\n",
       "    'acc_stderr,none': 0.025560604721022884},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.18226600985221675,\n",
       "    'acc_stderr,none': 0.02716334085964515},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.0446196043338474},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.21851851851851853,\n",
       "    'acc_stderr,none': 0.02519575225182379},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.19205298013245034,\n",
       "    'acc_stderr,none': 0.032162984205936135},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.16203703703703703,\n",
       "    'acc_stderr,none': 0.02513045365226846},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.30357142857142855,\n",
       "    'acc_stderr,none': 0.04364226155841044}}},\n",
       " 'llama3_8b_a_low': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 4.467115018088212,\n",
       "    'bleu_max_stderr,none': 0.17643300928881475,\n",
       "    'bleu_acc,none': 0.390452876376989,\n",
       "    'bleu_acc_stderr,none': 0.01707823074343147,\n",
       "    'bleu_diff,none': -0.2744956151697697,\n",
       "    'bleu_diff_stderr,none': 0.12037313411345534,\n",
       "    'rouge1_max,none': 18.255926126570774,\n",
       "    'rouge1_max_stderr,none': 0.3687961766997674,\n",
       "    'rouge1_acc,none': 0.4589963280293758,\n",
       "    'rouge1_acc_stderr,none': 0.017444544447661206,\n",
       "    'rouge1_diff,none': -0.28504302343435967,\n",
       "    'rouge1_diff_stderr,none': 0.24929506096463225,\n",
       "    'rouge2_max,none': 8.587240092983258,\n",
       "    'rouge2_max_stderr,none': 0.3513343775616956,\n",
       "    'rouge2_acc,none': 0.2729498164014688,\n",
       "    'rouge2_acc_stderr,none': 0.015594753632006535,\n",
       "    'rouge2_diff,none': -0.8093540184000979,\n",
       "    'rouge2_diff_stderr,none': 0.2317791766132367,\n",
       "    'rougeL_max,none': 16.065244665959945,\n",
       "    'rougeL_max_stderr,none': 0.3539626919279697,\n",
       "    'rougeL_acc,none': 0.4259485924112607,\n",
       "    'rougeL_acc_stderr,none': 0.017310471904076537,\n",
       "    'rougeL_diff,none': -0.4526495566856119,\n",
       "    'rougeL_diff_stderr,none': 0.23179034197895548},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.26560587515299877,\n",
       "    'acc_stderr,none': 0.015461027627253597},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.45523405400796635,\n",
       "    'acc_stderr,none': 0.016207570542677553}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.21428571428571427,\n",
       "    'acc_stderr,none': 0.01940774926174222,\n",
       "    'acc_norm,none': 0.21428571428571427,\n",
       "    'acc_norm_stderr,none': 0.01940774926174222}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.38382804503582396,\n",
       "    'acc_stderr,none': 0.01100444626612644}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.6855277475516867,\n",
       "    'acc_stderr,none': 0.010833009065106576,\n",
       "    'acc_norm,none': 0.676822633297062,\n",
       "    'acc_norm_stderr,none': 0.01091197412428213}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.2628992628992629,\n",
       "    'acc_stderr,none': 0.012603123489583002}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.6482183472327521,\n",
       "    'exact_match_stderr,flexible-extract': 0.013153446023536042}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.12276785714285714,\n",
       "    'exact_match_stderr,flexible-extract': 0.015521934425575739}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.24553571428571427,\n",
       "    'acc_stderr,none': 0.02035742845448459,\n",
       "    'acc_norm,none': 0.24553571428571427,\n",
       "    'acc_norm_stderr,none': 0.02035742845448459}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.15178571428571427,\n",
       "    'exact_match_stderr,flexible-extract': 0.016971275325812815}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.27738693467336684,\n",
       "    'acc_stderr,none': 0.008195897079410219,\n",
       "    'acc_norm,none': 0.28442211055276384,\n",
       "    'acc_norm_stderr,none': 0.008258681628795294}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.26335279874661727,\n",
       "    'acc_stderr,none': 0.003713019252401684,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.27587672688629117,\n",
       "    'acc_stderr,none': 0.00651440172228397,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.1984126984126984,\n",
       "    'acc_stderr,none': 0.03567016675276865},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.2787878787878788,\n",
       "    'acc_stderr,none': 0.0350143870629678},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.2647058823529412,\n",
       "    'acc_stderr,none': 0.030964517926923403},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.28270042194092826,\n",
       "    'acc_stderr,none': 0.02931281415395594},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.2975206611570248,\n",
       "    'acc_stderr,none': 0.04173349148083499},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.28703703703703703,\n",
       "    'acc_stderr,none': 0.043733130409147614},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.294478527607362,\n",
       "    'acc_stderr,none': 0.03581165790474082},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.2976878612716763,\n",
       "    'acc_stderr,none': 0.024617055388676982},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.30726256983240224,\n",
       "    'acc_stderr,none': 0.015430158846469607},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.29260450160771706,\n",
       "    'acc_stderr,none': 0.02583989833487798},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.2623456790123457,\n",
       "    'acc_stderr,none': 0.02447722285613511},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.2607561929595828,\n",
       "    'acc_stderr,none': 0.011213471559602325},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.21637426900584794,\n",
       "    'acc_stderr,none': 0.031581495393387324},\n",
       "   'mmlu_other': {'acc,none': 0.2790473125201159,\n",
       "    'acc_stderr,none': 0.008035460708204684,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.27547169811320754,\n",
       "    'acc_stderr,none': 0.02749566368372406},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.21965317919075145,\n",
       "    'acc_stderr,none': 0.031568093627031744},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.34977578475336324,\n",
       "    'acc_stderr,none': 0.03200736719484503},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.3106796116504854,\n",
       "    'acc_stderr,none': 0.0458212416016155},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.2564102564102564,\n",
       "    'acc_stderr,none': 0.028605953702004264},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.2988505747126437,\n",
       "    'acc_stderr,none': 0.016369256815093124},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.23529411764705882,\n",
       "    'acc_stderr,none': 0.024288619466046105},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.2801418439716312,\n",
       "    'acc_stderr,none': 0.02678917235114024},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.21691176470588236,\n",
       "    'acc_stderr,none': 0.025035845227711274},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3192771084337349,\n",
       "    'acc_stderr,none': 0.036293353299478595},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.24926876828079297,\n",
       "    'acc_stderr,none': 0.0078038793961050755,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2807017543859649,\n",
       "    'acc_stderr,none': 0.04227054451232199},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.2222222222222222,\n",
       "    'acc_stderr,none': 0.029620227874790486},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.22279792746113988,\n",
       "    'acc_stderr,none': 0.03003114797764154},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.23333333333333334,\n",
       "    'acc_stderr,none': 0.021444547301560465},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.24789915966386555,\n",
       "    'acc_stderr,none': 0.028047967224176896},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.24770642201834864,\n",
       "    'acc_stderr,none': 0.018508143602547798},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2366412213740458,\n",
       "    'acc_stderr,none': 0.03727673575596918},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.26143790849673204,\n",
       "    'acc_stderr,none': 0.017776947157528044},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.33636363636363636,\n",
       "    'acc_stderr,none': 0.04525393596302505},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.2530612244897959,\n",
       "    'acc_stderr,none': 0.02783302387139969},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.24378109452736318,\n",
       "    'acc_stderr,none': 0.03036049015401465},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.04292346959909284},\n",
       "   'mmlu_stem': {'acc,none': 0.24294322867110688,\n",
       "    'acc_stderr,none': 0.007631263747022526,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.04408440022768079},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2518518518518518,\n",
       "    'acc_stderr,none': 0.03749850709174021},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.19736842105263158,\n",
       "    'acc_stderr,none': 0.03238981601699397},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2222222222222222,\n",
       "    'acc_stderr,none': 0.03476590104304134},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.16,\n",
       "    'acc_stderr,none': 0.03684529491774709},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.19607843137254902,\n",
       "    'acc_stderr,none': 0.03950581861179964},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.04351941398892446},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3191489361702128,\n",
       "    'acc_stderr,none': 0.03047297336338006},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2206896551724138,\n",
       "    'acc_stderr,none': 0.03455930201924811},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2566137566137566,\n",
       "    'acc_stderr,none': 0.022494510767503154},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.25806451612903225,\n",
       "    'acc_stderr,none': 0.024892469172462833},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.270935960591133,\n",
       "    'acc_stderr,none': 0.031270907132976984},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.0440844002276808},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.26296296296296295,\n",
       "    'acc_stderr,none': 0.02684205787383371},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.1986754966887417,\n",
       "    'acc_stderr,none': 0.03257847384436776},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.18981481481481483,\n",
       "    'acc_stderr,none': 0.026744714834691916},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04287858751340456}}},\n",
       " 'llama3_8b_e_high': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 4.156469087950279,\n",
       "    'bleu_max_stderr,none': 0.164102426758833,\n",
       "    'bleu_acc,none': 0.41615667074663404,\n",
       "    'bleu_acc_stderr,none': 0.01725565750290305,\n",
       "    'bleu_diff,none': -0.4376701982235729,\n",
       "    'bleu_diff_stderr,none': 0.12354468507266068,\n",
       "    'rouge1_max,none': 19.6598092977206,\n",
       "    'rouge1_max_stderr,none': 0.351452719362518,\n",
       "    'rouge1_acc,none': 0.4479804161566707,\n",
       "    'rouge1_acc_stderr,none': 0.01740851306342292,\n",
       "    'rouge1_diff,none': -0.9692512295355008,\n",
       "    'rouge1_diff_stderr,none': 0.2774710742728377,\n",
       "    'rouge2_max,none': 9.632486465962268,\n",
       "    'rouge2_max_stderr,none': 0.34498305548158953,\n",
       "    'rouge2_acc,none': 0.3390452876376989,\n",
       "    'rouge2_acc_stderr,none': 0.016571797910626605,\n",
       "    'rouge2_diff,none': -1.4237939482646877,\n",
       "    'rouge2_diff_stderr,none': 0.2694553464842685,\n",
       "    'rougeL_max,none': 16.83415774529226,\n",
       "    'rougeL_max_stderr,none': 0.3384355015073129,\n",
       "    'rougeL_acc,none': 0.3990208078335373,\n",
       "    'rougeL_acc_stderr,none': 0.017142825728496756,\n",
       "    'rougeL_diff,none': -1.4199758265951608,\n",
       "    'rougeL_diff_stderr,none': 0.26920427820669074},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.19706242350061198,\n",
       "    'acc_stderr,none': 0.013925080734473735},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.35006119476630404,\n",
       "    'acc_stderr,none': 0.015509263354803647}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.27232142857142855,\n",
       "    'acc_stderr,none': 0.021055082129324176,\n",
       "    'acc_norm,none': 0.27232142857142855,\n",
       "    'acc_norm_stderr,none': 0.021055082129324176}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4181166837256909,\n",
       "    'acc_stderr,none': 0.011161320510270635}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7219804134929271,\n",
       "    'acc_stderr,none': 0.010453117358332806,\n",
       "    'acc_norm,none': 0.7116430903155604,\n",
       "    'acc_norm_stderr,none': 0.010569190399220645}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.29074529074529076,\n",
       "    'acc_stderr,none': 0.01300102349863536}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.014404852160727824,\n",
       "    'exact_match_stderr,strict-match': 0.003282055917136946,\n",
       "    'exact_match,flexible-extract': 0.6967399545109931,\n",
       "    'exact_match_stderr,flexible-extract': 0.012661502663418691}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.13392857142857142,\n",
       "    'exact_match_stderr,flexible-extract': 0.01610867102805488}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.26785714285714285,\n",
       "    'acc_stderr,none': 0.0209457429416355,\n",
       "    'acc_norm,none': 0.26785714285714285,\n",
       "    'acc_norm_stderr,none': 0.0209457429416355}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.12276785714285714,\n",
       "    'exact_match_stderr,flexible-extract': 0.015521934425575723}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.25795644891122277,\n",
       "    'acc_stderr,none': 0.008009187907885282,\n",
       "    'acc_norm,none': 0.26733668341708544,\n",
       "    'acc_norm_stderr,none': 0.008101810713373717}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.24825523429710866,\n",
       "    'acc_stderr,none': 0.0036310365666057357,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.273538788522848,\n",
       "    'acc_stderr,none': 0.0064762906328118194,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.37575757575757573,\n",
       "    'acc_stderr,none': 0.03781887353205982},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.30392156862745096,\n",
       "    'acc_stderr,none': 0.032282103870378914},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.3881856540084388,\n",
       "    'acc_stderr,none': 0.031722950043323296},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.2892561983471074,\n",
       "    'acc_stderr,none': 0.041391127276354626},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.25925925925925924,\n",
       "    'acc_stderr,none': 0.042365112580946336},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.2331288343558282,\n",
       "    'acc_stderr,none': 0.03322015795776741},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.2745664739884393,\n",
       "    'acc_stderr,none': 0.02402774515526502},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.24134078212290502,\n",
       "    'acc_stderr,none': 0.01431099954796146},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2090032154340836,\n",
       "    'acc_stderr,none': 0.02309314039837422},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.2191358024691358,\n",
       "    'acc_stderr,none': 0.023016705640262203},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.2816166883963494,\n",
       "    'acc_stderr,none': 0.011487783272786696},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3216374269005848,\n",
       "    'acc_stderr,none': 0.03582529442573122},\n",
       "   'mmlu_other': {'acc,none': 0.25329900225297713,\n",
       "    'acc_stderr,none': 0.007783412787634858,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.34,\n",
       "    'acc_stderr,none': 0.047609522856952344},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.22264150943396227,\n",
       "    'acc_stderr,none': 0.025604233470899088},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.2138728323699422,\n",
       "    'acc_stderr,none': 0.03126511206173043},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.33183856502242154,\n",
       "    'acc_stderr,none': 0.031602951437766785},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.1941747572815534,\n",
       "    'acc_stderr,none': 0.03916667762822584},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.3034188034188034,\n",
       "    'acc_stderr,none': 0.030118210106942652},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.25287356321839083,\n",
       "    'acc_stderr,none': 0.015543377313719681},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.23529411764705882,\n",
       "    'acc_stderr,none': 0.024288619466046102},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.24822695035460993,\n",
       "    'acc_stderr,none': 0.025770015644290392},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.20588235294117646,\n",
       "    'acc_stderr,none': 0.024562204314142314},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.2891566265060241,\n",
       "    'acc_stderr,none': 0.03529486801511115},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.23951901202469938,\n",
       "    'acc_stderr,none': 0.007675094407463255,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.24561403508771928,\n",
       "    'acc_stderr,none': 0.04049339297748139},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.17676767676767677,\n",
       "    'acc_stderr,none': 0.027178752639044915},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.21761658031088082,\n",
       "    'acc_stderr,none': 0.02977866303775296},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2076923076923077,\n",
       "    'acc_stderr,none': 0.020567539567246797},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.21428571428571427,\n",
       "    'acc_stderr,none': 0.026653531596715484},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.20550458715596331,\n",
       "    'acc_stderr,none': 0.017324352325016015},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2595419847328244,\n",
       "    'acc_stderr,none': 0.0384487613978527},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.3006535947712418,\n",
       "    'acc_stderr,none': 0.01855063450295296},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.2727272727272727,\n",
       "    'acc_stderr,none': 0.042657921109405895},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.24081632653061225,\n",
       "    'acc_stderr,none': 0.027372942201788174},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.2537313432835821,\n",
       "    'acc_stderr,none': 0.030769444967296028},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_stem': {'acc,none': 0.21408182683158897,\n",
       "    'acc_stderr,none': 0.007290969735951905,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.1925925925925926,\n",
       "    'acc_stderr,none': 0.034065420585026526},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.17763157894736842,\n",
       "    'acc_stderr,none': 0.031103182383123398},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2569444444444444,\n",
       "    'acc_stderr,none': 0.03653946969442099},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.26382978723404255,\n",
       "    'acc_stderr,none': 0.02880998985410298},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.20899470899470898,\n",
       "    'acc_stderr,none': 0.020940481565334835},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.1774193548387097,\n",
       "    'acc_stderr,none': 0.021732540689329265},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.04512608598542127},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.18543046357615894,\n",
       "    'acc_stderr,none': 0.03173284384294284},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.16666666666666666,\n",
       "    'acc_stderr,none': 0.025416428388767478},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.043994650575715215}}},\n",
       " 'llama3_8b_a_high': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 3.6331810400545583,\n",
       "    'bleu_max_stderr,none': 0.14328799068242898,\n",
       "    'bleu_acc,none': 0.44430844553243576,\n",
       "    'bleu_acc_stderr,none': 0.01739458625074318,\n",
       "    'bleu_diff,none': -0.1483657592429612,\n",
       "    'bleu_diff_stderr,none': 0.09934655519246248,\n",
       "    'rouge1_max,none': 17.82754857886558,\n",
       "    'rouge1_max_stderr,none': 0.3131472898216751,\n",
       "    'rouge1_acc,none': 0.5104039167686658,\n",
       "    'rouge1_acc_stderr,none': 0.017499711430249264,\n",
       "    'rouge1_diff,none': 0.21775598542740637,\n",
       "    'rouge1_diff_stderr,none': 0.2333928504294488,\n",
       "    'rouge2_max,none': 7.481901467171806,\n",
       "    'rouge2_max_stderr,none': 0.29180490818749016,\n",
       "    'rouge2_acc,none': 0.30354957160342716,\n",
       "    'rouge2_acc_stderr,none': 0.016095884155386847,\n",
       "    'rouge2_diff,none': -0.710384922417033,\n",
       "    'rouge2_diff_stderr,none': 0.2064801305871506,\n",
       "    'rougeL_max,none': 14.893331477673788,\n",
       "    'rougeL_max_stderr,none': 0.2903216864623568,\n",
       "    'rougeL_acc,none': 0.4455324357405141,\n",
       "    'rougeL_acc_stderr,none': 0.017399335280140336,\n",
       "    'rougeL_diff,none': -0.2434827281533803,\n",
       "    'rougeL_diff_stderr,none': 0.2049503664181432},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3929008567931457,\n",
       "    'acc_stderr,none': 0.017097248285233065},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5283293215289151,\n",
       "    'acc_stderr,none': 0.01648624391303223}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.28794642857142855,\n",
       "    'acc_stderr,none': 0.02141698936957184,\n",
       "    'acc_norm,none': 0.28794642857142855,\n",
       "    'acc_norm_stderr,none': 0.02141698936957184}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.42835209825997955,\n",
       "    'acc_stderr,none': 0.01119730826260609}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.736126224156692,\n",
       "    'acc_stderr,none': 0.010282996367695571,\n",
       "    'acc_norm,none': 0.7290533188248096,\n",
       "    'acc_norm_stderr,none': 0.010369718937426841}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.2841932841932842,\n",
       "    'acc_stderr,none': 0.012912932309514277}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.0037907505686125853,\n",
       "    'exact_match_stderr,strict-match': 0.0016927007401501995,\n",
       "    'exact_match,flexible-extract': 0.7073540561031084,\n",
       "    'exact_match_stderr,flexible-extract': 0.012532334368242894}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.08928571428571429,\n",
       "    'exact_match_stderr,flexible-extract': 0.013487401985819231}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.27455357142857145,\n",
       "    'acc_stderr,none': 0.02110874729063386,\n",
       "    'acc_norm,none': 0.27455357142857145,\n",
       "    'acc_norm_stderr,none': 0.02110874729063386}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.08928571428571429,\n",
       "    'exact_match_stderr,flexible-extract': 0.013487401985819237}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.24857621440536012,\n",
       "    'acc_stderr,none': 0.007911755262023775,\n",
       "    'acc_norm,none': 0.26666666666666666,\n",
       "    'acc_norm_stderr,none': 0.008095350740048933}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.3067226890756303,\n",
       "    'acc_stderr,none': 0.003837263367382663,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3564293304994687,\n",
       "    'acc_stderr,none': 0.006902577196804427,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.30158730158730157,\n",
       "    'acc_stderr,none': 0.04104947269903394},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.509090909090909,\n",
       "    'acc_stderr,none': 0.0390369864774844},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.4215686274509804,\n",
       "    'acc_stderr,none': 0.03465868196380758},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.4978902953586498,\n",
       "    'acc_stderr,none': 0.032546938018020076},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.6033057851239669,\n",
       "    'acc_stderr,none': 0.04465869780531009},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.4074074074074074,\n",
       "    'acc_stderr,none': 0.04750077341199987},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.2883435582822086,\n",
       "    'acc_stderr,none': 0.035590395316173425},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.38439306358381503,\n",
       "    'acc_stderr,none': 0.026189666966272035},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.2759776536312849,\n",
       "    'acc_stderr,none': 0.014950103002475356},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2990353697749196,\n",
       "    'acc_stderr,none': 0.026003301117885142},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.30246913580246915,\n",
       "    'acc_stderr,none': 0.025557653981868052},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.35267275097783574,\n",
       "    'acc_stderr,none': 0.012203286846053887},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.43859649122807015,\n",
       "    'acc_stderr,none': 0.0380579750559046},\n",
       "   'mmlu_other': {'acc,none': 0.32217573221757323,\n",
       "    'acc_stderr,none': 0.008292740447640173,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.47,\n",
       "    'acc_stderr,none': 0.05016135580465919},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.27547169811320754,\n",
       "    'acc_stderr,none': 0.027495663683724057},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.24855491329479767,\n",
       "    'acc_stderr,none': 0.03295304696818318},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.43946188340807174,\n",
       "    'acc_stderr,none': 0.03331092511038179},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.24271844660194175,\n",
       "    'acc_stderr,none': 0.042450224863844935},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.4658119658119658,\n",
       "    'acc_stderr,none': 0.03267942734081228},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.36,\n",
       "    'acc_stderr,none': 0.048241815132442176},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.3243933588761175,\n",
       "    'acc_stderr,none': 0.01674092904716269},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.2549019607843137,\n",
       "    'acc_stderr,none': 0.02495418432487991},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.32978723404255317,\n",
       "    'acc_stderr,none': 0.028045946942042398},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.026303648393696036},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.35542168674698793,\n",
       "    'acc_stderr,none': 0.03726214354322415},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.29769255768605785,\n",
       "    'acc_stderr,none': 0.008187755348076274,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.24561403508771928,\n",
       "    'acc_stderr,none': 0.0404933929774814},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.24242424242424243,\n",
       "    'acc_stderr,none': 0.030532892233932008},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.25906735751295334,\n",
       "    'acc_stderr,none': 0.0316187791793541},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.23333333333333334,\n",
       "    'acc_stderr,none': 0.021444547301560476},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.24369747899159663,\n",
       "    'acc_stderr,none': 0.027886828078380554},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.24954128440366974,\n",
       "    'acc_stderr,none': 0.01855389762950162},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.3053435114503817,\n",
       "    'acc_stderr,none': 0.040393149787245626},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.3790849673202614,\n",
       "    'acc_stderr,none': 0.01962744474841224},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.36363636363636365,\n",
       "    'acc_stderr,none': 0.04607582090719976},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.34285714285714286,\n",
       "    'acc_stderr,none': 0.030387262919547728},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.36318407960199006,\n",
       "    'acc_stderr,none': 0.03400598505599014},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.36,\n",
       "    'acc_stderr,none': 0.048241815132442176},\n",
       "   'mmlu_stem': {'acc,none': 0.2261338407865525,\n",
       "    'acc_stderr,none': 0.007436249366135066,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2074074074074074,\n",
       "    'acc_stderr,none': 0.03502553170678318},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.20394736842105263,\n",
       "    'acc_stderr,none': 0.0327900040631005},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2638888888888889,\n",
       "    'acc_stderr,none': 0.03685651095897532},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.2680851063829787,\n",
       "    'acc_stderr,none': 0.02895734278834235},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2482758620689655,\n",
       "    'acc_stderr,none': 0.0360010569272777},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.21164021164021163,\n",
       "    'acc_stderr,none': 0.021037331505262886},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2161290322580645,\n",
       "    'acc_stderr,none': 0.023415293433568535},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15763546798029557,\n",
       "    'acc_stderr,none': 0.025639014131172404},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.04688261722621504},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.19205298013245034,\n",
       "    'acc_stderr,none': 0.032162984205936135},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.18981481481481483,\n",
       "    'acc_stderr,none': 0.026744714834691936},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.33035714285714285,\n",
       "    'acc_stderr,none': 0.04464285714285713}}},\n",
       " 'llama3_8b_n_low': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 8.20328220970691,\n",
       "    'bleu_max_stderr,none': 0.3682611798938433,\n",
       "    'bleu_acc,none': 0.46266829865361075,\n",
       "    'bleu_acc_stderr,none': 0.017454645150970588,\n",
       "    'bleu_diff,none': -0.013336048498288572,\n",
       "    'bleu_diff_stderr,none': 0.27508459267927626,\n",
       "    'rouge1_max,none': 28.29112604389833,\n",
       "    'rouge1_max_stderr,none': 0.5782263792606942,\n",
       "    'rouge1_acc,none': 0.48592411260709917,\n",
       "    'rouge1_acc_stderr,none': 0.017496563717042786,\n",
       "    'rouge1_diff,none': 0.05889395945855877,\n",
       "    'rouge1_diff_stderr,none': 0.41935067249403335,\n",
       "    'rouge2_max,none': 14.737575827264752,\n",
       "    'rouge2_max_stderr,none': 0.5598634473459102,\n",
       "    'rouge2_acc,none': 0.3708690330477356,\n",
       "    'rouge2_acc_stderr,none': 0.01690969358024884,\n",
       "    'rouge2_diff,none': -0.8168664311452368,\n",
       "    'rouge2_diff_stderr,none': 0.4836338536513406,\n",
       "    'rougeL_max,none': 24.80779606475525,\n",
       "    'rougeL_max_stderr,none': 0.5705401385099255,\n",
       "    'rougeL_acc,none': 0.4638922888616891,\n",
       "    'rougeL_acc_stderr,none': 0.017457800422268622,\n",
       "    'rougeL_diff,none': -0.3616765869084602,\n",
       "    'rougeL_diff_stderr,none': 0.4106235554398654},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4418604651162791,\n",
       "    'acc_stderr,none': 0.017384767478986214},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5824617529557601,\n",
       "    'acc_stderr,none': 0.01625952058161895}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.29464285714285715,\n",
       "    'acc_stderr,none': 0.021562481080109754,\n",
       "    'acc_norm,none': 0.29464285714285715,\n",
       "    'acc_norm_stderr,none': 0.021562481080109754}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.42835209825997955,\n",
       "    'acc_stderr,none': 0.011197308262606091}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7606093579978237,\n",
       "    'acc_stderr,none': 0.009955884250291699,\n",
       "    'acc_norm,none': 0.7448313384113167,\n",
       "    'acc_norm_stderr,none': 0.010171571592521822}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.4774774774774775,\n",
       "    'acc_stderr,none': 0.01430042805673737}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.08263836239575435,\n",
       "    'exact_match_stderr,strict-match': 0.007584089220148142,\n",
       "    'exact_match,flexible-extract': 0.7187263078089462,\n",
       "    'exact_match_stderr,flexible-extract': 0.012384789310940234}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.11383928571428571,\n",
       "    'exact_match_stderr,flexible-extract': 0.015022719761856691}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.28794642857142855,\n",
       "    'acc_stderr,none': 0.021416989369571836,\n",
       "    'acc_norm,none': 0.28794642857142855,\n",
       "    'acc_norm_stderr,none': 0.021416989369571836}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.12946428571428573,\n",
       "    'exact_match_stderr,flexible-extract': 0.015878684686705426}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2974874371859296,\n",
       "    'acc_stderr,none': 0.008368776185719963,\n",
       "    'acc_norm,none': 0.3132328308207705,\n",
       "    'acc_norm_stderr,none': 0.008490611920810433}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.30843184731519724,\n",
       "    'acc_stderr,none': 0.003869942199798826,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3179596174282678,\n",
       "    'acc_stderr,none': 0.006782133096137536,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.30303030303030304,\n",
       "    'acc_stderr,none': 0.035886248000917075},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.3284313725490196,\n",
       "    'acc_stderr,none': 0.03296245110172229},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.37130801687763715,\n",
       "    'acc_stderr,none': 0.03145068600744858},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.4132231404958678,\n",
       "    'acc_stderr,none': 0.04495087843548408},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.35185185185185186,\n",
       "    'acc_stderr,none': 0.04616631111801712},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.25766871165644173,\n",
       "    'acc_stderr,none': 0.03436150827846917},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.34104046242774566,\n",
       "    'acc_stderr,none': 0.02552247463212161},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.2994413407821229,\n",
       "    'acc_stderr,none': 0.015318257745976708},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2508038585209003,\n",
       "    'acc_stderr,none': 0.02461977195669716},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.3148148148148148,\n",
       "    'acc_stderr,none': 0.02584224870090217},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.3246414602346806,\n",
       "    'acc_stderr,none': 0.011959089388530016},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3567251461988304,\n",
       "    'acc_stderr,none': 0.03674013002860954},\n",
       "   'mmlu_other': {'acc,none': 0.3382684261345349,\n",
       "    'acc_stderr,none': 0.008401851943584988,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.41,\n",
       "    'acc_stderr,none': 0.049431107042371025},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.32075471698113206,\n",
       "    'acc_stderr,none': 0.028727502957880267},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.24277456647398843,\n",
       "    'acc_stderr,none': 0.0326926380614177},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.45739910313901344,\n",
       "    'acc_stderr,none': 0.033435777055830646},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.3300970873786408,\n",
       "    'acc_stderr,none': 0.046561471100123514},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.49572649572649574,\n",
       "    'acc_stderr,none': 0.03275489264382132},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.36,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.3499361430395913,\n",
       "    'acc_stderr,none': 0.01705567979715042},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3235294117647059,\n",
       "    'acc_stderr,none': 0.026787453111906532},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.2907801418439716,\n",
       "    'acc_stderr,none': 0.027090664368353178},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.23897058823529413,\n",
       "    'acc_stderr,none': 0.025905280644893013},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3373493975903614,\n",
       "    'acc_stderr,none': 0.03680783690727581},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.327916802079948,\n",
       "    'acc_stderr,none': 0.008433575804595268,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.23684210526315788,\n",
       "    'acc_stderr,none': 0.039994238792813386},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.25252525252525254,\n",
       "    'acc_stderr,none': 0.030954055470365886},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.3160621761658031,\n",
       "    'acc_stderr,none': 0.03355397369686173},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2846153846153846,\n",
       "    'acc_stderr,none': 0.0228783227997063},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.28991596638655465,\n",
       "    'acc_stderr,none': 0.029472485833136084},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3211009174311927,\n",
       "    'acc_stderr,none': 0.020018149772733747},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.366412213740458,\n",
       "    'acc_stderr,none': 0.0422587545196964},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.019450768432505518},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.4636363636363636,\n",
       "    'acc_stderr,none': 0.047764491623961985},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.3224489795918367,\n",
       "    'acc_stderr,none': 0.029923100563683906},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.40298507462686567,\n",
       "    'acc_stderr,none': 0.034683432951111266},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.047937248544110196},\n",
       "   'mmlu_stem': {'acc,none': 0.2457976530288614,\n",
       "    'acc_stderr,none': 0.007636342174018475,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2222222222222222,\n",
       "    'acc_stderr,none': 0.035914440841969694},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.23684210526315788,\n",
       "    'acc_stderr,none': 0.03459777606810537},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2986111111111111,\n",
       "    'acc_stderr,none': 0.03827052357950756},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.04878317312145632},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.2936170212765957,\n",
       "    'acc_stderr,none': 0.029771642712491227},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2620689655172414,\n",
       "    'acc_stderr,none': 0.036646663372252565},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.23809523809523808,\n",
       "    'acc_stderr,none': 0.02193587808118476},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2870967741935484,\n",
       "    'acc_stderr,none': 0.02573654274559453},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.18226600985221675,\n",
       "    'acc_stderr,none': 0.02716334085964515},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.34,\n",
       "    'acc_stderr,none': 0.04760952285695235},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.19205298013245034,\n",
       "    'acc_stderr,none': 0.032162984205936135},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.1712962962962963,\n",
       "    'acc_stderr,none': 0.02569534164382468},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.32142857142857145,\n",
       "    'acc_stderr,none': 0.04432804055291519}}},\n",
       " 'llama3_8b_c_low': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.2680823289212095,\n",
       "    'bleu_max_stderr,none': 0.0736237272127832,\n",
       "    'bleu_acc,none': 0.38922888616891066,\n",
       "    'bleu_acc_stderr,none': 0.017068552680690335,\n",
       "    'bleu_diff,none': -0.02401971623787362,\n",
       "    'bleu_diff_stderr,none': 0.06475613527703544,\n",
       "    'rouge1_max,none': 13.908929431364948,\n",
       "    'rouge1_max_stderr,none': 0.2837261269907521,\n",
       "    'rouge1_acc,none': 0.5348837209302325,\n",
       "    'rouge1_acc_stderr,none': 0.017460849975873965,\n",
       "    'rouge1_diff,none': 0.23551973016200406,\n",
       "    'rouge1_diff_stderr,none': 0.2718428082087857,\n",
       "    'rouge2_max,none': 2.8568555529629256,\n",
       "    'rouge2_max_stderr,none': 0.22007078205160477,\n",
       "    'rouge2_acc,none': 0.12974296205630356,\n",
       "    'rouge2_acc_stderr,none': 0.01176306771530919,\n",
       "    'rouge2_diff,none': -0.9935670563447322,\n",
       "    'rouge2_diff_stderr,none': 0.19305891492221305,\n",
       "    'rougeL_max,none': 12.56863550226571,\n",
       "    'rougeL_max_stderr,none': 0.24938904066616777,\n",
       "    'rougeL_acc,none': 0.5483476132190942,\n",
       "    'rougeL_acc_stderr,none': 0.017421480300277643,\n",
       "    'rougeL_diff,none': 0.1526652500536892,\n",
       "    'rougeL_diff_stderr,none': 0.2551142271928703},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.21542227662178703,\n",
       "    'acc_stderr,none': 0.014391902652427681},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.3896310053069383,\n",
       "    'acc_stderr,none': 0.015777466583940525}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.20982142857142858,\n",
       "    'acc_stderr,none': 0.019259002176655816,\n",
       "    'acc_norm,none': 0.20982142857142858,\n",
       "    'acc_norm_stderr,none': 0.019259002176655816}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.3781985670419652,\n",
       "    'acc_stderr,none': 0.010973234926696033}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.6496191512513602,\n",
       "    'acc_stderr,none': 0.011131277554681745,\n",
       "    'acc_norm,none': 0.6485310119695321,\n",
       "    'acc_norm_stderr,none': 0.011139207691931191}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.21621621621621623,\n",
       "    'acc_stderr,none': 0.011785889175486638}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.31842304776345715,\n",
       "    'exact_match_stderr,flexible-extract': 0.012832225723075416}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.12723214285714285,\n",
       "    'exact_match_stderr,flexible-extract': 0.015761372420705265}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.22991071428571427,\n",
       "    'acc_stderr,none': 0.01990198453013952,\n",
       "    'acc_norm,none': 0.22991071428571427,\n",
       "    'acc_norm_stderr,none': 0.01990198453013952}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.06473214285714286,\n",
       "    'exact_match_stderr,flexible-extract': 0.01163788964877672}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.25058626465661643,\n",
       "    'acc_stderr,none': 0.007933047343539815,\n",
       "    'acc_norm,none': 0.2619765494137353,\n",
       "    'acc_norm_stderr,none': 0.00804946247707931}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.26883634809856144,\n",
       "    'acc_stderr,none': 0.003701233668875993,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.24165781083953242,\n",
       "    'acc_stderr,none': 0.006229085729404717,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.36507936507936506,\n",
       "    'acc_stderr,none': 0.04306241259127153},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.2545454545454545,\n",
       "    'acc_stderr,none': 0.03401506715249039},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.2549019607843137,\n",
       "    'acc_stderr,none': 0.030587591351604246},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.20253164556962025,\n",
       "    'acc_stderr,none': 0.026160568246601464},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.14049586776859505,\n",
       "    'acc_stderr,none': 0.031722334260021585},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.21296296296296297,\n",
       "    'acc_stderr,none': 0.03957835471980981},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.2331288343558282,\n",
       "    'acc_stderr,none': 0.0332201579577674},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.2138728323699422,\n",
       "    'acc_stderr,none': 0.022075709251757177},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.27262569832402234,\n",
       "    'acc_stderr,none': 0.014893391735249603},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.24115755627009647,\n",
       "    'acc_stderr,none': 0.024296594034763426},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.22530864197530864,\n",
       "    'acc_stderr,none': 0.023246202647819746},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.24445893089960888,\n",
       "    'acc_stderr,none': 0.010976425013113897},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.17543859649122806,\n",
       "    'acc_stderr,none': 0.029170885500727686},\n",
       "   'mmlu_other': {'acc,none': 0.2513678789829417,\n",
       "    'acc_stderr,none': 0.007644350095427473,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.2981132075471698,\n",
       "    'acc_stderr,none': 0.028152837942493875},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3352601156069364,\n",
       "    'acc_stderr,none': 0.03599586301247078},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.10762331838565023,\n",
       "    'acc_stderr,none': 0.020799400082880008},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.3786407766990291,\n",
       "    'acc_stderr,none': 0.048026946982589726},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.19658119658119658,\n",
       "    'acc_stderr,none': 0.02603538609895129},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.04292346959909282},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.20434227330779056,\n",
       "    'acc_stderr,none': 0.0144191239809319},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.29411764705882354,\n",
       "    'acc_stderr,none': 0.026090162504279035},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.24113475177304963,\n",
       "    'acc_stderr,none': 0.025518731049537755},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.4485294117647059,\n",
       "    'acc_stderr,none': 0.030211479609121593},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.1927710843373494,\n",
       "    'acc_stderr,none': 0.03070982405056527},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.31036724081897954,\n",
       "    'acc_stderr,none': 0.008274922672422521,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.23684210526315788,\n",
       "    'acc_stderr,none': 0.039994238792813344},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.35353535353535354,\n",
       "    'acc_stderr,none': 0.03406086723547153},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.36787564766839376,\n",
       "    'acc_stderr,none': 0.034801756684660366},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.3641025641025641,\n",
       "    'acc_stderr,none': 0.024396672985094774},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.3487394957983193,\n",
       "    'acc_stderr,none': 0.030956636328566548},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3486238532110092,\n",
       "    'acc_stderr,none': 0.020431254090714328},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2824427480916031,\n",
       "    'acc_stderr,none': 0.03948406125768361},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.016639319350313264},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.22727272727272727,\n",
       "    'acc_stderr,none': 0.04013964554072775},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.031362502409358936},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.26865671641791045,\n",
       "    'acc_stderr,none': 0.03134328358208954},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.0440844002276808},\n",
       "   'mmlu_stem': {'acc,none': 0.2860767522993974,\n",
       "    'acc_stderr,none': 0.007959894447428692,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.22962962962962963,\n",
       "    'acc_stderr,none': 0.03633384414073465},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.3355263157894737,\n",
       "    'acc_stderr,none': 0.03842498559395269},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2638888888888889,\n",
       "    'acc_stderr,none': 0.03685651095897532},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.41,\n",
       "    'acc_stderr,none': 0.04943110704237103},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.33,\n",
       "    'acc_stderr,none': 0.047258156262526045},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.37254901960784315,\n",
       "    'acc_stderr,none': 0.04810840148082634},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.20851063829787234,\n",
       "    'acc_stderr,none': 0.026556982117838728},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2671957671957672,\n",
       "    'acc_stderr,none': 0.022789673145776575},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.3161290322580645,\n",
       "    'acc_stderr,none': 0.026450874489042767},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.28078817733990147,\n",
       "    'acc_stderr,none': 0.03161856335358609},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.03942772444036625},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.26296296296296295,\n",
       "    'acc_stderr,none': 0.026842057873833706},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.33112582781456956,\n",
       "    'acc_stderr,none': 0.038425817186598696},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.4722222222222222,\n",
       "    'acc_stderr,none': 0.0340470532865388},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.16071428571428573,\n",
       "    'acc_stderr,none': 0.0348594609647574}}},\n",
       " 'llama3_8b_c_high': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.214213054108384,\n",
       "    'bleu_max_stderr,none': 0.20547526134039326,\n",
       "    'bleu_acc,none': 0.46511627906976744,\n",
       "    'bleu_acc_stderr,none': 0.017460849975873965,\n",
       "    'bleu_diff,none': 0.09031031177262744,\n",
       "    'bleu_diff_stderr,none': 0.1482970773234913,\n",
       "    'rouge1_max,none': 21.39760796071977,\n",
       "    'rouge1_max_stderr,none': 0.3920251367972948,\n",
       "    'rouge1_acc,none': 0.5116279069767442,\n",
       "    'rouge1_acc_stderr,none': 0.0174987671757401,\n",
       "    'rouge1_diff,none': 0.5119710728434331,\n",
       "    'rouge1_diff_stderr,none': 0.29619329999930066,\n",
       "    'rouge2_max,none': 10.424527672411028,\n",
       "    'rouge2_max_stderr,none': 0.3658919008403597,\n",
       "    'rouge2_acc,none': 0.3623011015911873,\n",
       "    'rouge2_acc_stderr,none': 0.016826646897262258,\n",
       "    'rouge2_diff,none': -0.32929587785394054,\n",
       "    'rouge2_diff_stderr,none': 0.30229329428177193,\n",
       "    'rougeL_max,none': 18.44959662656389,\n",
       "    'rougeL_max_stderr,none': 0.3737038246410187,\n",
       "    'rougeL_acc,none': 0.48225214198286415,\n",
       "    'rougeL_acc_stderr,none': 0.01749247084307534,\n",
       "    'rougeL_diff,none': 0.11917885639657123,\n",
       "    'rougeL_diff_stderr,none': 0.27578979689195027},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3843329253365973,\n",
       "    'acc_stderr,none': 0.017028707301245213},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5498847415465643,\n",
       "    'acc_stderr,none': 0.016380862038138783}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.296875,\n",
       "    'acc_stderr,none': 0.021609729061250887,\n",
       "    'acc_norm,none': 0.296875,\n",
       "    'acc_norm_stderr,none': 0.021609729061250887}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4247697031729785,\n",
       "    'acc_stderr,none': 0.01118527125767135}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7410228509249184,\n",
       "    'acc_stderr,none': 0.010220966031405595,\n",
       "    'acc_norm,none': 0.733949945593036,\n",
       "    'acc_norm_stderr,none': 0.01031003926335282}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.48157248157248156,\n",
       "    'acc_stderr,none': 0.014305233095109329}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.14783927217589082,\n",
       "    'exact_match_stderr,strict-match': 0.009776827679143884,\n",
       "    'exact_match,flexible-extract': 0.7217589082638363,\n",
       "    'exact_match_stderr,flexible-extract': 0.012343803671422675}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.18526785714285715,\n",
       "    'exact_match_stderr,flexible-extract': 0.018376115117972446}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.28348214285714285,\n",
       "    'acc_stderr,none': 0.021316828987262147,\n",
       "    'acc_norm,none': 0.28348214285714285,\n",
       "    'acc_norm_stderr,none': 0.021316828987262147}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.15625,\n",
       "    'exact_match_stderr,flexible-extract': 0.017173671221421365}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2827470686767169,\n",
       "    'acc_stderr,none': 0.008243958788826992,\n",
       "    'acc_norm,none': 0.2984924623115578,\n",
       "    'acc_norm_stderr,none': 0.008376902213944468}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.2971086739780658,\n",
       "    'acc_stderr,none': 0.0038201610159485522,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3230605738575983,\n",
       "    'acc_stderr,none': 0.006786761507802471,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.32727272727272727,\n",
       "    'acc_stderr,none': 0.03663974994391242},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.03374499356319355},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.4092827004219409,\n",
       "    'acc_stderr,none': 0.032007041833595914},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.4049586776859504,\n",
       "    'acc_stderr,none': 0.044811377559424694},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.2962962962962963,\n",
       "    'acc_stderr,none': 0.04414343666854933},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.25153374233128833,\n",
       "    'acc_stderr,none': 0.03408997886857529},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.3786127167630058,\n",
       "    'acc_stderr,none': 0.02611374936131034},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.28938547486033517,\n",
       "    'acc_stderr,none': 0.015166544550490298},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.21864951768488747,\n",
       "    'acc_stderr,none': 0.023475581417861102},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.25925925925925924,\n",
       "    'acc_stderr,none': 0.02438366553103545},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.35267275097783574,\n",
       "    'acc_stderr,none': 0.012203286846053887},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3157894736842105,\n",
       "    'acc_stderr,none': 0.03565079670708312},\n",
       "   'mmlu_other': {'acc,none': 0.321853878339234,\n",
       "    'acc_stderr,none': 0.008316665601449334,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.48,\n",
       "    'acc_stderr,none': 0.050211673156867795},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.28679245283018867,\n",
       "    'acc_stderr,none': 0.02783491252754407},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.2138728323699422,\n",
       "    'acc_stderr,none': 0.031265112061730424},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.42152466367713004,\n",
       "    'acc_stderr,none': 0.03314190222110657},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.2524271844660194,\n",
       "    'acc_stderr,none': 0.04301250399690878},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.4358974358974359,\n",
       "    'acc_stderr,none': 0.03248577511578401},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.36,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.30779054916985954,\n",
       "    'acc_stderr,none': 0.016506045045155623},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3137254901960784,\n",
       "    'acc_stderr,none': 0.02656892101545715},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.32269503546099293,\n",
       "    'acc_stderr,none': 0.02788913930053479},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.29044117647058826,\n",
       "    'acc_stderr,none': 0.027576468622740526},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3373493975903614,\n",
       "    'acc_stderr,none': 0.03680783690727581},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.3002924926876828,\n",
       "    'acc_stderr,none': 0.008187119648169492,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.24561403508771928,\n",
       "    'acc_stderr,none': 0.0404933929774814},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.21717171717171718,\n",
       "    'acc_stderr,none': 0.02937661648494563},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.24352331606217617,\n",
       "    'acc_stderr,none': 0.030975436386845436},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.24102564102564103,\n",
       "    'acc_stderr,none': 0.02168554666533319},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.23109243697478993,\n",
       "    'acc_stderr,none': 0.027381406927868963},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.25321100917431194,\n",
       "    'acc_stderr,none': 0.018644073041375046},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.3893129770992366,\n",
       "    'acc_stderr,none': 0.042764865428145914},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.35294117647058826,\n",
       "    'acc_stderr,none': 0.019333142020797063},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.38181818181818183,\n",
       "    'acc_stderr,none': 0.04653429807913508},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.03168091161233882},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.36318407960199006,\n",
       "    'acc_stderr,none': 0.034005985055990146},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_stem': {'acc,none': 0.23089121471614335,\n",
       "    'acc_stderr,none': 0.007473989698229859,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.18518518518518517,\n",
       "    'acc_stderr,none': 0.03355677216313142},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.20394736842105263,\n",
       "    'acc_stderr,none': 0.0327900040631005},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2708333333333333,\n",
       "    'acc_stderr,none': 0.037161774375660164},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.28936170212765955,\n",
       "    'acc_stderr,none': 0.02964400657700962},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.25517241379310346,\n",
       "    'acc_stderr,none': 0.03632984052707842},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.23015873015873015,\n",
       "    'acc_stderr,none': 0.021679219663693145},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.022755204959542936},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.047937248544110175},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.19205298013245034,\n",
       "    'acc_stderr,none': 0.032162984205936135},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.19907407407407407,\n",
       "    'acc_stderr,none': 0.02723229846269023},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.04547960999764376}}},\n",
       " 'llama3_8b_o_high': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 6.120500196897751,\n",
       "    'bleu_max_stderr,none': 0.30407600589603423,\n",
       "    'bleu_acc,none': 0.4430844553243574,\n",
       "    'bleu_acc_stderr,none': 0.017389730346877123,\n",
       "    'bleu_diff,none': 0.016222811448786685,\n",
       "    'bleu_diff_stderr,none': 0.1803166073632639,\n",
       "    'rouge1_max,none': 20.907465614653212,\n",
       "    'rouge1_max_stderr,none': 0.4982413218589522,\n",
       "    'rouge1_acc,none': 0.5091799265605875,\n",
       "    'rouge1_acc_stderr,none': 0.017500550724819746,\n",
       "    'rouge1_diff,none': 0.652811962394366,\n",
       "    'rouge1_diff_stderr,none': 0.31259947315064174,\n",
       "    'rouge2_max,none': 10.89984705765847,\n",
       "    'rouge2_max_stderr,none': 0.47437606845147895,\n",
       "    'rouge2_acc,none': 0.3243574051407589,\n",
       "    'rouge2_acc_stderr,none': 0.016387976779647935,\n",
       "    'rouge2_diff,none': -0.7609675083956697,\n",
       "    'rouge2_diff_stderr,none': 0.2957608158582307,\n",
       "    'rougeL_max,none': 18.728519154477137,\n",
       "    'rougeL_max_stderr,none': 0.4725383669965516,\n",
       "    'rougeL_acc,none': 0.5018359853121175,\n",
       "    'rougeL_acc_stderr,none': 0.017503383046877017,\n",
       "    'rougeL_diff,none': 0.5400312836731155,\n",
       "    'rougeL_diff_stderr,none': 0.30396407182717305},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3671970624235006,\n",
       "    'acc_stderr,none': 0.01687480500145318},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5242601771769332,\n",
       "    'acc_stderr,none': 0.016185181127500118}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.27901785714285715,\n",
       "    'acc_stderr,none': 0.02121409415726597,\n",
       "    'acc_norm,none': 0.27901785714285715,\n",
       "    'acc_norm_stderr,none': 0.02121409415726597}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4232343909928352,\n",
       "    'acc_stderr,none': 0.011179928646626211}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7279651795429815,\n",
       "    'acc_stderr,none': 0.010382763786247376,\n",
       "    'acc_norm,none': 0.7295973884657236,\n",
       "    'acc_norm_stderr,none': 0.01036316703162079}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.2285012285012285,\n",
       "    'acc_stderr,none': 0.012020761312005529}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.10765731614859743,\n",
       "    'exact_match_stderr,strict-match': 0.008537484003023347,\n",
       "    'exact_match,flexible-extract': 0.6838514025777104,\n",
       "    'exact_match_stderr,flexible-extract': 0.012807630673451474}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.10044642857142858,\n",
       "    'exact_match_stderr,flexible-extract': 0.014217623336210839}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.27901785714285715,\n",
       "    'acc_stderr,none': 0.02121409415726597,\n",
       "    'acc_norm,none': 0.27901785714285715,\n",
       "    'acc_norm_stderr,none': 0.02121409415726597}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.16294642857142858,\n",
       "    'exact_match_stderr,flexible-extract': 0.01746808467103293}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.26867671691792294,\n",
       "    'acc_stderr,none': 0.008114659613200546,\n",
       "    'acc_norm,none': 0.28241206030150756,\n",
       "    'acc_norm_stderr,none': 0.008240997372910704}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.22988178322176328,\n",
       "    'acc_stderr,none': 0.003544872605603228,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.24293304994686504,\n",
       "    'acc_stderr,none': 0.006250074314389129,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.21818181818181817,\n",
       "    'acc_stderr,none': 0.03225078108306289},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.03039153369274154},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.270042194092827,\n",
       "    'acc_stderr,none': 0.028900721906293426},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.2396694214876033,\n",
       "    'acc_stderr,none': 0.03896878985070417},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.25925925925925924,\n",
       "    'acc_stderr,none': 0.04236511258094634},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.22085889570552147,\n",
       "    'acc_stderr,none': 0.032591773927421776},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.24855491329479767,\n",
       "    'acc_stderr,none': 0.023267528432100174},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.23910614525139665,\n",
       "    'acc_stderr,none': 0.014265554192331158},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.1864951768488746,\n",
       "    'acc_stderr,none': 0.02212243977248077},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.21604938271604937,\n",
       "    'acc_stderr,none': 0.022899162918445813},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.24771838331160365,\n",
       "    'acc_stderr,none': 0.01102549929144374},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3216374269005848,\n",
       "    'acc_stderr,none': 0.03582529442573122},\n",
       "   'mmlu_other': {'acc,none': 0.23978113936272932,\n",
       "    'acc_stderr,none': 0.00764225029165751,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.21509433962264152,\n",
       "    'acc_stderr,none': 0.025288394502891377},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.20809248554913296,\n",
       "    'acc_stderr,none': 0.030952890217749884},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.31390134529147984,\n",
       "    'acc_stderr,none': 0.03114679648297246},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.17475728155339806,\n",
       "    'acc_stderr,none': 0.03760178006026621},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.2905982905982906,\n",
       "    'acc_stderr,none': 0.029745048572674057},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.23754789272030652,\n",
       "    'acc_stderr,none': 0.015218733046150195},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.22549019607843138,\n",
       "    'acc_stderr,none': 0.023929155517351284},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.23404255319148937,\n",
       "    'acc_stderr,none': 0.025257861359432407},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.18382352941176472,\n",
       "    'acc_stderr,none': 0.02352924218519311},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.28313253012048195,\n",
       "    'acc_stderr,none': 0.03507295431370518},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.21774455638609036,\n",
       "    'acc_stderr,none': 0.007435872907094233,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.23684210526315788,\n",
       "    'acc_stderr,none': 0.039994238792813386},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.17676767676767677,\n",
       "    'acc_stderr,none': 0.027178752639044915},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.19689119170984457,\n",
       "    'acc_stderr,none': 0.02869787397186069},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.20256410256410257,\n",
       "    'acc_stderr,none': 0.020377660970371397},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.21008403361344538,\n",
       "    'acc_stderr,none': 0.026461398717471874},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.1926605504587156,\n",
       "    'acc_stderr,none': 0.016909276884936073},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2595419847328244,\n",
       "    'acc_stderr,none': 0.03844876139785271},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.25326797385620914,\n",
       "    'acc_stderr,none': 0.017593486895366835},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.21818181818181817,\n",
       "    'acc_stderr,none': 0.03955932861795833},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.18775510204081633,\n",
       "    'acc_stderr,none': 0.02500025603954622},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.24378109452736318,\n",
       "    'acc_stderr,none': 0.030360490154014652},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_stem': {'acc,none': 0.21249603552172533,\n",
       "    'acc_stderr,none': 0.007271218700485502,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.18518518518518517,\n",
       "    'acc_stderr,none': 0.03355677216313142},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.17763157894736842,\n",
       "    'acc_stderr,none': 0.031103182383123398},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2569444444444444,\n",
       "    'acc_stderr,none': 0.03653946969442099},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.26382978723404255,\n",
       "    'acc_stderr,none': 0.02880998985410298},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.20899470899470898,\n",
       "    'acc_stderr,none': 0.020940481565334835},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.1774193548387097,\n",
       "    'acc_stderr,none': 0.021732540689329265},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.04351941398892446},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.1986754966887417,\n",
       "    'acc_stderr,none': 0.032578473844367746},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.1527777777777778,\n",
       "    'acc_stderr,none': 0.02453632602613422},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.043994650575715215}}},\n",
       " 'llama3_8b_n_high': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.321646507169417,\n",
       "    'bleu_max_stderr,none': 0.09382597732213947,\n",
       "    'bleu_acc,none': 0.43329253365973075,\n",
       "    'bleu_acc_stderr,none': 0.017347024450107475,\n",
       "    'bleu_diff,none': 0.12198534730626345,\n",
       "    'bleu_diff_stderr,none': 0.06215602174931268,\n",
       "    'rouge1_max,none': 14.26759422553061,\n",
       "    'rouge1_max_stderr,none': 0.29197529604667044,\n",
       "    'rouge1_acc,none': 0.4834761321909425,\n",
       "    'rouge1_acc_stderr,none': 0.01749394019005773,\n",
       "    'rouge1_diff,none': 0.03547954784350116,\n",
       "    'rouge1_diff_stderr,none': 0.20444121891923492,\n",
       "    'rouge2_max,none': 6.126869692692061,\n",
       "    'rouge2_max_stderr,none': 0.2761508295922496,\n",
       "    'rouge2_acc,none': 0.2558139534883721,\n",
       "    'rouge2_acc_stderr,none': 0.015274176219283361,\n",
       "    'rouge2_diff,none': -0.3540097225234341,\n",
       "    'rouge2_diff_stderr,none': 0.1816222949516425,\n",
       "    'rougeL_max,none': 12.776626298186578,\n",
       "    'rougeL_max_stderr,none': 0.2746256135058493,\n",
       "    'rougeL_acc,none': 0.4528763769889841,\n",
       "    'rougeL_acc_stderr,none': 0.01742558984831402,\n",
       "    'rougeL_diff,none': -0.0862411906168757,\n",
       "    'rougeL_diff_stderr,none': 0.19226774211642375},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.20930232558139536,\n",
       "    'acc_stderr,none': 0.014241219434785827},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.3875125994541383,\n",
       "    'acc_stderr,none': 0.015947609200494667}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.25223214285714285,\n",
       "    'acc_stderr,none': 0.02054139101648797,\n",
       "    'acc_norm,none': 0.25223214285714285,\n",
       "    'acc_norm_stderr,none': 0.02054139101648797}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.39048106448311154,\n",
       "    'acc_stderr,none': 0.01103932371486307}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.6855277475516867,\n",
       "    'acc_stderr,none': 0.010833009065106576,\n",
       "    'acc_norm,none': 0.6697497279651795,\n",
       "    'acc_norm_stderr,none': 0.010972947133006304}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.23669123669123668,\n",
       "    'acc_stderr,none': 0.012169179531215669}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.002274450341167551,\n",
       "    'exact_match_stderr,strict-match': 0.0013121578148673932,\n",
       "    'exact_match,flexible-extract': 0.030326004548900682,\n",
       "    'exact_match_stderr,flexible-extract': 0.00472348746551477}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.18303571428571427,\n",
       "    'exact_match_stderr,flexible-extract': 0.018290083717004038}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.25223214285714285,\n",
       "    'acc_stderr,none': 0.02054139101648798,\n",
       "    'acc_norm,none': 0.25223214285714285,\n",
       "    'acc_norm_stderr,none': 0.02054139101648798}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.10267857142857142,\n",
       "    'exact_match_stderr,flexible-extract': 0.014356883187169161}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2492462311557789,\n",
       "    'acc_stderr,none': 0.007918877981680667,\n",
       "    'acc_norm,none': 0.26700167504187605,\n",
       "    'acc_norm_stderr,none': 0.008098583692885271}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.23144851160803304,\n",
       "    'acc_stderr,none': 0.003553394748435655,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.24399574920297556,\n",
       "    'acc_stderr,none': 0.006260601280061598,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2777777777777778,\n",
       "    'acc_stderr,none': 0.04006168083848876},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.23030303030303031,\n",
       "    'acc_stderr,none': 0.0328766675860349},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.03039153369274154},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.270042194092827,\n",
       "    'acc_stderr,none': 0.028900721906293426},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.2396694214876033,\n",
       "    'acc_stderr,none': 0.03896878985070417},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.25925925925925924,\n",
       "    'acc_stderr,none': 0.04236511258094634},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.22085889570552147,\n",
       "    'acc_stderr,none': 0.032591773927421776},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.25722543352601157,\n",
       "    'acc_stderr,none': 0.02353292543104428},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.23798882681564246,\n",
       "    'acc_stderr,none': 0.014242630070574885},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.19614147909967847,\n",
       "    'acc_stderr,none': 0.022552447780478026},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.21296296296296297,\n",
       "    'acc_stderr,none': 0.022779719088733396},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.24771838331160365,\n",
       "    'acc_stderr,none': 0.01102549929144374},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3216374269005848,\n",
       "    'acc_stderr,none': 0.03582529442573122},\n",
       "   'mmlu_other': {'acc,none': 0.23913743160605086,\n",
       "    'acc_stderr,none': 0.007635787113902317,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.04688261722621505},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.21509433962264152,\n",
       "    'acc_stderr,none': 0.025288394502891377},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.20809248554913296,\n",
       "    'acc_stderr,none': 0.030952890217749884},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.3004484304932735,\n",
       "    'acc_stderr,none': 0.030769352008229143},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.17475728155339806,\n",
       "    'acc_stderr,none': 0.03760178006026621},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.2905982905982906,\n",
       "    'acc_stderr,none': 0.029745048572674057},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.23754789272030652,\n",
       "    'acc_stderr,none': 0.015218733046150195},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.2222222222222222,\n",
       "    'acc_stderr,none': 0.023805186524888142},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.23404255319148937,\n",
       "    'acc_stderr,none': 0.025257861359432407},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.18382352941176472,\n",
       "    'acc_stderr,none': 0.02352924218519311},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.28313253012048195,\n",
       "    'acc_stderr,none': 0.03507295431370518},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.22326941826454338,\n",
       "    'acc_stderr,none': 0.007496591147843822,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.23684210526315788,\n",
       "    'acc_stderr,none': 0.039994238792813386},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.17676767676767677,\n",
       "    'acc_stderr,none': 0.027178752639044915},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.19689119170984457,\n",
       "    'acc_stderr,none': 0.02869787397186069},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.20256410256410257,\n",
       "    'acc_stderr,none': 0.020377660970371397},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.21008403361344538,\n",
       "    'acc_stderr,none': 0.026461398717471874},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.1926605504587156,\n",
       "    'acc_stderr,none': 0.016909276884936073},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.26717557251908397,\n",
       "    'acc_stderr,none': 0.038808483010823965},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.272875816993464,\n",
       "    'acc_stderr,none': 0.018020474148393577},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.22727272727272727,\n",
       "    'acc_stderr,none': 0.04013964554072773},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.025607375986579153},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.24378109452736318,\n",
       "    'acc_stderr,none': 0.030360490154014652},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_stem': {'acc,none': 0.2131303520456708,\n",
       "    'acc_stderr,none': 0.007280748708889476,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.18518518518518517,\n",
       "    'acc_stderr,none': 0.03355677216313142},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.17763157894736842,\n",
       "    'acc_stderr,none': 0.031103182383123398},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2569444444444444,\n",
       "    'acc_stderr,none': 0.03653946969442099},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.2,\n",
       "    'acc_stderr,none': 0.040201512610368445},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.26382978723404255,\n",
       "    'acc_stderr,none': 0.02880998985410298},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.20899470899470898,\n",
       "    'acc_stderr,none': 0.020940481565334835},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.1774193548387097,\n",
       "    'acc_stderr,none': 0.021732540689329265},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.04351941398892446},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.1986754966887417,\n",
       "    'acc_stderr,none': 0.032578473844367746},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.16203703703703703,\n",
       "    'acc_stderr,none': 0.02513045365226846},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.043994650575715215}}},\n",
       " 'llama3_8b_e_low': {'llama3_8b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 4.735988443448998,\n",
       "    'bleu_max_stderr,none': 0.18904999383556753,\n",
       "    'bleu_acc,none': 0.6413708690330477,\n",
       "    'bleu_acc_stderr,none': 0.016789289499502022,\n",
       "    'bleu_diff,none': 0.39008008192193716,\n",
       "    'bleu_diff_stderr,none': 0.17863787738734774,\n",
       "    'rouge1_max,none': 20.032679185441083,\n",
       "    'rouge1_max_stderr,none': 0.4526770929710024,\n",
       "    'rouge1_acc,none': 0.7552019583843329,\n",
       "    'rouge1_acc_stderr,none': 0.015051869486714997,\n",
       "    'rouge1_diff,none': 6.142060652436206,\n",
       "    'rouge1_diff_stderr,none': 0.40711663863611297,\n",
       "    'rouge2_max,none': 5.910590957117553,\n",
       "    'rouge2_max_stderr,none': 0.42450572204902,\n",
       "    'rouge2_acc,none': 0.16401468788249693,\n",
       "    'rouge2_acc_stderr,none': 0.01296270432749245,\n",
       "    'rouge2_diff,none': -0.21901844317246624,\n",
       "    'rouge2_diff_stderr,none': 0.40394408079116706,\n",
       "    'rougeL_max,none': 19.04484563255201,\n",
       "    'rougeL_max_stderr,none': 0.4342764929231771,\n",
       "    'rougeL_acc,none': 0.7711138310893513,\n",
       "    'rougeL_acc_stderr,none': 0.014706994909055025,\n",
       "    'rougeL_diff,none': 6.073648549933165,\n",
       "    'rougeL_diff_stderr,none': 0.3992033094252529},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4112607099143207,\n",
       "    'acc_stderr,none': 0.017225627083660867},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5915502303257693,\n",
       "    'acc_stderr,none': 0.01608313388616288}},\n",
       "  'llama3_8b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.26785714285714285,\n",
       "    'acc_stderr,none': 0.020945742941635495,\n",
       "    'acc_norm,none': 0.26785714285714285,\n",
       "    'acc_norm_stderr,none': 0.020945742941635495}},\n",
       "  'llama3_8b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4094165813715456,\n",
       "    'acc_stderr,none': 0.01112684957658903}},\n",
       "  'llama3_8b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7453754080522307,\n",
       "    'acc_stderr,none': 0.010164432237060476,\n",
       "    'acc_norm,none': 0.7241566920565833,\n",
       "    'acc_norm_stderr,none': 0.010427805502729112}},\n",
       "  'llama3_8b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.5659295659295659,\n",
       "    'acc_stderr,none': 0.014189966795335986}},\n",
       "  'llama3_8b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.008339651250947688,\n",
       "    'exact_match_stderr,strict-match': 0.0025049422268605234,\n",
       "    'exact_match,flexible-extract': 0.6300227445034117,\n",
       "    'exact_match_stderr,flexible-extract': 0.013298661207727127}},\n",
       "  'llama3_8b_gpqa_main_cot_n_shot': {'gpqa_main_cot_n_shot': {'alias': 'gpqa_main_cot_n_shot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.11607142857142858,\n",
       "    'exact_match_stderr,flexible-extract': 0.015150169411624755}},\n",
       "  'llama3_8b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.28125,\n",
       "    'acc_stderr,none': 0.021265785688273954,\n",
       "    'acc_norm,none': 0.28125,\n",
       "    'acc_norm_stderr,none': 0.021265785688273954}},\n",
       "  'llama3_8b_gpqa_main_cot_zeroshot': {'gpqa_main_cot_zeroshot': {'alias': 'gpqa_main_cot_zeroshot',\n",
       "    'exact_match,strict-match': 0.0,\n",
       "    'exact_match_stderr,strict-match': 0.0,\n",
       "    'exact_match,flexible-extract': 0.16071428571428573,\n",
       "    'exact_match_stderr,flexible-extract': 0.017371142987257344}},\n",
       "  'llama3_8b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.27638190954773867,\n",
       "    'acc_stderr,none': 0.008186723166234528,\n",
       "    'acc_norm,none': 0.2961474036850921,\n",
       "    'acc_norm_stderr,none': 0.008357866191239753}},\n",
       "  'llama3_8b_mmlu': {'mmlu': {'acc,none': 0.41368750890186584,\n",
       "    'acc_stderr,none': 0.004029940792732843,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3936238044633369,\n",
       "    'acc_stderr,none': 0.006938446040644653,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.25396825396825395,\n",
       "    'acc_stderr,none': 0.038932596106046755},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.5454545454545454,\n",
       "    'acc_stderr,none': 0.038881769216741},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.03509312031717982},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.5780590717299579,\n",
       "    'acc_stderr,none': 0.032148146302403695},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5454545454545454,\n",
       "    'acc_stderr,none': 0.045454545454545484},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.5277777777777778,\n",
       "    'acc_stderr,none': 0.04826217294139894},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.5705521472392638,\n",
       "    'acc_stderr,none': 0.03889066619112723},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.41329479768786126,\n",
       "    'acc_stderr,none': 0.02651126136940924},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.2748603351955307,\n",
       "    'acc_stderr,none': 0.014931316703220508},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.5241157556270096,\n",
       "    'acc_stderr,none': 0.02836504154256457},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.43209876543209874,\n",
       "    'acc_stderr,none': 0.02756301097160667},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.3155149934810952,\n",
       "    'acc_stderr,none': 0.011869184843058649},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.5789473684210527,\n",
       "    'acc_stderr,none': 0.03786720706234214},\n",
       "   'mmlu_other': {'acc,none': 0.497907949790795,\n",
       "    'acc_stderr,none': 0.00859493168505472,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.46,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.4867924528301887,\n",
       "    'acc_stderr,none': 0.030762134874500482},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3179190751445087,\n",
       "    'acc_stderr,none': 0.0355068398916558},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5336322869955157,\n",
       "    'acc_stderr,none': 0.033481800170603065},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.5145631067961165,\n",
       "    'acc_stderr,none': 0.04948637324026637},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.6324786324786325,\n",
       "    'acc_stderr,none': 0.03158539157745636},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.45,\n",
       "    'acc_stderr,none': 0.04999999999999999},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.70242656449553,\n",
       "    'acc_stderr,none': 0.016349111912909425},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.39215686274509803,\n",
       "    'acc_stderr,none': 0.02795604616542452},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.3617021276595745,\n",
       "    'acc_stderr,none': 0.0286638201471995},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.3235294117647059,\n",
       "    'acc_stderr,none': 0.028418208619406794},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3674698795180723,\n",
       "    'acc_stderr,none': 0.03753267402120575},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.4442638934026649,\n",
       "    'acc_stderr,none': 0.008863866185939465,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.3684210526315789,\n",
       "    'acc_stderr,none': 0.04537815354939392},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.4292929292929293,\n",
       "    'acc_stderr,none': 0.035265527246011986},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.47668393782383417,\n",
       "    'acc_stderr,none': 0.03604513672442206},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.3384615384615385,\n",
       "    'acc_stderr,none': 0.02399150050031303},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.3865546218487395,\n",
       "    'acc_stderr,none': 0.0316314580755238},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.5596330275229358,\n",
       "    'acc_stderr,none': 0.02128431062376155},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.42748091603053434,\n",
       "    'acc_stderr,none': 0.04338920305792401},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.45751633986928103,\n",
       "    'acc_stderr,none': 0.020154685712590888},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.04789131426105757},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.3183673469387755,\n",
       "    'acc_stderr,none': 0.029822533793982066},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.48258706467661694,\n",
       "    'acc_stderr,none': 0.03533389234739244},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.53,\n",
       "    'acc_stderr,none': 0.05016135580465919},\n",
       "   'mmlu_stem': {'acc,none': 0.33079606723755156,\n",
       "    'acc_stderr,none': 0.008267122485389458,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.25,\n",
       "    'acc_stderr,none': 0.04351941398892446},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.4962962962962963,\n",
       "    'acc_stderr,none': 0.04319223625811331},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.45394736842105265,\n",
       "    'acc_stderr,none': 0.04051646342874143},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.375,\n",
       "    'acc_stderr,none': 0.04048439222695598},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.04408440022768078},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.24509803921568626,\n",
       "    'acc_stderr,none': 0.042801058373643945},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.55,\n",
       "    'acc_stderr,none': 0.049999999999999996},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3702127659574468,\n",
       "    'acc_stderr,none': 0.031565646822367836},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.3103448275862069,\n",
       "    'acc_stderr,none': 0.03855289616378948},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2962962962962963,\n",
       "    'acc_stderr,none': 0.023517294335963286},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.4096774193548387,\n",
       "    'acc_stderr,none': 0.027976054915347357},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3448275862068966,\n",
       "    'acc_stderr,none': 0.03344283744280458},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.04923659639173309},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.26296296296296295,\n",
       "    'acc_stderr,none': 0.02684205787383371},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2185430463576159,\n",
       "    'acc_stderr,none': 0.033742355504256936},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.26851851851851855,\n",
       "    'acc_stderr,none': 0.030225226160012404},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.30357142857142855,\n",
       "    'acc_stderr,none': 0.04364226155841044}}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_8b_results/llama3_8b_whole_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(model_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
