{
    "llama3_8b_openness_low": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7693144722524483,
                "acc_stderr,none": 0.009828959550983084,
                "acc_norm,none": 0.7709466811751904,
                "acc_norm_stderr,none": 0.009804509865175504
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.6060606060606061,
                "acc_stderr,none": 0.013989198052984349
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2827470686767169,
                "acc_stderr,none": 0.008243958788826996,
                "acc_norm,none": 0.2998324958123953,
                "acc_norm_stderr,none": 0.008387661895516164
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.11902956785443518,
                "exact_match_stderr,strict-match": 0.00891970291116162,
                "exact_match,flexible-extract": 0.5837755875663382,
                "exact_match_stderr,flexible-extract": 0.013577788334652665
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.29062811565304086,
                "acc_stderr,none": 0.003788729507250455,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.28522848034006376,
                "acc_stderr,none": 0.006580575290749833,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.24603174603174602,
                "acc_stderr,none": 0.03852273364924316
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2787878787878788,
                "acc_stderr,none": 0.0350143870629678
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.28431372549019607,
                "acc_stderr,none": 0.031660096793998116
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.31645569620253167,
                "acc_stderr,none": 0.030274974880218974
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2975206611570248,
                "acc_stderr,none": 0.04173349148083499
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.044143436668549335
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.3128834355828221,
                "acc_stderr,none": 0.03642914578292404
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.31213872832369943,
                "acc_stderr,none": 0.02494679222527231
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.25921787709497207,
                "acc_stderr,none": 0.01465578083749771
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.3183279742765273,
                "acc_stderr,none": 0.026457225067811025
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.29012345679012347,
                "acc_stderr,none": 0.025251173936495026
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.27183833116036504,
                "acc_stderr,none": 0.011363135278651426
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3684210526315789,
                "acc_stderr,none": 0.036996580176568775
            },
            "mmlu_other": {
                "acc,none": 0.3601544898616028,
                "acc_stderr,none": 0.00838255196098877,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.34,
                "acc_stderr,none": 0.047609522856952365
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.35471698113207545,
                "acc_stderr,none": 0.02944517532819959
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.23121387283236994,
                "acc_stderr,none": 0.03214737302029469
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3901345291479821,
                "acc_stderr,none": 0.03273766725459156
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.33980582524271846,
                "acc_stderr,none": 0.04689765937278134
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.33760683760683763,
                "acc_stderr,none": 0.030980296992618558
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.5402298850574713,
                "acc_stderr,none": 0.01782199409693354
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.024954184324879905
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2624113475177305,
                "acc_stderr,none": 0.026244920349843014
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.02576725201085595
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3313253012048193,
                "acc_stderr,none": 0.03664314777288085
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2759181020474488,
                "acc_stderr,none": 0.00804189719093279,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.04372748290278007
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.25252525252525254,
                "acc_stderr,none": 0.030954055470365904
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.23316062176165803,
                "acc_stderr,none": 0.03051611137147601
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.23333333333333334,
                "acc_stderr,none": 0.021444547301560465
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.24789915966386555,
                "acc_stderr,none": 0.028047967224176892
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.26605504587155965,
                "acc_stderr,none": 0.018946022322225586
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2900763358778626,
                "acc_stderr,none": 0.03980066246467765
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.29901960784313725,
                "acc_stderr,none": 0.018521756215423027
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.4090909090909091,
                "acc_stderr,none": 0.04709306978661895
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.24489795918367346,
                "acc_stderr,none": 0.027529637440174927
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.31343283582089554,
                "acc_stderr,none": 0.03280188205348642
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.34,
                "acc_stderr,none": 0.04760952285695236
            },
            "mmlu_stem": {
                "acc,none": 0.2445290199809705,
                "acc_stderr,none": 0.007644873923120952,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720683
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.03785714465066654
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.21710526315789475,
                "acc_stderr,none": 0.03355045304882923
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.03476590104304134
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.17,
                "acc_stderr,none": 0.0377525168068637
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.20588235294117646,
                "acc_stderr,none": 0.04023382273617746
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.33191489361702126,
                "acc_stderr,none": 0.03078373675774567
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.25132275132275134,
                "acc_stderr,none": 0.022340482339643895
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25483870967741934,
                "acc_stderr,none": 0.02479011845933221
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.270935960591133,
                "acc_stderr,none": 0.031270907132976984
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.02684205787383371
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.03257847384436776
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1712962962962963,
                "acc_stderr,none": 0.025695341643824685
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.26785714285714285,
                "acc_stderr,none": 0.04203277291467762
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 7.851150269689111,
                "bleu_max_stderr,none": 0.43643587765181274,
                "bleu_acc,none": 0.5263157894736842,
                "bleu_acc_stderr,none": 0.01747924116197545,
                "bleu_diff,none": 0.20981918435033942,
                "bleu_diff_stderr,none": 0.299948742245442,
                "rouge1_max,none": 23.52649714301993,
                "rouge1_max_stderr,none": 0.6724455915216483,
                "rouge1_acc,none": 0.6609547123623011,
                "rouge1_acc_stderr,none": 0.016571797910626625,
                "rouge1_diff,none": 3.554605578821476,
                "rouge1_diff_stderr,none": 0.6491558361633821,
                "rouge2_max,none": 9.262198205539631,
                "rouge2_max_stderr,none": 0.6510855440188075,
                "rouge2_acc,none": 0.1689106487148103,
                "rouge2_acc_stderr,none": 0.01311617777567378,
                "rouge2_diff,none": -2.2099581439172056,
                "rouge2_diff_stderr,none": 0.5783805647019511,
                "rougeL_max,none": 22.211010370449234,
                "rougeL_max_stderr,none": 0.6429453906345687,
                "rougeL_acc,none": 0.6719706242350061,
                "rougeL_acc_stderr,none": 0.016435632932815046,
                "rougeL_diff,none": 3.441981658558931,
                "rougeL_diff_stderr,none": 0.6460378603371444
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.34516523867809057,
                "acc_stderr,none": 0.01664310331927494
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5149211439526642,
                "acc_stderr,none": 0.015899421104769818
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.28794642857142855,
                "acc_stderr,none": 0.02141698936957183,
                "acc_norm,none": 0.28794642857142855,
                "acc_norm_stderr,none": 0.02141698936957183
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4227226202661208,
                "acc_stderr,none": 0.011178123214465787
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.265625,
                "acc_stderr,none": 0.02089005840079951,
                "acc_norm,none": 0.265625,
                "acc_norm_stderr,none": 0.02089005840079951
            }
        }
    },
    "llama3_8b_agreeableness_high": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7687704026115343,
                "acc_stderr,none": 0.009837063180625326,
                "acc_norm,none": 0.7519042437431991,
                "acc_norm_stderr,none": 0.010077118315574712
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.5536445536445537,
                "acc_stderr,none": 0.014232330485071302
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2723618090452261,
                "acc_stderr,none": 0.008149508900211346,
                "acc_norm,none": 0.2891122278056951,
                "acc_norm_stderr,none": 0.00829916392351651
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.039423805913570885,
                "exact_match_stderr,strict-match": 0.0053602800303424285,
                "exact_match,flexible-extract": 0.2926459438968916,
                "exact_match_stderr,flexible-extract": 0.012532334368242897
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2767412049565589,
                "acc_stderr,none": 0.003741068541299341,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2641870350690754,
                "acc_stderr,none": 0.006426877565951713,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.1984126984126984,
                "acc_stderr,none": 0.035670166752768656
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2727272727272727,
                "acc_stderr,none": 0.0347769116216366
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.030587591351604246
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.26582278481012656,
                "acc_stderr,none": 0.028756799629658342
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2727272727272727,
                "acc_stderr,none": 0.04065578140908706
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.26993865030674846,
                "acc_stderr,none": 0.034878251684978906
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.315028901734104,
                "acc_stderr,none": 0.025009313790069695
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2536312849162011,
                "acc_stderr,none": 0.014551553659369923
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.31189710610932475,
                "acc_stderr,none": 0.02631185807185416
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2716049382716049,
                "acc_stderr,none": 0.02474862449053737
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24902216427640156,
                "acc_stderr,none": 0.01104489226404077
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.26900584795321636,
                "acc_stderr,none": 0.03401052620104089
            },
            "mmlu_other": {
                "acc,none": 0.3437399420663019,
                "acc_stderr,none": 0.00835535463878957,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.33,
                "acc_stderr,none": 0.047258156262526045
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.3471698113207547,
                "acc_stderr,none": 0.029300101705549652
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0326926380614177
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.4304932735426009,
                "acc_stderr,none": 0.0332319730294294
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.3592233009708738,
                "acc_stderr,none": 0.04750458399041692
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.3547008547008547,
                "acc_stderr,none": 0.03134250486245403
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.4789272030651341,
                "acc_stderr,none": 0.017864076786212907
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.238562091503268,
                "acc_stderr,none": 0.024404394928087873
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.026011992930902
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.20220588235294118,
                "acc_stderr,none": 0.024398192986654924
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3192771084337349,
                "acc_stderr,none": 0.036293353299478595
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2638934026649334,
                "acc_stderr,none": 0.007923443936990111,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2807017543859649,
                "acc_stderr,none": 0.04227054451232199
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.0298575156733864
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.21243523316062177,
                "acc_stderr,none": 0.029519282616817247
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2230769230769231,
                "acc_stderr,none": 0.021107730127244
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23109243697478993,
                "acc_stderr,none": 0.027381406927868973
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.26788990825688075,
                "acc_stderr,none": 0.018987462257978652
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.29770992366412213,
                "acc_stderr,none": 0.04010358942462203
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.3055555555555556,
                "acc_stderr,none": 0.018635594034423976
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.36363636363636365,
                "acc_stderr,none": 0.046075820907199756
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2,
                "acc_stderr,none": 0.025607375986579153
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.31840796019900497,
                "acc_stderr,none": 0.032941184790540944
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_stem": {
                "acc,none": 0.2419917538851887,
                "acc_stderr,none": 0.007617492915710377,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.03785714465066653
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.21710526315789475,
                "acc_stderr,none": 0.033550453048829226
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.03476590104304134
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.15,
                "acc_stderr,none": 0.0358870281282637
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.19607843137254902,
                "acc_stderr,none": 0.03950581861179964
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3191489361702128,
                "acc_stderr,none": 0.030472973363380056
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.22758620689655173,
                "acc_stderr,none": 0.03493950380131184
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503154
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25483870967741934,
                "acc_stderr,none": 0.02479011845933221
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.270935960591133,
                "acc_stderr,none": 0.031270907132976984
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909283
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.02684205787383371
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.03257847384436776
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16666666666666666,
                "acc_stderr,none": 0.02541642838876747
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.2767857142857143,
                "acc_stderr,none": 0.04246624336697625
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 4.60573365727217,
                "bleu_max_stderr,none": 0.24772631220318217,
                "bleu_acc,none": 0.4810281517747858,
                "bleu_acc_stderr,none": 0.017490896405762364,
                "bleu_diff,none": -0.01638041024135537,
                "bleu_diff_stderr,none": 0.14264609614962714,
                "rouge1_max,none": 18.141284151705165,
                "rouge1_max_stderr,none": 0.4376265201828342,
                "rouge1_acc,none": 0.5091799265605875,
                "rouge1_acc_stderr,none": 0.01750055072481975,
                "rouge1_diff,none": 0.6569319326296889,
                "rouge1_diff_stderr,none": 0.2808388599477109,
                "rouge2_max,none": 8.614117460565785,
                "rouge2_max_stderr,none": 0.41015154994577396,
                "rouge2_acc,none": 0.30966952264381886,
                "rouge2_acc_stderr,none": 0.016185744355144926,
                "rouge2_diff,none": -0.6996991640978596,
                "rouge2_diff_stderr,none": 0.2540353612300885,
                "rougeL_max,none": 15.981449256178895,
                "rougeL_max_stderr,none": 0.4217093683626164,
                "rougeL_acc,none": 0.47980416156670747,
                "rougeL_acc_stderr,none": 0.017489216849737064,
                "rougeL_diff,none": 0.33602970191925674,
                "rougeL_diff_stderr,none": 0.2699450533475428
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3463892288861689,
                "acc_stderr,none": 0.01665699710912514
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.4915960173771838,
                "acc_stderr,none": 0.01614540213841082
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.29017857142857145,
                "acc_stderr,none": 0.021466115440571216,
                "acc_norm,none": 0.29017857142857145,
                "acc_norm_stderr,none": 0.021466115440571216
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.41760491299897645,
                "acc_stderr,none": 0.01115939189492249
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.27901785714285715,
                "acc_stderr,none": 0.021214094157265974,
                "acc_norm,none": 0.27901785714285715,
                "acc_norm_stderr,none": 0.021214094157265974
            }
        }
    },
    "llama3_8b_neuroticism_low": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.766050054406964,
                "acc_stderr,none": 0.009877236895137462,
                "acc_norm,none": 0.766050054406964,
                "acc_norm_stderr,none": 0.009877236895137462
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.33906633906633904,
                "acc_stderr,none": 0.013553184756047191
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2814070351758794,
                "acc_stderr,none": 0.00823207932032532,
                "acc_norm,none": 0.29413735343383585,
                "acc_norm_stderr,none": 0.008341339176593604
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.06368460955269144,
                "exact_match_stderr,strict-match": 0.006726213078805723,
                "exact_match,flexible-extract": 0.24109173616376042,
                "exact_match_stderr,flexible-extract": 0.011782246325099716
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2344395385272753,
                "acc_stderr,none": 0.0035696482517447135,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24250797024442083,
                "acc_stderr,none": 0.00624639127117202,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2147239263803681,
                "acc_stderr,none": 0.03226219377286774
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2514450867052023,
                "acc_stderr,none": 0.023357365785874037
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23575418994413408,
                "acc_stderr,none": 0.014196375686290804
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.18971061093247588,
                "acc_stderr,none": 0.022268196258783225
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24771838331160365,
                "acc_stderr,none": 0.011025499291443738
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.24460894753781784,
                "acc_stderr,none": 0.0076936295737235864,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.025447863825108604
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2515964240102171,
                "acc_stderr,none": 0.01551732236552963
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22875816993464052,
                "acc_stderr,none": 0.024051029739912258
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.18382352941176472,
                "acc_stderr,none": 0.02352924218519311
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.29518072289156627,
                "acc_stderr,none": 0.0355092018568963
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22489437764055897,
                "acc_stderr,none": 0.007521148313637619,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.20707070707070707,
                "acc_stderr,none": 0.02886977846026704
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.20207253886010362,
                "acc_stderr,none": 0.02897908979429673
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20512820512820512,
                "acc_stderr,none": 0.020473233173551965
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.02665353159671549
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1981651376146789,
                "acc_stderr,none": 0.017090573804217905
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2900763358778626,
                "acc_stderr,none": 0.03980066246467765
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.26143790849673204,
                "acc_stderr,none": 0.017776947157528023
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24875621890547264,
                "acc_stderr,none": 0.03056767593891672
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.22169362511893434,
                "acc_stderr,none": 0.0073819087832080665,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.18421052631578946,
                "acc_stderr,none": 0.0315469804508223
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2916666666666667,
                "acc_stderr,none": 0.03800968060554858
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25161290322580643,
                "acc_stderr,none": 0.02468597928623997
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1527777777777778,
                "acc_stderr,none": 0.02453632602613422
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 10.52471365129671,
                "bleu_max_stderr,none": 0.5244050535900594,
                "bleu_acc,none": 0.4369645042839657,
                "bleu_acc_stderr,none": 0.01736384450319596,
                "bleu_diff,none": -0.6337400630587428,
                "bleu_diff_stderr,none": 0.4309591326328966,
                "rouge1_max,none": 28.866847601365585,
                "rouge1_max_stderr,none": 0.7146057031778461,
                "rouge1_acc,none": 0.4810281517747858,
                "rouge1_acc_stderr,none": 0.017490896405762357,
                "rouge1_diff,none": -1.0828813949844414,
                "rouge1_diff_stderr,none": 0.505667646132068,
                "rouge2_max,none": 17.121073653282764,
                "rouge2_max_stderr,none": 0.6980395409469036,
                "rouge2_acc,none": 0.35495716034271724,
                "rouge2_acc_stderr,none": 0.0167508623813759,
                "rouge2_diff,none": -2.3646509036251135,
                "rouge2_diff_stderr,none": 0.6198400696464698,
                "rougeL_max,none": 26.312932609312753,
                "rougeL_max_stderr,none": 0.7000686845110243,
                "rougeL_acc,none": 0.4504283965728274,
                "rougeL_acc_stderr,none": 0.017417264371967642,
                "rougeL_diff,none": -1.5086630980898443,
                "rougeL_diff_stderr,none": 0.5124643668847236
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.38922888616891066,
                "acc_stderr,none": 0.01706855268069033
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.545668973526556,
                "acc_stderr,none": 0.016103046839825885
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.28794642857142855,
                "acc_stderr,none": 0.02141698936957184,
                "acc_norm,none": 0.28794642857142855,
                "acc_norm_stderr,none": 0.02141698936957184
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4211873080859775,
                "acc_stderr,none": 0.011172633149198382
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.29017857142857145,
                "acc_stderr,none": 0.021466115440571216,
                "acc_norm,none": 0.29017857142857145,
                "acc_norm_stderr,none": 0.021466115440571216
            }
        }
    },
    "llama3_8b_conscientiousness_high": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7665941240478781,
                "acc_stderr,none": 0.009869247889521015,
                "acc_norm,none": 0.7529923830250272,
                "acc_norm_stderr,none": 0.01006226814077261
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.38001638001638,
                "acc_stderr,none": 0.013896689396282174
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27872696817420434,
                "acc_stderr,none": 0.008208048863665954,
                "acc_norm,none": 0.29346733668341707,
                "acc_norm_stderr,none": 0.008335786794439961
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.07050796057619409,
                "exact_match_stderr,strict-match": 0.0070515438139836135,
                "exact_match,flexible-extract": 0.23426838514025777,
                "exact_match_stderr,flexible-extract": 0.011666415127631034
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.23180458624127617,
                "acc_stderr,none": 0.003554597379508395,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24378320935175346,
                "acc_stderr,none": 0.0062578204563901434,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2147239263803681,
                "acc_stderr,none": 0.03226219377286774
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24566473988439305,
                "acc_stderr,none": 0.02317629820399201
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24581005586592178,
                "acc_stderr,none": 0.014400296429225598
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.18971061093247588,
                "acc_stderr,none": 0.022268196258783225
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2470664928292047,
                "acc_stderr,none": 0.011015752255279329
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.2417122626327647,
                "acc_stderr,none": 0.007661751769439958,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2188679245283019,
                "acc_stderr,none": 0.025447863825108604
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.32286995515695066,
                "acc_stderr,none": 0.03138147637575499
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.18446601941747573,
                "acc_stderr,none": 0.03840423627288276
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23754789272030652,
                "acc_stderr,none": 0.015218733046150195
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22875816993464052,
                "acc_stderr,none": 0.024051029739912258
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.18382352941176472,
                "acc_stderr,none": 0.02352924218519311
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22229444263893403,
                "acc_stderr,none": 0.007485759331629514,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20512820512820512,
                "acc_stderr,none": 0.020473233173551965
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1944954128440367,
                "acc_stderr,none": 0.01697028909045803
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.29770992366412213,
                "acc_stderr,none": 0.04010358942462203
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.26143790849673204,
                "acc_stderr,none": 0.017776947157528023
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.040139645540727735
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.19183673469387755,
                "acc_stderr,none": 0.025206963154225395
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.2134475103076435,
                "acc_stderr,none": 0.007281856728242435,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329265
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1574074074074074,
                "acc_stderr,none": 0.024837173518242384
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 5.522382605316912,
                "bleu_max_stderr,none": 0.26084928525317447,
                "bleu_acc,none": 0.41615667074663404,
                "bleu_acc_stderr,none": 0.01725565750290305,
                "bleu_diff,none": -0.18881444995765603,
                "bleu_diff_stderr,none": 0.1599375450032442,
                "rouge1_max,none": 19.963289314636697,
                "rouge1_max_stderr,none": 0.49288512114920546,
                "rouge1_acc,none": 0.4969400244798042,
                "rouge1_acc_stderr,none": 0.017503173260960632,
                "rouge1_diff,none": -0.19454507226160983,
                "rouge1_diff_stderr,none": 0.30215565920589293,
                "rouge2_max,none": 10.346117633108303,
                "rouge2_max_stderr,none": 0.44541340083188,
                "rouge2_acc,none": 0.3072215422276622,
                "rouge2_acc_stderr,none": 0.01615020132132303,
                "rouge2_diff,none": -1.0018893235371125,
                "rouge2_diff_stderr,none": 0.30541956594013375,
                "rougeL_max,none": 17.739678599779783,
                "rougeL_max_stderr,none": 0.46668067462123247,
                "rougeL_acc,none": 0.45532435740514077,
                "rougeL_acc_stderr,none": 0.017433490102538755,
                "rougeL_diff,none": -0.3706652337649607,
                "rougeL_diff_stderr,none": 0.2865604420505172
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.36474908200734396,
                "acc_stderr,none": 0.01685096106172012
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5061365919795262,
                "acc_stderr,none": 0.01612599057908278
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.021367228699836042,
                "acc_norm,none": 0.2857142857142857,
                "acc_norm_stderr,none": 0.021367228699836042
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.41146366427840325,
                "acc_stderr,none": 0.01113528311654019
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.28794642857142855,
                "acc_stderr,none": 0.021416989369571836,
                "acc_norm,none": 0.28794642857142855,
                "acc_norm_stderr,none": 0.021416989369571836
            }
        }
    },
    "llama3_8b_agreeableness_low": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.750272034820457,
                "acc_stderr,none": 0.010099232969867498,
                "acc_norm,none": 0.7442872687704026,
                "acc_norm_stderr,none": 0.010178690109459858
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.3628173628173628,
                "acc_stderr,none": 0.0137656296877885
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2807370184254606,
                "acc_stderr,none": 0.008226105720127505,
                "acc_norm,none": 0.2988274706867672,
                "acc_norm_stderr,none": 0.008379600162134914
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.058377558756633814,
                "exact_match_stderr,strict-match": 0.006458083557832464,
                "exact_match,flexible-extract": 0.7164518574677786,
                "exact_match_stderr,flexible-extract": 0.012415070917508132
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.25452214784218774,
                "acc_stderr,none": 0.003669019711804243,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.25802337938363445,
                "acc_stderr,none": 0.006381927105139073,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.03718489006818115
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.24848484848484848,
                "acc_stderr,none": 0.03374402644139405
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.03019028245350195
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.27848101265822783,
                "acc_stderr,none": 0.029178682304842544
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.256198347107438,
                "acc_stderr,none": 0.03984979653302872
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.28703703703703703,
                "acc_stderr,none": 0.043733130409147614
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2883435582822086,
                "acc_stderr,none": 0.035590395316173425
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2658959537572254,
                "acc_stderr,none": 0.023786203255508283
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574882
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2861736334405145,
                "acc_stderr,none": 0.025670259242188943
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.02438366553103545
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2646675358539765,
                "acc_stderr,none": 0.01126733299284553
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.21052631578947367,
                "acc_stderr,none": 0.031267817146631786
            },
            "mmlu_other": {
                "acc,none": 0.2777598970067589,
                "acc_stderr,none": 0.008005904267737021,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206824
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2792452830188679,
                "acc_stderr,none": 0.027611163402399715
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.21965317919075145,
                "acc_stderr,none": 0.031568093627031744
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621505
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.38565022421524664,
                "acc_stderr,none": 0.03266842214289201
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.2524271844660194,
                "acc_stderr,none": 0.04301250399690878
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2564102564102564,
                "acc_stderr,none": 0.028605953702004264
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.30779054916985954,
                "acc_stderr,none": 0.01650604504515563
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.23202614379084968,
                "acc_stderr,none": 0.02417084087934102
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.026011992930902
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.19852941176470587,
                "acc_stderr,none": 0.02423101337054107
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3253012048192771,
                "acc_stderr,none": 0.03647168523683227
            },
            "mmlu_social_sciences": {
                "acc,none": 0.24081897952551187,
                "acc_stderr,none": 0.007696865578320351,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.30701754385964913,
                "acc_stderr,none": 0.0433913832257986
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.21717171717171718,
                "acc_stderr,none": 0.029376616484945637
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.20725388601036268,
                "acc_stderr,none": 0.029252823291803627
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2205128205128205,
                "acc_stderr,none": 0.021020672680827912
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.027553614467863814
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23669724770642203,
                "acc_stderr,none": 0.01822407811729907
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.24427480916030533,
                "acc_stderr,none": 0.037683359597287434
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.272875816993464,
                "acc_stderr,none": 0.018020474148393577
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.33636363636363636,
                "acc_stderr,none": 0.04525393596302505
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.17551020408163265,
                "acc_stderr,none": 0.02435280072297001
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.25870646766169153,
                "acc_stderr,none": 0.03096590312357303
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.21,
                "acc_stderr,none": 0.04093601807403326
            },
            "mmlu_stem": {
                "acc,none": 0.23977164605137963,
                "acc_stderr,none": 0.007592259079775671,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.29,
                "acc_stderr,none": 0.04560480215720683
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2518518518518518,
                "acc_stderr,none": 0.03749850709174021
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.18421052631578946,
                "acc_stderr,none": 0.0315469804508223
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.22916666666666666,
                "acc_stderr,none": 0.035146974678623884
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.18,
                "acc_stderr,none": 0.03861229196653695
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.20588235294117646,
                "acc_stderr,none": 0.04023382273617746
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.32340425531914896,
                "acc_stderr,none": 0.03057944277361035
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2206896551724138,
                "acc_stderr,none": 0.03455930201924811
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.25396825396825395,
                "acc_stderr,none": 0.022418042891113946
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25806451612903225,
                "acc_stderr,none": 0.024892469172462833
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.270935960591133,
                "acc_stderr,none": 0.031270907132976984
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.042295258468165044
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.026719240783712166
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2119205298013245,
                "acc_stderr,none": 0.03336767086567977
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16203703703703703,
                "acc_stderr,none": 0.02513045365226846
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.22321428571428573,
                "acc_stderr,none": 0.039523019677025116
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 6.194335547392274,
                "bleu_max_stderr,none": 0.27424010445580427,
                "bleu_acc,none": 0.4430844553243574,
                "bleu_acc_stderr,none": 0.01738973034687712,
                "bleu_diff,none": -0.5318507769294739,
                "bleu_diff_stderr,none": 0.17302183767426832,
                "rouge1_max,none": 22.389629785535504,
                "rouge1_max_stderr,none": 0.49703804438514576,
                "rouge1_acc,none": 0.4602203182374541,
                "rouge1_acc_stderr,none": 0.017448017223960867,
                "rouge1_diff,none": -0.4808791683253484,
                "rouge1_diff_stderr,none": 0.323961048876279,
                "rouge2_max,none": 12.46154910641313,
                "rouge2_max_stderr,none": 0.49679503953868004,
                "rouge2_acc,none": 0.3378212974296206,
                "rouge2_acc_stderr,none": 0.016557167322516872,
                "rouge2_diff,none": -1.2408448043685356,
                "rouge2_diff_stderr,none": 0.33947954785583195,
                "rougeL_max,none": 20.099663057888176,
                "rougeL_max_stderr,none": 0.48488171903325444,
                "rougeL_acc,none": 0.44430844553243576,
                "rougeL_acc_stderr,none": 0.017394586250743183,
                "rougeL_diff,none": -0.8156343003230504,
                "rougeL_diff_stderr,none": 0.3154732085156177
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.34149326805385555,
                "acc_stderr,none": 0.016600688619950826
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.502848210329337,
                "acc_stderr,none": 0.015820056989929488
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.27232142857142855,
                "acc_stderr,none": 0.02105508212932417,
                "acc_norm,none": 0.27232142857142855,
                "acc_norm_stderr,none": 0.02105508212932417
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.394575230296827,
                "acc_stderr,none": 0.011059713589720794
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.02136722869983605,
                "acc_norm,none": 0.2857142857142857,
                "acc_norm_stderr,none": 0.02136722869983605
            }
        }
    },
    "llama3_8b_openness_high": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7671381936887922,
                "acc_stderr,none": 0.009861236071080751,
                "acc_norm,none": 0.7519042437431991,
                "acc_norm_stderr,none": 0.010077118315574717
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.6461916461916462,
                "acc_stderr,none": 0.0136894120442716
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27604690117252934,
                "acc_stderr,none": 0.008183653718163576,
                "acc_norm,none": 0.2884422110552764,
                "acc_norm_stderr,none": 0.00829344725702754
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.03866565579984837,
                "exact_match_stderr,strict-match": 0.005310583162098012,
                "exact_match,flexible-extract": 0.13495072024260804,
                "exact_match_stderr,flexible-extract": 0.00941131528257115
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.3751602335849594,
                "acc_stderr,none": 0.003957702673872231,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3708820403825717,
                "acc_stderr,none": 0.0068509360318628025,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.49696969696969695,
                "acc_stderr,none": 0.03904272341431857
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.5,
                "acc_stderr,none": 0.03509312031717982
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.5822784810126582,
                "acc_stderr,none": 0.032103530322412685
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.512396694214876,
                "acc_stderr,none": 0.045629515481807666
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.5,
                "acc_stderr,none": 0.04833682445228318
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.4110429447852761,
                "acc_stderr,none": 0.038656978537853624
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.3699421965317919,
                "acc_stderr,none": 0.025992472029306376
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2916201117318436,
                "acc_stderr,none": 0.015201032512520418
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.45980707395498394,
                "acc_stderr,none": 0.028306190403305693
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.4444444444444444,
                "acc_stderr,none": 0.027648477877413324
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.27053455019556716,
                "acc_stderr,none": 0.01134599674353926
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.6608187134502924,
                "acc_stderr,none": 0.03631053496488904
            },
            "mmlu_other": {
                "acc,none": 0.4280656581911812,
                "acc_stderr,none": 0.008593438673880738,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.49,
                "acc_stderr,none": 0.05024183937956912
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.37735849056603776,
                "acc_stderr,none": 0.029832808114796005
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.31213872832369943,
                "acc_stderr,none": 0.035331333893236574
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.21,
                "acc_stderr,none": 0.04093601807403326
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.5246636771300448,
                "acc_stderr,none": 0.03351695167652628
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.47572815533980584,
                "acc_stderr,none": 0.049449010929737795
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.6111111111111112,
                "acc_stderr,none": 0.031937057262002924
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.42,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.5568326947637292,
                "acc_stderr,none": 0.017764085035348404
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.3790849673202614,
                "acc_stderr,none": 0.02778014120702335
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2765957446808511,
                "acc_stderr,none": 0.02668456434046099
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.21691176470588236,
                "acc_stderr,none": 0.02503584522771125
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.39759036144578314,
                "acc_stderr,none": 0.038099730845402184
            },
            "mmlu_social_sciences": {
                "acc,none": 0.43971400714982123,
                "acc_stderr,none": 0.008784085192056673,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.24561403508771928,
                "acc_stderr,none": 0.04049339297748139
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.42424242424242425,
                "acc_stderr,none": 0.03521224908841583
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.5233160621761658,
                "acc_stderr,none": 0.03604513672442202
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.3128205128205128,
                "acc_stderr,none": 0.023507579020645347
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.3445378151260504,
                "acc_stderr,none": 0.03086868260412163
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.5082568807339449,
                "acc_stderr,none": 0.021434399918214327
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.4351145038167939,
                "acc_stderr,none": 0.043482080516448585
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.4264705882352941,
                "acc_stderr,none": 0.020007912739359358
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.5818181818181818,
                "acc_stderr,none": 0.04724577405731572
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.3673469387755102,
                "acc_stderr,none": 0.030862144921087558
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.6368159203980099,
                "acc_stderr,none": 0.034005985055990146
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.59,
                "acc_stderr,none": 0.04943110704237102
            },
            "mmlu_stem": {
                "acc,none": 0.2664129400570885,
                "acc_stderr,none": 0.007806588310505543,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.3111111111111111,
                "acc_stderr,none": 0.039992628766177214
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.35526315789473684,
                "acc_stderr,none": 0.038947344870133176
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.04020151261036846
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.04280105837364395
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.46,
                "acc_stderr,none": 0.05009082659620333
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3574468085106383,
                "acc_stderr,none": 0.03132941789476425
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.30344827586206896,
                "acc_stderr,none": 0.038312260488503336
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.23015873015873015,
                "acc_stderr,none": 0.021679219663693145
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.3032258064516129,
                "acc_stderr,none": 0.02614868593067174
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.24630541871921183,
                "acc_stderr,none": 0.030315099285617736
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621504
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.16556291390728478,
                "acc_stderr,none": 0.030348183410303615
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.18055555555555555,
                "acc_stderr,none": 0.026232878971491652
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.32142857142857145,
                "acc_stderr,none": 0.04432804055291519
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 4.193313030497953,
                "bleu_max_stderr,none": 0.20303427902131937,
                "bleu_acc,none": 0.4455324357405141,
                "bleu_acc_stderr,none": 0.017399335280140347,
                "bleu_diff,none": -0.03534214924139452,
                "bleu_diff_stderr,none": 0.11871604494251138,
                "rouge1_max,none": 16.987317332402974,
                "rouge1_max_stderr,none": 0.38276980061699784,
                "rouge1_acc,none": 0.5104039167686658,
                "rouge1_acc_stderr,none": 0.017499711430249264,
                "rouge1_diff,none": 0.466232344754611,
                "rouge1_diff_stderr,none": 0.23838062382443417,
                "rouge2_max,none": 8.0032291028385,
                "rouge2_max_stderr,none": 0.3629521785130386,
                "rouge2_acc,none": 0.2962056303549572,
                "rouge2_acc_stderr,none": 0.015983595101811385,
                "rouge2_diff,none": -0.6024803535138669,
                "rouge2_diff_stderr,none": 0.23957295355708122,
                "rougeL_max,none": 15.039514980468358,
                "rougeL_max_stderr,none": 0.3661035873221252,
                "rougeL_acc,none": 0.4908200734394125,
                "rougeL_acc_stderr,none": 0.017500550724819767,
                "rougeL_diff,none": 0.26312467921097416,
                "rougeL_diff_stderr,none": 0.2233787198981842
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3463892288861689,
                "acc_stderr,none": 0.01665699710912514
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.489611831379221,
                "acc_stderr,none": 0.016099737718727258
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.29017857142857145,
                "acc_stderr,none": 0.021466115440571216,
                "acc_norm,none": 0.29017857142857145,
                "acc_norm_stderr,none": 0.021466115440571216
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.41914022517911975,
                "acc_stderr,none": 0.011165140708170325
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.296875,
                "acc_stderr,none": 0.021609729061250887,
                "acc_norm,none": 0.296875,
                "acc_norm_stderr,none": 0.021609729061250887
            }
        }
    },
    "llama3_8b_extraversion_low": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7758433079434167,
                "acc_stderr,none": 0.009729897956410034,
                "acc_norm,none": 0.764417845484222,
                "acc_norm_stderr,none": 0.009901067586473912
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.5495495495495496,
                "acc_stderr,none": 0.01424449396472249
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2777219430485762,
                "acc_stderr,none": 0.008198943594859155,
                "acc_norm,none": 0.29380234505862646,
                "acc_norm_stderr,none": 0.008338565702892806
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.1288855193328279,
                "exact_match_stderr,strict-match": 0.009229580761400251,
                "exact_match,flexible-extract": 0.5708870356330553,
                "exact_match_stderr,flexible-extract": 0.013633369425647237
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2923372738926079,
                "acc_stderr,none": 0.003802685442157055,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.28522848034006376,
                "acc_stderr,none": 0.006553414191708073,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.28484848484848485,
                "acc_stderr,none": 0.035243908445117836
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.3088235294117647,
                "acc_stderr,none": 0.03242661719827218
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.35443037974683544,
                "acc_stderr,none": 0.031137304297185798
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.371900826446281,
                "acc_stderr,none": 0.04412015806624504
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.37037037037037035,
                "acc_stderr,none": 0.046684080330249324
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.3128834355828221,
                "acc_stderr,none": 0.036429145782924055
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.30057803468208094,
                "acc_stderr,none": 0.024685316867257803
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2424581005586592,
                "acc_stderr,none": 0.014333522059217887
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.27009646302250806,
                "acc_stderr,none": 0.02521804037341062
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.30246913580246915,
                "acc_stderr,none": 0.025557653981868045
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.25749674054758803,
                "acc_stderr,none": 0.011167706014904156
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.45614035087719296,
                "acc_stderr,none": 0.038200425866029654
            },
            "mmlu_other": {
                "acc,none": 0.33762471837785646,
                "acc_stderr,none": 0.008356615174733524,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.4,
                "acc_stderr,none": 0.04923659639173309
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.32075471698113206,
                "acc_stderr,none": 0.02872750295788027
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.23699421965317918,
                "acc_stderr,none": 0.03242414757483098
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.2,
                "acc_stderr,none": 0.04020151261036846
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.4349775784753363,
                "acc_stderr,none": 0.033272833702713445
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.32038834951456313,
                "acc_stderr,none": 0.04620284082280039
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.47435897435897434,
                "acc_stderr,none": 0.03271298896811159
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.39,
                "acc_stderr,none": 0.04902071300001975
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.4061302681992337,
                "acc_stderr,none": 0.017562037406478916
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.27450980392156865,
                "acc_stderr,none": 0.025553169991826528
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2624113475177305,
                "acc_stderr,none": 0.026244920349843007
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1875,
                "acc_stderr,none": 0.023709788253811766
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3373493975903614,
                "acc_stderr,none": 0.03680783690727581
            },
            "mmlu_social_sciences": {
                "acc,none": 0.30744231394215144,
                "acc_stderr,none": 0.0082732447329355,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.03999423879281338
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.24242424242424243,
                "acc_stderr,none": 0.03053289223393202
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.26424870466321243,
                "acc_stderr,none": 0.031821550509166484
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.26153846153846155,
                "acc_stderr,none": 0.022282141204204423
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.25630252100840334,
                "acc_stderr,none": 0.02835962087053395
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3211009174311927,
                "acc_stderr,none": 0.020018149772733747
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.37404580152671757,
                "acc_stderr,none": 0.04243869242230524
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.3431372549019608,
                "acc_stderr,none": 0.01920660684882536
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.42727272727272725,
                "acc_stderr,none": 0.04738198703545483
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.23673469387755103,
                "acc_stderr,none": 0.02721283588407315
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.3880597014925373,
                "acc_stderr,none": 0.03445789964362749
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.4,
                "acc_stderr,none": 0.049236596391733084
            },
            "mmlu_stem": {
                "acc,none": 0.24357754519505234,
                "acc_stderr,none": 0.0076266973141871945,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.26666666666666666,
                "acc_stderr,none": 0.03820169914517905
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.27631578947368424,
                "acc_stderr,none": 0.03639057569952924
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2847222222222222,
                "acc_stderr,none": 0.03773809990686934
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.04220773659171452
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.37,
                "acc_stderr,none": 0.048523658709391
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.28085106382978725,
                "acc_stderr,none": 0.029379170464124815
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2482758620689655,
                "acc_stderr,none": 0.03600105692727771
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.24603174603174602,
                "acc_stderr,none": 0.02218203720294836
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.2645161290322581,
                "acc_stderr,none": 0.025091892378859275
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.18719211822660098,
                "acc_stderr,none": 0.027444924966882618
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.18543046357615894,
                "acc_stderr,none": 0.031732843842942844
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16666666666666666,
                "acc_stderr,none": 0.025416428388767478
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 5.55911501361266,
                "bleu_max_stderr,none": 0.2952359349820275,
                "bleu_acc,none": 0.5006119951040392,
                "bleu_acc_stderr,none": 0.01750348793889251,
                "bleu_diff,none": -0.23075731873073993,
                "bleu_diff_stderr,none": 0.1898654624599789,
                "rouge1_max,none": 20.559910811705652,
                "rouge1_max_stderr,none": 0.5336826898203896,
                "rouge1_acc,none": 0.5899632802937577,
                "rouge1_acc_stderr,none": 0.017217844717449325,
                "rouge1_diff,none": 0.8495968997076364,
                "rouge1_diff_stderr,none": 0.33612888666034174,
                "rouge2_max,none": 10.141499938284461,
                "rouge2_max_stderr,none": 0.5261109719802546,
                "rouge2_acc,none": 0.27539779681762544,
                "rouge2_acc_stderr,none": 0.01563813566777552,
                "rouge2_diff,none": -1.1569428797778394,
                "rouge2_diff_stderr,none": 0.3522748118564545,
                "rougeL_max,none": 18.374593441090457,
                "rougeL_max_stderr,none": 0.5230775790674042,
                "rougeL_acc,none": 0.5679314565483476,
                "rougeL_acc_stderr,none": 0.017341202394988327,
                "rougeL_diff,none": 0.3563373005065132,
                "rougeL_diff_stderr,none": 0.32767711902236624
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3623011015911873,
                "acc_stderr,none": 0.016826646897262258
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5194183975217875,
                "acc_stderr,none": 0.016050449416359115
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.2924107142857143,
                "acc_stderr,none": 0.021514611259928533,
                "acc_norm,none": 0.2924107142857143,
                "acc_norm_stderr,none": 0.021514611259928533
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.41606960081883315,
                "acc_stderr,none": 0.011153531906320223
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.265625,
                "acc_stderr,none": 0.02089005840079951,
                "acc_norm,none": 0.265625,
                "acc_norm_stderr,none": 0.02089005840079951
            }
        }
    },
    "llama3_8b_conscientiousness_low": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7383025027203483,
                "acc_stderr,none": 0.010255630772708232,
                "acc_norm,none": 0.7372143634385201,
                "acc_norm_stderr,none": 0.010269354068140762
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.31285831285831284,
                "acc_stderr,none": 0.013274466706393652
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2733668341708543,
                "acc_stderr,none": 0.0081588906125507,
                "acc_norm,none": 0.2891122278056951,
                "acc_norm_stderr,none": 0.00829916392351651
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.030326004548900682,
                "exact_match_stderr,strict-match": 0.004723487465514771,
                "exact_match,flexible-extract": 0.6103108415466262,
                "exact_match_stderr,flexible-extract": 0.0134331232361107
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2695484973650477,
                "acc_stderr,none": 0.0037086037677228987,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24229543039319873,
                "acc_stderr,none": 0.00623823423409431,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.3253968253968254,
                "acc_stderr,none": 0.04190596438871136
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2545454545454545,
                "acc_stderr,none": 0.03401506715249039
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.030587591351604246
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.20253164556962025,
                "acc_stderr,none": 0.026160568246601464
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.14049586776859505,
                "acc_stderr,none": 0.031722334260021585
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.03957835471980981
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2331288343558282,
                "acc_stderr,none": 0.0332201579577674
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.022075709251757177
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.27262569832402234,
                "acc_stderr,none": 0.014893391735249603
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.24758842443729903,
                "acc_stderr,none": 0.024513879973621967
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.22530864197530864,
                "acc_stderr,none": 0.023246202647819746
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24837027379400262,
                "acc_stderr,none": 0.01103521259803451
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.17543859649122806,
                "acc_stderr,none": 0.029170885500727686
            },
            "mmlu_other": {
                "acc,none": 0.2520115867396202,
                "acc_stderr,none": 0.0076614913382074944,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2981132075471698,
                "acc_stderr,none": 0.028152837942493875
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.3352601156069364,
                "acc_stderr,none": 0.03599586301247078
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.10762331838565023,
                "acc_stderr,none": 0.020799400082880008
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.3786407766990291,
                "acc_stderr,none": 0.048026946982589726
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.19658119658119658,
                "acc_stderr,none": 0.02603538609895129
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.20434227330779056,
                "acc_stderr,none": 0.0144191239809319
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2973856209150327,
                "acc_stderr,none": 0.02617390850671858
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.026011992930902013
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.4338235294117647,
                "acc_stderr,none": 0.03010563657001663
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.1927710843373494,
                "acc_stderr,none": 0.03070982405056527
            },
            "mmlu_social_sciences": {
                "acc,none": 0.311667208319792,
                "acc_stderr,none": 0.008281012058331852,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.22807017543859648,
                "acc_stderr,none": 0.03947152782669415
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.35353535353535354,
                "acc_stderr,none": 0.03406086723547153
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.36787564766839376,
                "acc_stderr,none": 0.034801756684660366
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.36666666666666664,
                "acc_stderr,none": 0.024433016466052455
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.35294117647058826,
                "acc_stderr,none": 0.031041941304059278
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3522935779816514,
                "acc_stderr,none": 0.020480568843999
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2748091603053435,
                "acc_stderr,none": 0.03915345408847836
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2173202614379085,
                "acc_stderr,none": 0.016684820929148594
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.04013964554072775
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.40408163265306124,
                "acc_stderr,none": 0.03141470802586589
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.26865671641791045,
                "acc_stderr,none": 0.03134328358208954
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_stem": {
                "acc,none": 0.28639391056137015,
                "acc_stderr,none": 0.007989928496684258,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.23703703703703705,
                "acc_stderr,none": 0.03673731683969506
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.3355263157894737,
                "acc_stderr,none": 0.03842498559395269
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.38,
                "acc_stderr,none": 0.048783173121456316
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.37254901960784315,
                "acc_stderr,none": 0.04810840148082633
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.20851063829787234,
                "acc_stderr,none": 0.026556982117838728
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2671957671957672,
                "acc_stderr,none": 0.02278967314577657
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.3193548387096774,
                "acc_stderr,none": 0.02652270967466776
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.33004926108374383,
                "acc_stderr,none": 0.033085304262282574
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036625
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2518518518518518,
                "acc_stderr,none": 0.02646611753895991
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.31788079470198677,
                "acc_stderr,none": 0.038020397601079024
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.4398148148148148,
                "acc_stderr,none": 0.033851779760448106
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.20535714285714285,
                "acc_stderr,none": 0.038342410214190714
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 2.499404711163944,
                "bleu_max_stderr,none": 0.10892599905288441,
                "bleu_acc,none": 0.45532435740514077,
                "bleu_acc_stderr,none": 0.01743349010253876,
                "bleu_diff,none": -0.17339940584161176,
                "bleu_diff_stderr,none": 0.08971536308975675,
                "rouge1_max,none": 13.154028628135336,
                "rouge1_max_stderr,none": 0.2701355456346904,
                "rouge1_acc,none": 0.6413708690330477,
                "rouge1_acc_stderr,none": 0.016789289499502022,
                "rouge1_diff,none": 1.8779781665517195,
                "rouge1_diff_stderr,none": 0.23897842282907286,
                "rouge2_max,none": 3.601965793807205,
                "rouge2_max_stderr,none": 0.23873454098072933,
                "rouge2_acc,none": 0.1481028151774786,
                "rouge2_acc_stderr,none": 0.012434552750319303,
                "rouge2_diff,none": -0.756604871931151,
                "rouge2_diff_stderr,none": 0.18577744801835325,
                "rougeL_max,none": 11.959368510892514,
                "rougeL_max_stderr,none": 0.2512613532111202,
                "rougeL_acc,none": 0.6242350061199511,
                "rougeL_acc_stderr,none": 0.016954584060214304,
                "rougeL_diff,none": 1.6696611173277758,
                "rougeL_diff_stderr,none": 0.23450612266726015
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.2913096695226438,
                "acc_stderr,none": 0.015905987048184828
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.4435493763773696,
                "acc_stderr,none": 0.01569004154625148
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.22991071428571427,
                "acc_stderr,none": 0.01990198453013952,
                "acc_norm,none": 0.22991071428571427,
                "acc_norm_stderr,none": 0.01990198453013952
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.3925281473899693,
                "acc_stderr,none": 0.011049620449690383
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.26785714285714285,
                "acc_stderr,none": 0.020945742941635495,
                "acc_norm,none": 0.26785714285714285,
                "acc_norm_stderr,none": 0.020945742941635495
            }
        }
    },
    "llama3_8b_neuroticism_high": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7606093579978237,
                "acc_stderr,none": 0.009955884250291711,
                "acc_norm,none": 0.7415669205658324,
                "acc_norm_stderr,none": 0.01021397163677332
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.2334152334152334,
                "acc_stderr,none": 0.012110575321206386
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2592964824120603,
                "acc_stderr,none": 0.00802271023810577,
                "acc_norm,none": 0.2827470686767169,
                "acc_norm_stderr,none": 0.008243958788826986
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.03639120545868082,
                "exact_match_stderr,strict-match": 0.00515811348923119,
                "exact_match,flexible-extract": 0.3191811978771797,
                "exact_match_stderr,flexible-extract": 0.012840345676251655
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.23785785500640935,
                "acc_stderr,none": 0.0035858960037814126,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2507970244420829,
                "acc_stderr,none": 0.006319548605058453,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.23809523809523808,
                "acc_stderr,none": 0.038095238095238106
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2545454545454545,
                "acc_stderr,none": 0.0340150671524904
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.029771775228145635
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.29535864978902954,
                "acc_stderr,none": 0.02969633871342289
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.26380368098159507,
                "acc_stderr,none": 0.03462419931615624
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0230836585869842
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23910614525139665,
                "acc_stderr,none": 0.014265554192331154
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.22508038585209003,
                "acc_stderr,none": 0.023720088516179027
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445806
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.258148631029987,
                "acc_stderr,none": 0.01117692371931341
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.32748538011695905,
                "acc_stderr,none": 0.03599335771456027
            },
            "mmlu_other": {
                "acc,none": 0.2481493401995494,
                "acc_stderr,none": 0.007720162203253166,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.20754716981132076,
                "acc_stderr,none": 0.02495991802891127
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.19653179190751446,
                "acc_stderr,none": 0.030299574664788137
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.17,
                "acc_stderr,none": 0.0377525168068637
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3452914798206278,
                "acc_stderr,none": 0.03191100192835795
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.1650485436893204,
                "acc_stderr,none": 0.036756688322331886
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674054
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.26053639846743293,
                "acc_stderr,none": 0.01569600856380708
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.02380518652488815
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.026011992930902013
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.19117647058823528,
                "acc_stderr,none": 0.023886881922440355
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.2346441338966526,
                "acc_stderr,none": 0.007633074592646007,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2543859649122807,
                "acc_stderr,none": 0.040969851398436695
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19170984455958548,
                "acc_stderr,none": 0.02840895362624526
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.02665353159671549
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23302752293577983,
                "acc_stderr,none": 0.018125669180861486
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2824427480916031,
                "acc_stderr,none": 0.03948406125768362
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2581699346405229,
                "acc_stderr,none": 0.01770453165325007
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.04013964554072773
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.23265306122448978,
                "acc_stderr,none": 0.02704925791589618
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.2885572139303483,
                "acc_stderr,none": 0.03203841040213322
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.29,
                "acc_stderr,none": 0.045604802157206845
            },
            "mmlu_stem": {
                "acc,none": 0.21154456073580716,
                "acc_stderr,none": 0.007261723827872149,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.041633319989322695
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03621034121889507
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.21164021164021163,
                "acc_stderr,none": 0.021037331505262886
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.18387096774193548,
                "acc_stderr,none": 0.022037217340267846
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.21481481481481482,
                "acc_stderr,none": 0.025040443877000683
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.14351851851851852,
                "acc_stderr,none": 0.02391077925264438
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 2.0768543157690122,
                "bleu_max_stderr,none": 0.08434536666730479,
                "bleu_acc,none": 0.4920440636474908,
                "bleu_acc_stderr,none": 0.017501285074551818,
                "bleu_diff,none": -0.07837059349254583,
                "bleu_diff_stderr,none": 0.05324686206081628,
                "rouge1_max,none": 13.106302394552989,
                "rouge1_max_stderr,none": 0.22938408486558753,
                "rouge1_acc,none": 0.598531211750306,
                "rouge1_acc_stderr,none": 0.01716027390169365,
                "rouge1_diff,none": 1.8945981211259821,
                "rouge1_diff_stderr,none": 0.21273943238310514,
                "rouge2_max,none": 5.246732695208819,
                "rouge2_max_stderr,none": 0.2370353664949951,
                "rouge2_acc,none": 0.24112607099143207,
                "rouge2_acc_stderr,none": 0.014974827279752329,
                "rouge2_diff,none": -0.6224623012152453,
                "rouge2_diff_stderr,none": 0.16008351644998076,
                "rougeL_max,none": 10.81663753055861,
                "rougeL_max_stderr,none": 0.2301405534834514,
                "rougeL_acc,none": 0.5483476132190942,
                "rougeL_acc_stderr,none": 0.017421480300277643,
                "rougeL_diff,none": 0.6258303332554928,
                "rougeL_diff_stderr,none": 0.18563734418354744
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3072215422276622,
                "acc_stderr,none": 0.01615020132132303
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.45159332663131363,
                "acc_stderr,none": 0.015789874922580768
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.28348214285714285,
                "acc_stderr,none": 0.02131682898726215,
                "acc_norm,none": 0.28348214285714285,
                "acc_norm_stderr,none": 0.02131682898726215
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.39355168884339814,
                "acc_stderr,none": 0.011054692433938036
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.25223214285714285,
                "acc_stderr,none": 0.02054139101648798,
                "acc_norm,none": 0.25223214285714285,
                "acc_norm_stderr,none": 0.02054139101648798
            }
        }
    },
    "llama3_8b_extraversion_high": {
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7578890097932536,
                "acc_stderr,none": 0.009994371269104364,
                "acc_norm,none": 0.7475516866158868,
                "acc_norm_stderr,none": 0.010135665547362366
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.4594594594594595,
                "acc_stderr,none": 0.014267826484806907
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2710217755443886,
                "acc_stderr,none": 0.008136918413120765,
                "acc_norm,none": 0.2897822445561139,
                "acc_norm_stderr,none": 0.008304858539875106
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.06292645943896892,
                "exact_match_stderr,strict-match": 0.00668876258153276,
                "exact_match,flexible-extract": 0.3995451099317665,
                "exact_match_stderr,flexible-extract": 0.01349166029881599
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.24732944025067655,
                "acc_stderr,none": 0.0036350233089757563,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.2450584484590861,
                "acc_stderr,none": 0.0062746677253196805,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.1984126984126984,
                "acc_stderr,none": 0.03567016675276865
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.24242424242424243,
                "acc_stderr,none": 0.033464098810559534
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.02977177522814563
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.2616033755274262,
                "acc_stderr,none": 0.028609516716994934
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.24793388429752067,
                "acc_stderr,none": 0.03941897526516302
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.24539877300613497,
                "acc_stderr,none": 0.03380939813943354
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24566473988439305,
                "acc_stderr,none": 0.023176298203992005
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2424581005586592,
                "acc_stderr,none": 0.014333522059217887
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2733118971061093,
                "acc_stderr,none": 0.02531176597542612
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2654320987654321,
                "acc_stderr,none": 0.024569223600460845
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2392438070404172,
                "acc_stderr,none": 0.010896123652676669
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.21052631578947367,
                "acc_stderr,none": 0.031267817146631786
            },
            "mmlu_other": {
                "acc,none": 0.272288381074992,
                "acc_stderr,none": 0.007961594420239672,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.27169811320754716,
                "acc_stderr,none": 0.02737770662467071
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749867
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.37668161434977576,
                "acc_stderr,none": 0.032521134899291884
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.2621359223300971,
                "acc_stderr,none": 0.043546310772605956
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2564102564102564,
                "acc_stderr,none": 0.028605953702004264
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2988505747126437,
                "acc_stderr,none": 0.016369256815093124
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22875816993464052,
                "acc_stderr,none": 0.024051029739912255
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2553191489361702,
                "acc_stderr,none": 0.026011992930902
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.20220588235294118,
                "acc_stderr,none": 0.024398192986654924
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3192771084337349,
                "acc_stderr,none": 0.03629335329947859
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23431914202144946,
                "acc_stderr,none": 0.007628545315653108,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2807017543859649,
                "acc_stderr,none": 0.04227054451232199
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.21717171717171718,
                "acc_stderr,none": 0.029376616484945637
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.20725388601036268,
                "acc_stderr,none": 0.029252823291803627
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2205128205128205,
                "acc_stderr,none": 0.021020672680827912
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23109243697478993,
                "acc_stderr,none": 0.027381406927868973
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.23669724770642203,
                "acc_stderr,none": 0.01822407811729907
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.22900763358778625,
                "acc_stderr,none": 0.036853466317118506
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.2565359477124183,
                "acc_stderr,none": 0.017667841612379
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.34545454545454546,
                "acc_stderr,none": 0.04554619617541054
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.17142857142857143,
                "acc_stderr,none": 0.02412746346265016
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.23880597014925373,
                "acc_stderr,none": 0.030147775935409217
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.21,
                "acc_stderr,none": 0.04093601807403326
            },
            "mmlu_stem": {
                "acc,none": 0.23882017126546146,
                "acc_stderr,none": 0.007578131187194976,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2518518518518518,
                "acc_stderr,none": 0.03749850709174021
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.18421052631578946,
                "acc_stderr,none": 0.0315469804508223
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.03476590104304134
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.15,
                "acc_stderr,none": 0.0358870281282637
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.19607843137254902,
                "acc_stderr,none": 0.03950581861179964
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.32340425531914896,
                "acc_stderr,none": 0.03057944277361035
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2206896551724138,
                "acc_stderr,none": 0.03455930201924811
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503154
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25483870967741934,
                "acc_stderr,none": 0.02479011845933221
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.270935960591133,
                "acc_stderr,none": 0.031270907132976984
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.23,
                "acc_stderr,none": 0.042295258468165044
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.02684205787383371
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.03257847384436776
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16203703703703703,
                "acc_stderr,none": 0.02513045365226846
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04287858751340456
            }
        },
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 2.599631465622022,
                "bleu_max_stderr,none": 0.06589186933929682,
                "bleu_acc,none": 0.4908200734394125,
                "bleu_acc_stderr,none": 0.017500550724819756,
                "bleu_diff,none": 0.8988938663772197,
                "bleu_diff_stderr,none": 0.07110081325809892,
                "rouge1_max,none": 12.59304128858336,
                "rouge1_max_stderr,none": 0.2627444392602265,
                "rouge1_acc,none": 0.6009791921664627,
                "rouge1_acc_stderr,none": 0.017142825728496763,
                "rouge1_diff,none": 4.726146845270564,
                "rouge1_diff_stderr,none": 0.2973160537103582,
                "rouge2_max,none": 2.396111106334669,
                "rouge2_max_stderr,none": 0.19915952185445188,
                "rouge2_acc,none": 0.09791921664626684,
                "rouge2_acc_stderr,none": 0.010404269701032628,
                "rouge2_diff,none": -0.31479976514095703,
                "rouge2_diff_stderr,none": 0.1148963799860598,
                "rougeL_max,none": 11.76543920801899,
                "rougeL_max_stderr,none": 0.24706476111438433,
                "rougeL_acc,none": 0.6083231334149327,
                "rougeL_acc_stderr,none": 0.017087795881769625,
                "rougeL_diff,none": 4.629400785164695,
                "rougeL_diff_stderr,none": 0.28954102495059525
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3182374541003672,
                "acc_stderr,none": 0.016305988648920616
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.45286187459796673,
                "acc_stderr,none": 0.01583982163812737
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.02136722869983604,
                "acc_norm,none": 0.2857142857142857,
                "acc_norm_stderr,none": 0.02136722869983604
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4150460593654043,
                "acc_stderr,none": 0.0111495633967375
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.28348214285714285,
                "acc_stderr,none": 0.021316828987262136,
                "acc_norm,none": 0.28348214285714285,
                "acc_norm_stderr,none": 0.021316828987262136
            }
        }
    }
}