{
    "llama3_8b": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 5.3380196416969925,
                "bleu_max_stderr,none": 0.4092051328468082,
                "bleu_acc,none": 0.3488372093023256,
                "bleu_acc_stderr,none": 0.01668441985998691,
                "bleu_diff,none": 0.2552742332414611,
                "bleu_diff_stderr,none": 0.31370770260516684,
                "rouge1_max,none": 17.218678410935823,
                "rouge1_max_stderr,none": 0.6472401069019209,
                "rouge1_acc,none": 0.4283965728274174,
                "rouge1_acc_stderr,none": 0.01732308859731474,
                "rouge1_diff,none": 1.4485256899291867,
                "rouge1_diff_stderr,none": 0.49627920101175216,
                "rouge2_max,none": 6.2388167559037395,
                "rouge2_max_stderr,none": 0.5701500336183569,
                "rouge2_acc,none": 0.0966952264381885,
                "rouge2_acc_stderr,none": 0.010346050422310914,
                "rouge2_diff,none": -0.9786642095386358,
                "rouge2_diff_stderr,none": 0.49279170264406413,
                "rougeL_max,none": 16.151552430820328,
                "rougeL_max_stderr,none": 0.6111764248517502,
                "rougeL_acc,none": 0.41982864137086906,
                "rougeL_acc_stderr,none": 0.01727703030177577,
                "rougeL_diff,none": 1.2620630752485404,
                "rougeL_diff_stderr,none": 0.48857104372239585
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3684210526315789,
                "acc_stderr,none": 0.016886551261046046
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.534938756833991,
                "acc_stderr,none": 0.01592298890607095
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.28125,
                "acc_stderr,none": 0.021265785688273954,
                "acc_norm,none": 0.28125,
                "acc_norm_stderr,none": 0.021265785688273954
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4969293756397134,
                "acc_stderr,none": 0.011313857198301221
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7818280739934712,
                "acc_stderr,none": 0.009636081958374381,
                "acc_norm,none": 0.7682263329706203,
                "acc_norm_stderr,none": 0.009845143772794029
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.5176085176085176,
                "acc_stderr,none": 0.01430607861484495
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.15238817285822592,
                "exact_match_stderr,strict-match": 0.00989957225479421,
                "exact_match,flexible-extract": 0.6467020470053071,
                "exact_match_stderr,flexible-extract": 0.013166337192115686
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.15178571428571427,
                "exact_match_stderr,flexible-extract": 0.01697127532581282
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.29910714285714285,
                "acc_stderr,none": 0.021656359273376974,
                "acc_norm,none": 0.29910714285714285,
                "acc_norm_stderr,none": 0.021656359273376974
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.125,
                "exact_match_stderr,flexible-extract": 0.015642467864593956
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27872696817420434,
                "acc_stderr,none": 0.008208048863665954,
                "acc_norm,none": 0.29547738693467335,
                "acc_norm_stderr,none": 0.008352378831993173
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.5122489673835636,
                "acc_stderr,none": 0.003966375641672073,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.4682252922422954,
                "acc_stderr,none": 0.007012287737408272,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.5151515151515151,
                "acc_stderr,none": 0.03902551007374449
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.6715686274509803,
                "acc_stderr,none": 0.032962451101722294
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.6329113924050633,
                "acc_stderr,none": 0.031376240725616185
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.743801652892562,
                "acc_stderr,none": 0.03984979653302872
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.5185185185185185,
                "acc_stderr,none": 0.04830366024635331
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.5828220858895705,
                "acc_stderr,none": 0.038741028598180814
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.48265895953757226,
                "acc_stderr,none": 0.02690290045866664
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.3229050279329609,
                "acc_stderr,none": 0.015638440380241488
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.5305466237942122,
                "acc_stderr,none": 0.028345045864840622
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.6358024691358025,
                "acc_stderr,none": 0.026774929899722313
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.3878748370273794,
                "acc_stderr,none": 0.012444998309675617
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.7719298245614035,
                "acc_stderr,none": 0.03218093795602357
            },
            "mmlu_other": {
                "acc,none": 0.6414547795300933,
                "acc_stderr,none": 0.008195331703656971,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.58,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.7018867924528301,
                "acc_stderr,none": 0.028152837942493878
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.49710982658959535,
                "acc_stderr,none": 0.038124005659748335
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.44,
                "acc_stderr,none": 0.04988876515698589
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.6547085201793722,
                "acc_stderr,none": 0.03191100192835794
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.8058252427184466,
                "acc_stderr,none": 0.039166677628225836
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.8376068376068376,
                "acc_stderr,none": 0.02416161812798774
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.73,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.7828863346104725,
                "acc_stderr,none": 0.014743125394823297
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.6274509803921569,
                "acc_stderr,none": 0.027684181883302895
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.4326241134751773,
                "acc_stderr,none": 0.029555454236778845
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.3897058823529412,
                "acc_stderr,none": 0.0296246635811597
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.5301204819277109,
                "acc_stderr,none": 0.03885425420866766
            },
            "mmlu_social_sciences": {
                "acc,none": 0.6090347741306468,
                "acc_stderr,none": 0.008619034127420865,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.04372748290278008
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.6515151515151515,
                "acc_stderr,none": 0.033948539651564025
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.8186528497409327,
                "acc_stderr,none": 0.027807032360686088
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.5307692307692308,
                "acc_stderr,none": 0.025302958890850154
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.5504201680672269,
                "acc_stderr,none": 0.03231293497137707
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.6293577981651376,
                "acc_stderr,none": 0.020707458164352984
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.6717557251908397,
                "acc_stderr,none": 0.04118438565806298
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.5751633986928104,
                "acc_stderr,none": 0.019997973035458333
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.6090909090909091,
                "acc_stderr,none": 0.04673752333670237
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.5387755102040817,
                "acc_stderr,none": 0.03191282052669277
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.7711442786069652,
                "acc_stderr,none": 0.029705284056772436
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.76,
                "acc_stderr,none": 0.04292346959909281
            },
            "mmlu_stem": {
                "acc,none": 0.3561687281953695,
                "acc_stderr,none": 0.008158121926467055,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.6296296296296297,
                "acc_stderr,none": 0.04171654161354543
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.5723684210526315,
                "acc_stderr,none": 0.04026097083296563
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.4930555555555556,
                "acc_stderr,none": 0.04180806750294938
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.27450980392156865,
                "acc_stderr,none": 0.044405219061793275
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.72,
                "acc_stderr,none": 0.04512608598542129
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.4553191489361702,
                "acc_stderr,none": 0.03255525359340356
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.46206896551724136,
                "acc_stderr,none": 0.041546596717075474
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503154
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.44516129032258067,
                "acc_stderr,none": 0.02827241018621491
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.26108374384236455,
                "acc_stderr,none": 0.030903796952114482
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.43,
                "acc_stderr,none": 0.04975698519562428
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2074074074074074,
                "acc_stderr,none": 0.024720713193952165
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.027920963147993666
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.36607142857142855,
                "acc_stderr,none": 0.0457237235873743
            }
        }
    },
    "llama3_8b_o_low": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 10.69577926730629,
                "bleu_max_stderr,none": 0.6049028051144563,
                "bleu_acc,none": 0.49326805385556916,
                "bleu_acc_stderr,none": 0.017501914492655396,
                "bleu_diff,none": 0.03959123172346862,
                "bleu_diff_stderr,none": 0.4066439295613978,
                "rouge1_max,none": 27.806686860565733,
                "rouge1_max_stderr,none": 0.7994703608607435,
                "rouge1_acc,none": 0.5691554467564259,
                "rouge1_acc_stderr,none": 0.017335272475332366,
                "rouge1_diff,none": 0.13071517335795035,
                "rouge1_diff_stderr,none": 0.5104336724899193,
                "rouge2_max,none": 15.594675697191715,
                "rouge2_max_stderr,none": 0.7757948087176472,
                "rouge2_acc,none": 0.26438188494492043,
                "rouge2_acc_stderr,none": 0.015438211119522502,
                "rouge2_diff,none": -1.8104244773966431,
                "rouge2_diff_stderr,none": 0.5884680147381549,
                "rougeL_max,none": 25.444071145176544,
                "rougeL_max_stderr,none": 0.7704118845431382,
                "rougeL_acc,none": 0.5618115055079559,
                "rougeL_acc_stderr,none": 0.017369236164404424,
                "rougeL_diff,none": -0.12991472855038588,
                "rougeL_diff_stderr,none": 0.5112842160093753
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.34149326805385555,
                "acc_stderr,none": 0.016600688619950826
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.4911052063941109,
                "acc_stderr,none": 0.01605473397594555
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.25,
                "acc_stderr,none": 0.02048079801297601,
                "acc_norm,none": 0.25,
                "acc_norm_stderr,none": 0.02048079801297601
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.43807574206755373,
                "acc_stderr,none": 0.011226965068029933
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7230685527747551,
                "acc_stderr,none": 0.010440499969334526,
                "acc_norm,none": 0.733949945593036,
                "acc_norm_stderr,none": 0.01031003926335283
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.24815724815724816,
                "acc_stderr,none": 0.012366507794696467
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.002274450341167551,
                "exact_match_stderr,strict-match": 0.0013121578148674348,
                "exact_match,flexible-extract": 0.31766489764973466,
                "exact_match_stderr,flexible-extract": 0.012824066621488849
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.029017857142857144,
                "exact_match_stderr,flexible-extract": 0.007939342343352675
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.26339285714285715,
                "acc_stderr,none": 0.0208336900165786,
                "acc_norm,none": 0.26339285714285715,
                "acc_norm_stderr,none": 0.0208336900165786
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.06919642857142858,
                "exact_match_stderr,flexible-extract": 0.012003754338543692
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27839195979899495,
                "acc_stderr,none": 0.008205019480641219,
                "acc_norm,none": 0.2948073701842546,
                "acc_norm_stderr,none": 0.008346869841397377
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2977496083179034,
                "acc_stderr,none": 0.0038116075267708156,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.26950053134962804,
                "acc_stderr,none": 0.0064405516254948695,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.22424242424242424,
                "acc_stderr,none": 0.03256866661681102
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.030964517926923393
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.28270042194092826,
                "acc_stderr,none": 0.029312814153955914
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.36363636363636365,
                "acc_stderr,none": 0.043913262867240704
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.3425925925925926,
                "acc_stderr,none": 0.04587904741301809
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2822085889570552,
                "acc_stderr,none": 0.03536117886664743
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2630057803468208,
                "acc_stderr,none": 0.023703099525258165
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24804469273743016,
                "acc_stderr,none": 0.014444157808261462
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2379421221864952,
                "acc_stderr,none": 0.0241851506478187
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.30246913580246915,
                "acc_stderr,none": 0.02555765398186806
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24902216427640156,
                "acc_stderr,none": 0.01104489226404077
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.4678362573099415,
                "acc_stderr,none": 0.03826882417660371
            },
            "mmlu_other": {
                "acc,none": 0.3607981976182813,
                "acc_stderr,none": 0.008452096899662954,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.42,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.35094339622641507,
                "acc_stderr,none": 0.029373646253234686
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2658959537572254,
                "acc_stderr,none": 0.03368762932259431
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816507
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.37668161434977576,
                "acc_stderr,none": 0.032521134899291884
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.36893203883495146,
                "acc_stderr,none": 0.04777615181156739
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.5085470085470085,
                "acc_stderr,none": 0.0327513030009703
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.38,
                "acc_stderr,none": 0.048783173121456316
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.4648786717752235,
                "acc_stderr,none": 0.01783579880629064
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.28104575163398693,
                "acc_stderr,none": 0.02573885479781874
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2695035460992908,
                "acc_stderr,none": 0.026469036818590627
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.1875,
                "acc_stderr,none": 0.023709788253811766
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3674698795180723,
                "acc_stderr,none": 0.03753267402120575
            },
            "mmlu_social_sciences": {
                "acc,none": 0.32206694832629185,
                "acc_stderr,none": 0.008391549855449829,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.24561403508771928,
                "acc_stderr,none": 0.04049339297748139
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.30303030303030304,
                "acc_stderr,none": 0.03274287914026866
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.3626943005181347,
                "acc_stderr,none": 0.03469713791704372
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2948717948717949,
                "acc_stderr,none": 0.023119362758232297
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.28991596638655465,
                "acc_stderr,none": 0.029472485833136084
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3412844036697248,
                "acc_stderr,none": 0.020328612816592446
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.3893129770992366,
                "acc_stderr,none": 0.042764865428145914
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.31209150326797386,
                "acc_stderr,none": 0.018745011201277657
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.45454545454545453,
                "acc_stderr,none": 0.04769300568972744
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.22857142857142856,
                "acc_stderr,none": 0.026882144922307748
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.3681592039800995,
                "acc_stderr,none": 0.03410410565495301
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.41,
                "acc_stderr,none": 0.04943110704237102
            },
            "mmlu_stem": {
                "acc,none": 0.25404376784015226,
                "acc_stderr,none": 0.007703892104258955,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.35555555555555557,
                "acc_stderr,none": 0.04135176749720386
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.27631578947368424,
                "acc_stderr,none": 0.03639057569952925
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2916666666666667,
                "acc_stderr,none": 0.03800968060554858
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036623
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.42,
                "acc_stderr,none": 0.049604496374885836
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3276595744680851,
                "acc_stderr,none": 0.030683020843231008
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2827586206896552,
                "acc_stderr,none": 0.037528339580033376
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.23809523809523808,
                "acc_stderr,none": 0.021935878081184763
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.2806451612903226,
                "acc_stderr,none": 0.025560604721022884
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.18226600985221675,
                "acc_stderr,none": 0.02716334085964515
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.0446196043338474
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.21851851851851853,
                "acc_stderr,none": 0.02519575225182379
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.19205298013245034,
                "acc_stderr,none": 0.032162984205936135
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16203703703703703,
                "acc_stderr,none": 0.02513045365226846
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        }
    },
    "llama3_8b_a_low": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 4.467115018088212,
                "bleu_max_stderr,none": 0.17643300928881475,
                "bleu_acc,none": 0.390452876376989,
                "bleu_acc_stderr,none": 0.01707823074343147,
                "bleu_diff,none": -0.2744956151697697,
                "bleu_diff_stderr,none": 0.12037313411345534,
                "rouge1_max,none": 18.255926126570774,
                "rouge1_max_stderr,none": 0.3687961766997674,
                "rouge1_acc,none": 0.4589963280293758,
                "rouge1_acc_stderr,none": 0.017444544447661206,
                "rouge1_diff,none": -0.28504302343435967,
                "rouge1_diff_stderr,none": 0.24929506096463225,
                "rouge2_max,none": 8.587240092983258,
                "rouge2_max_stderr,none": 0.3513343775616956,
                "rouge2_acc,none": 0.2729498164014688,
                "rouge2_acc_stderr,none": 0.015594753632006535,
                "rouge2_diff,none": -0.8093540184000979,
                "rouge2_diff_stderr,none": 0.2317791766132367,
                "rougeL_max,none": 16.065244665959945,
                "rougeL_max_stderr,none": 0.3539626919279697,
                "rougeL_acc,none": 0.4259485924112607,
                "rougeL_acc_stderr,none": 0.017310471904076537,
                "rougeL_diff,none": -0.4526495566856119,
                "rougeL_diff_stderr,none": 0.23179034197895548
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.26560587515299877,
                "acc_stderr,none": 0.015461027627253597
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.45523405400796635,
                "acc_stderr,none": 0.016207570542677553
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.01940774926174222,
                "acc_norm,none": 0.21428571428571427,
                "acc_norm_stderr,none": 0.01940774926174222
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.38382804503582396,
                "acc_stderr,none": 0.01100444626612644
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.6855277475516867,
                "acc_stderr,none": 0.010833009065106576,
                "acc_norm,none": 0.676822633297062,
                "acc_norm_stderr,none": 0.01091197412428213
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.2628992628992629,
                "acc_stderr,none": 0.012603123489583002
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.6482183472327521,
                "exact_match_stderr,flexible-extract": 0.013153446023536042
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.12276785714285714,
                "exact_match_stderr,flexible-extract": 0.015521934425575739
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.24553571428571427,
                "acc_stderr,none": 0.02035742845448459,
                "acc_norm,none": 0.24553571428571427,
                "acc_norm_stderr,none": 0.02035742845448459
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.15178571428571427,
                "exact_match_stderr,flexible-extract": 0.016971275325812815
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27738693467336684,
                "acc_stderr,none": 0.008195897079410219,
                "acc_norm,none": 0.28442211055276384,
                "acc_norm_stderr,none": 0.008258681628795294
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.26335279874661727,
                "acc_stderr,none": 0.003713019252401684,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.27587672688629117,
                "acc_stderr,none": 0.00651440172228397,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.1984126984126984,
                "acc_stderr,none": 0.03567016675276865
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2787878787878788,
                "acc_stderr,none": 0.0350143870629678
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2647058823529412,
                "acc_stderr,none": 0.030964517926923403
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.28270042194092826,
                "acc_stderr,none": 0.02931281415395594
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2975206611570248,
                "acc_stderr,none": 0.04173349148083499
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.28703703703703703,
                "acc_stderr,none": 0.043733130409147614
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.294478527607362,
                "acc_stderr,none": 0.03581165790474082
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2976878612716763,
                "acc_stderr,none": 0.024617055388676982
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.30726256983240224,
                "acc_stderr,none": 0.015430158846469607
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.29260450160771706,
                "acc_stderr,none": 0.02583989833487798
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2623456790123457,
                "acc_stderr,none": 0.02447722285613511
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2607561929595828,
                "acc_stderr,none": 0.011213471559602325
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.21637426900584794,
                "acc_stderr,none": 0.031581495393387324
            },
            "mmlu_other": {
                "acc,none": 0.2790473125201159,
                "acc_stderr,none": 0.008035460708204684,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.27547169811320754,
                "acc_stderr,none": 0.02749566368372406
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.21965317919075145,
                "acc_stderr,none": 0.031568093627031744
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.34977578475336324,
                "acc_stderr,none": 0.03200736719484503
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.3106796116504854,
                "acc_stderr,none": 0.0458212416016155
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2564102564102564,
                "acc_stderr,none": 0.028605953702004264
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.2988505747126437,
                "acc_stderr,none": 0.016369256815093124
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.024288619466046105
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2801418439716312,
                "acc_stderr,none": 0.02678917235114024
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.21691176470588236,
                "acc_stderr,none": 0.025035845227711274
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3192771084337349,
                "acc_stderr,none": 0.036293353299478595
            },
            "mmlu_social_sciences": {
                "acc,none": 0.24926876828079297,
                "acc_stderr,none": 0.0078038793961050755,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.2807017543859649,
                "acc_stderr,none": 0.04227054451232199
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.029620227874790486
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.22279792746113988,
                "acc_stderr,none": 0.03003114797764154
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.23333333333333334,
                "acc_stderr,none": 0.021444547301560465
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.24789915966386555,
                "acc_stderr,none": 0.028047967224176896
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.24770642201834864,
                "acc_stderr,none": 0.018508143602547798
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2366412213740458,
                "acc_stderr,none": 0.03727673575596918
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.26143790849673204,
                "acc_stderr,none": 0.017776947157528044
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.33636363636363636,
                "acc_stderr,none": 0.04525393596302505
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2530612244897959,
                "acc_stderr,none": 0.02783302387139969
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.03036049015401465
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909284
            },
            "mmlu_stem": {
                "acc,none": 0.24294322867110688,
                "acc_stderr,none": 0.007631263747022526,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768079
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2518518518518518,
                "acc_stderr,none": 0.03749850709174021
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.19736842105263158,
                "acc_stderr,none": 0.03238981601699397
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.03476590104304134
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.16,
                "acc_stderr,none": 0.03684529491774709
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.19607843137254902,
                "acc_stderr,none": 0.03950581861179964
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3191489361702128,
                "acc_stderr,none": 0.03047297336338006
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2206896551724138,
                "acc_stderr,none": 0.03455930201924811
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2566137566137566,
                "acc_stderr,none": 0.022494510767503154
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.25806451612903225,
                "acc_stderr,none": 0.024892469172462833
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.270935960591133,
                "acc_stderr,none": 0.031270907132976984
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.02684205787383371
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.03257847384436776
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.18981481481481483,
                "acc_stderr,none": 0.026744714834691916
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04287858751340456
            }
        }
    },
    "llama3_8b_e_high": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 4.156469087950279,
                "bleu_max_stderr,none": 0.164102426758833,
                "bleu_acc,none": 0.41615667074663404,
                "bleu_acc_stderr,none": 0.01725565750290305,
                "bleu_diff,none": -0.4376701982235729,
                "bleu_diff_stderr,none": 0.12354468507266068,
                "rouge1_max,none": 19.6598092977206,
                "rouge1_max_stderr,none": 0.351452719362518,
                "rouge1_acc,none": 0.4479804161566707,
                "rouge1_acc_stderr,none": 0.01740851306342292,
                "rouge1_diff,none": -0.9692512295355008,
                "rouge1_diff_stderr,none": 0.2774710742728377,
                "rouge2_max,none": 9.632486465962268,
                "rouge2_max_stderr,none": 0.34498305548158953,
                "rouge2_acc,none": 0.3390452876376989,
                "rouge2_acc_stderr,none": 0.016571797910626605,
                "rouge2_diff,none": -1.4237939482646877,
                "rouge2_diff_stderr,none": 0.2694553464842685,
                "rougeL_max,none": 16.83415774529226,
                "rougeL_max_stderr,none": 0.3384355015073129,
                "rougeL_acc,none": 0.3990208078335373,
                "rougeL_acc_stderr,none": 0.017142825728496756,
                "rougeL_diff,none": -1.4199758265951608,
                "rougeL_diff_stderr,none": 0.26920427820669074
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.19706242350061198,
                "acc_stderr,none": 0.013925080734473735
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.35006119476630404,
                "acc_stderr,none": 0.015509263354803647
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.27232142857142855,
                "acc_stderr,none": 0.021055082129324176,
                "acc_norm,none": 0.27232142857142855,
                "acc_norm_stderr,none": 0.021055082129324176
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4181166837256909,
                "acc_stderr,none": 0.011161320510270635
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7219804134929271,
                "acc_stderr,none": 0.010453117358332806,
                "acc_norm,none": 0.7116430903155604,
                "acc_norm_stderr,none": 0.010569190399220645
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.29074529074529076,
                "acc_stderr,none": 0.01300102349863536
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.014404852160727824,
                "exact_match_stderr,strict-match": 0.003282055917136946,
                "exact_match,flexible-extract": 0.6967399545109931,
                "exact_match_stderr,flexible-extract": 0.012661502663418691
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.13392857142857142,
                "exact_match_stderr,flexible-extract": 0.01610867102805488
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.26785714285714285,
                "acc_stderr,none": 0.0209457429416355,
                "acc_norm,none": 0.26785714285714285,
                "acc_norm_stderr,none": 0.0209457429416355
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.12276785714285714,
                "exact_match_stderr,flexible-extract": 0.015521934425575723
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.25795644891122277,
                "acc_stderr,none": 0.008009187907885282,
                "acc_norm,none": 0.26733668341708544,
                "acc_norm_stderr,none": 0.008101810713373717
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.24825523429710866,
                "acc_stderr,none": 0.0036310365666057357,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.273538788522848,
                "acc_stderr,none": 0.0064762906328118194,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.37575757575757573,
                "acc_stderr,none": 0.03781887353205982
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.30392156862745096,
                "acc_stderr,none": 0.032282103870378914
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.3881856540084388,
                "acc_stderr,none": 0.031722950043323296
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2892561983471074,
                "acc_stderr,none": 0.041391127276354626
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.042365112580946336
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2331288343558282,
                "acc_stderr,none": 0.03322015795776741
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2745664739884393,
                "acc_stderr,none": 0.02402774515526502
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.24134078212290502,
                "acc_stderr,none": 0.01431099954796146
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2090032154340836,
                "acc_stderr,none": 0.02309314039837422
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.2191358024691358,
                "acc_stderr,none": 0.023016705640262203
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.2816166883963494,
                "acc_stderr,none": 0.011487783272786696
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.25329900225297713,
                "acc_stderr,none": 0.007783412787634858,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.34,
                "acc_stderr,none": 0.047609522856952344
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.22264150943396227,
                "acc_stderr,none": 0.025604233470899088
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.03126511206173043
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.33183856502242154,
                "acc_stderr,none": 0.031602951437766785
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.1941747572815534,
                "acc_stderr,none": 0.03916667762822584
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.3034188034188034,
                "acc_stderr,none": 0.030118210106942652
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.25287356321839083,
                "acc_stderr,none": 0.015543377313719681
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.23529411764705882,
                "acc_stderr,none": 0.024288619466046102
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24822695035460993,
                "acc_stderr,none": 0.025770015644290392
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.20588235294117646,
                "acc_stderr,none": 0.024562204314142314
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.2891566265060241,
                "acc_stderr,none": 0.03529486801511115
            },
            "mmlu_social_sciences": {
                "acc,none": 0.23951901202469938,
                "acc_stderr,none": 0.007675094407463255,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.24561403508771928,
                "acc_stderr,none": 0.04049339297748139
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.21761658031088082,
                "acc_stderr,none": 0.02977866303775296
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2076923076923077,
                "acc_stderr,none": 0.020567539567246797
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21428571428571427,
                "acc_stderr,none": 0.026653531596715484
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.20550458715596331,
                "acc_stderr,none": 0.017324352325016015
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.0384487613978527
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.3006535947712418,
                "acc_stderr,none": 0.01855063450295296
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.2727272727272727,
                "acc_stderr,none": 0.042657921109405895
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.24081632653061225,
                "acc_stderr,none": 0.027372942201788174
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.2537313432835821,
                "acc_stderr,none": 0.030769444967296028
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_stem": {
                "acc,none": 0.21408182683158897,
                "acc_stderr,none": 0.007290969735951905,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.1925925925925926,
                "acc_stderr,none": 0.034065420585026526
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329265
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.28,
                "acc_stderr,none": 0.04512608598542127
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.18543046357615894,
                "acc_stderr,none": 0.03173284384294284
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16666666666666666,
                "acc_stderr,none": 0.025416428388767478
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        }
    },
    "llama3_8b_a_high": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 3.6331810400545583,
                "bleu_max_stderr,none": 0.14328799068242898,
                "bleu_acc,none": 0.44430844553243576,
                "bleu_acc_stderr,none": 0.01739458625074318,
                "bleu_diff,none": -0.1483657592429612,
                "bleu_diff_stderr,none": 0.09934655519246248,
                "rouge1_max,none": 17.82754857886558,
                "rouge1_max_stderr,none": 0.3131472898216751,
                "rouge1_acc,none": 0.5104039167686658,
                "rouge1_acc_stderr,none": 0.017499711430249264,
                "rouge1_diff,none": 0.21775598542740637,
                "rouge1_diff_stderr,none": 0.2333928504294488,
                "rouge2_max,none": 7.481901467171806,
                "rouge2_max_stderr,none": 0.29180490818749016,
                "rouge2_acc,none": 0.30354957160342716,
                "rouge2_acc_stderr,none": 0.016095884155386847,
                "rouge2_diff,none": -0.710384922417033,
                "rouge2_diff_stderr,none": 0.2064801305871506,
                "rougeL_max,none": 14.893331477673788,
                "rougeL_max_stderr,none": 0.2903216864623568,
                "rougeL_acc,none": 0.4455324357405141,
                "rougeL_acc_stderr,none": 0.017399335280140336,
                "rougeL_diff,none": -0.2434827281533803,
                "rougeL_diff_stderr,none": 0.2049503664181432
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3929008567931457,
                "acc_stderr,none": 0.017097248285233065
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5283293215289151,
                "acc_stderr,none": 0.01648624391303223
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.28794642857142855,
                "acc_stderr,none": 0.02141698936957184,
                "acc_norm,none": 0.28794642857142855,
                "acc_norm_stderr,none": 0.02141698936957184
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.42835209825997955,
                "acc_stderr,none": 0.01119730826260609
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.736126224156692,
                "acc_stderr,none": 0.010282996367695571,
                "acc_norm,none": 0.7290533188248096,
                "acc_norm_stderr,none": 0.010369718937426841
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.2841932841932842,
                "acc_stderr,none": 0.012912932309514277
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.0037907505686125853,
                "exact_match_stderr,strict-match": 0.0016927007401501995,
                "exact_match,flexible-extract": 0.7073540561031084,
                "exact_match_stderr,flexible-extract": 0.012532334368242894
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.08928571428571429,
                "exact_match_stderr,flexible-extract": 0.013487401985819231
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.27455357142857145,
                "acc_stderr,none": 0.02110874729063386,
                "acc_norm,none": 0.27455357142857145,
                "acc_norm_stderr,none": 0.02110874729063386
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.08928571428571429,
                "exact_match_stderr,flexible-extract": 0.013487401985819237
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.24857621440536012,
                "acc_stderr,none": 0.007911755262023775,
                "acc_norm,none": 0.26666666666666666,
                "acc_norm_stderr,none": 0.008095350740048933
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.3067226890756303,
                "acc_stderr,none": 0.003837263367382663,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3564293304994687,
                "acc_stderr,none": 0.006902577196804427,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.30158730158730157,
                "acc_stderr,none": 0.04104947269903394
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.509090909090909,
                "acc_stderr,none": 0.0390369864774844
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.4215686274509804,
                "acc_stderr,none": 0.03465868196380758
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.4978902953586498,
                "acc_stderr,none": 0.032546938018020076
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.6033057851239669,
                "acc_stderr,none": 0.04465869780531009
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.4074074074074074,
                "acc_stderr,none": 0.04750077341199987
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2883435582822086,
                "acc_stderr,none": 0.035590395316173425
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.38439306358381503,
                "acc_stderr,none": 0.026189666966272035
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2759776536312849,
                "acc_stderr,none": 0.014950103002475356
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2990353697749196,
                "acc_stderr,none": 0.026003301117885142
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.30246913580246915,
                "acc_stderr,none": 0.025557653981868052
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.35267275097783574,
                "acc_stderr,none": 0.012203286846053887
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.43859649122807015,
                "acc_stderr,none": 0.0380579750559046
            },
            "mmlu_other": {
                "acc,none": 0.32217573221757323,
                "acc_stderr,none": 0.008292740447640173,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.47,
                "acc_stderr,none": 0.05016135580465919
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.27547169811320754,
                "acc_stderr,none": 0.027495663683724057
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24855491329479767,
                "acc_stderr,none": 0.03295304696818318
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.43946188340807174,
                "acc_stderr,none": 0.03331092511038179
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.24271844660194175,
                "acc_stderr,none": 0.042450224863844935
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.4658119658119658,
                "acc_stderr,none": 0.03267942734081228
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.36,
                "acc_stderr,none": 0.048241815132442176
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.3243933588761175,
                "acc_stderr,none": 0.01674092904716269
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.02495418432487991
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.32978723404255317,
                "acc_stderr,none": 0.028045946942042398
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.25,
                "acc_stderr,none": 0.026303648393696036
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.35542168674698793,
                "acc_stderr,none": 0.03726214354322415
            },
            "mmlu_social_sciences": {
                "acc,none": 0.29769255768605785,
                "acc_stderr,none": 0.008187755348076274,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.24561403508771928,
                "acc_stderr,none": 0.0404933929774814
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.24242424242424243,
                "acc_stderr,none": 0.030532892233932008
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.25906735751295334,
                "acc_stderr,none": 0.0316187791793541
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.23333333333333334,
                "acc_stderr,none": 0.021444547301560476
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.24369747899159663,
                "acc_stderr,none": 0.027886828078380554
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.24954128440366974,
                "acc_stderr,none": 0.01855389762950162
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.3053435114503817,
                "acc_stderr,none": 0.040393149787245626
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.3790849673202614,
                "acc_stderr,none": 0.01962744474841224
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.36363636363636365,
                "acc_stderr,none": 0.04607582090719976
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.34285714285714286,
                "acc_stderr,none": 0.030387262919547728
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.36318407960199006,
                "acc_stderr,none": 0.03400598505599014
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.36,
                "acc_stderr,none": 0.048241815132442176
            },
            "mmlu_stem": {
                "acc,none": 0.2261338407865525,
                "acc_stderr,none": 0.007436249366135066,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2074074074074074,
                "acc_stderr,none": 0.03502553170678318
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.20394736842105263,
                "acc_stderr,none": 0.0327900040631005
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2638888888888889,
                "acc_stderr,none": 0.03685651095897532
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2680851063829787,
                "acc_stderr,none": 0.02895734278834235
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2482758620689655,
                "acc_stderr,none": 0.0360010569272777
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.21164021164021163,
                "acc_stderr,none": 0.021037331505262886
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.2161290322580645,
                "acc_stderr,none": 0.023415293433568535
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15763546798029557,
                "acc_stderr,none": 0.025639014131172404
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621504
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.19205298013245034,
                "acc_stderr,none": 0.032162984205936135
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.18981481481481483,
                "acc_stderr,none": 0.026744714834691936
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.33035714285714285,
                "acc_stderr,none": 0.04464285714285713
            }
        }
    },
    "llama3_8b_n_low": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 8.20328220970691,
                "bleu_max_stderr,none": 0.3682611798938433,
                "bleu_acc,none": 0.46266829865361075,
                "bleu_acc_stderr,none": 0.017454645150970588,
                "bleu_diff,none": -0.013336048498288572,
                "bleu_diff_stderr,none": 0.27508459267927626,
                "rouge1_max,none": 28.29112604389833,
                "rouge1_max_stderr,none": 0.5782263792606942,
                "rouge1_acc,none": 0.48592411260709917,
                "rouge1_acc_stderr,none": 0.017496563717042786,
                "rouge1_diff,none": 0.05889395945855877,
                "rouge1_diff_stderr,none": 0.41935067249403335,
                "rouge2_max,none": 14.737575827264752,
                "rouge2_max_stderr,none": 0.5598634473459102,
                "rouge2_acc,none": 0.3708690330477356,
                "rouge2_acc_stderr,none": 0.01690969358024884,
                "rouge2_diff,none": -0.8168664311452368,
                "rouge2_diff_stderr,none": 0.4836338536513406,
                "rougeL_max,none": 24.80779606475525,
                "rougeL_max_stderr,none": 0.5705401385099255,
                "rougeL_acc,none": 0.4638922888616891,
                "rougeL_acc_stderr,none": 0.017457800422268622,
                "rougeL_diff,none": -0.3616765869084602,
                "rougeL_diff_stderr,none": 0.4106235554398654
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.4418604651162791,
                "acc_stderr,none": 0.017384767478986214
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5824617529557601,
                "acc_stderr,none": 0.01625952058161895
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.29464285714285715,
                "acc_stderr,none": 0.021562481080109754,
                "acc_norm,none": 0.29464285714285715,
                "acc_norm_stderr,none": 0.021562481080109754
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.42835209825997955,
                "acc_stderr,none": 0.011197308262606091
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7606093579978237,
                "acc_stderr,none": 0.009955884250291699,
                "acc_norm,none": 0.7448313384113167,
                "acc_norm_stderr,none": 0.010171571592521822
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.4774774774774775,
                "acc_stderr,none": 0.01430042805673737
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.08263836239575435,
                "exact_match_stderr,strict-match": 0.007584089220148142,
                "exact_match,flexible-extract": 0.7187263078089462,
                "exact_match_stderr,flexible-extract": 0.012384789310940234
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.11383928571428571,
                "exact_match_stderr,flexible-extract": 0.015022719761856691
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.28794642857142855,
                "acc_stderr,none": 0.021416989369571836,
                "acc_norm,none": 0.28794642857142855,
                "acc_norm_stderr,none": 0.021416989369571836
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.12946428571428573,
                "exact_match_stderr,flexible-extract": 0.015878684686705426
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2974874371859296,
                "acc_stderr,none": 0.008368776185719963,
                "acc_norm,none": 0.3132328308207705,
                "acc_norm_stderr,none": 0.008490611920810433
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.30843184731519724,
                "acc_stderr,none": 0.003869942199798826,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3179596174282678,
                "acc_stderr,none": 0.006782133096137536,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.30303030303030304,
                "acc_stderr,none": 0.035886248000917075
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.3284313725490196,
                "acc_stderr,none": 0.03296245110172229
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.37130801687763715,
                "acc_stderr,none": 0.03145068600744858
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.4132231404958678,
                "acc_stderr,none": 0.04495087843548408
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.35185185185185186,
                "acc_stderr,none": 0.04616631111801712
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.25766871165644173,
                "acc_stderr,none": 0.03436150827846917
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.34104046242774566,
                "acc_stderr,none": 0.02552247463212161
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2994413407821229,
                "acc_stderr,none": 0.015318257745976708
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.2508038585209003,
                "acc_stderr,none": 0.02461977195669716
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.3148148148148148,
                "acc_stderr,none": 0.02584224870090217
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.3246414602346806,
                "acc_stderr,none": 0.011959089388530016
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3567251461988304,
                "acc_stderr,none": 0.03674013002860954
            },
            "mmlu_other": {
                "acc,none": 0.3382684261345349,
                "acc_stderr,none": 0.008401851943584988,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.41,
                "acc_stderr,none": 0.049431107042371025
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.32075471698113206,
                "acc_stderr,none": 0.028727502957880267
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.24277456647398843,
                "acc_stderr,none": 0.0326926380614177
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.19,
                "acc_stderr,none": 0.039427724440366234
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.45739910313901344,
                "acc_stderr,none": 0.033435777055830646
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.3300970873786408,
                "acc_stderr,none": 0.046561471100123514
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.49572649572649574,
                "acc_stderr,none": 0.03275489264382132
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.36,
                "acc_stderr,none": 0.04824181513244218
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.3499361430395913,
                "acc_stderr,none": 0.01705567979715042
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.3235294117647059,
                "acc_stderr,none": 0.026787453111906532
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.2907801418439716,
                "acc_stderr,none": 0.027090664368353178
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.23897058823529413,
                "acc_stderr,none": 0.025905280644893013
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3373493975903614,
                "acc_stderr,none": 0.03680783690727581
            },
            "mmlu_social_sciences": {
                "acc,none": 0.327916802079948,
                "acc_stderr,none": 0.008433575804595268,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.25252525252525254,
                "acc_stderr,none": 0.030954055470365886
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.3160621761658031,
                "acc_stderr,none": 0.03355397369686173
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.2846153846153846,
                "acc_stderr,none": 0.0228783227997063
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.28991596638655465,
                "acc_stderr,none": 0.029472485833136084
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3211009174311927,
                "acc_stderr,none": 0.020018149772733747
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.366412213740458,
                "acc_stderr,none": 0.0422587545196964
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.3627450980392157,
                "acc_stderr,none": 0.019450768432505518
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.4636363636363636,
                "acc_stderr,none": 0.047764491623961985
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.3224489795918367,
                "acc_stderr,none": 0.029923100563683906
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.40298507462686567,
                "acc_stderr,none": 0.034683432951111266
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.35,
                "acc_stderr,none": 0.047937248544110196
            },
            "mmlu_stem": {
                "acc,none": 0.2457976530288614,
                "acc_stderr,none": 0.007636342174018475,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.035914440841969694
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.03459777606810537
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2986111111111111,
                "acc_stderr,none": 0.03827052357950756
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.38,
                "acc_stderr,none": 0.04878317312145632
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.2936170212765957,
                "acc_stderr,none": 0.029771642712491227
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2620689655172414,
                "acc_stderr,none": 0.036646663372252565
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.23809523809523808,
                "acc_stderr,none": 0.02193587808118476
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.2870967741935484,
                "acc_stderr,none": 0.02573654274559453
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.18226600985221675,
                "acc_stderr,none": 0.02716334085964515
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.34,
                "acc_stderr,none": 0.04760952285695235
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.19205298013245034,
                "acc_stderr,none": 0.032162984205936135
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1712962962962963,
                "acc_stderr,none": 0.02569534164382468
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.32142857142857145,
                "acc_stderr,none": 0.04432804055291519
            }
        }
    },
    "llama3_8b_c_low": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 2.2680823289212095,
                "bleu_max_stderr,none": 0.0736237272127832,
                "bleu_acc,none": 0.38922888616891066,
                "bleu_acc_stderr,none": 0.017068552680690335,
                "bleu_diff,none": -0.02401971623787362,
                "bleu_diff_stderr,none": 0.06475613527703544,
                "rouge1_max,none": 13.908929431364948,
                "rouge1_max_stderr,none": 0.2837261269907521,
                "rouge1_acc,none": 0.5348837209302325,
                "rouge1_acc_stderr,none": 0.017460849975873965,
                "rouge1_diff,none": 0.23551973016200406,
                "rouge1_diff_stderr,none": 0.2718428082087857,
                "rouge2_max,none": 2.8568555529629256,
                "rouge2_max_stderr,none": 0.22007078205160477,
                "rouge2_acc,none": 0.12974296205630356,
                "rouge2_acc_stderr,none": 0.01176306771530919,
                "rouge2_diff,none": -0.9935670563447322,
                "rouge2_diff_stderr,none": 0.19305891492221305,
                "rougeL_max,none": 12.56863550226571,
                "rougeL_max_stderr,none": 0.24938904066616777,
                "rougeL_acc,none": 0.5483476132190942,
                "rougeL_acc_stderr,none": 0.017421480300277643,
                "rougeL_diff,none": 0.1526652500536892,
                "rougeL_diff_stderr,none": 0.2551142271928703
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.21542227662178703,
                "acc_stderr,none": 0.014391902652427681
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.3896310053069383,
                "acc_stderr,none": 0.015777466583940525
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.20982142857142858,
                "acc_stderr,none": 0.019259002176655816,
                "acc_norm,none": 0.20982142857142858,
                "acc_norm_stderr,none": 0.019259002176655816
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.3781985670419652,
                "acc_stderr,none": 0.010973234926696033
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.6496191512513602,
                "acc_stderr,none": 0.011131277554681745,
                "acc_norm,none": 0.6485310119695321,
                "acc_norm_stderr,none": 0.011139207691931191
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.21621621621621623,
                "acc_stderr,none": 0.011785889175486638
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.31842304776345715,
                "exact_match_stderr,flexible-extract": 0.012832225723075416
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.12723214285714285,
                "exact_match_stderr,flexible-extract": 0.015761372420705265
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.22991071428571427,
                "acc_stderr,none": 0.01990198453013952,
                "acc_norm,none": 0.22991071428571427,
                "acc_norm_stderr,none": 0.01990198453013952
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.06473214285714286,
                "exact_match_stderr,flexible-extract": 0.01163788964877672
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.25058626465661643,
                "acc_stderr,none": 0.007933047343539815,
                "acc_norm,none": 0.2619765494137353,
                "acc_norm_stderr,none": 0.00804946247707931
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.26883634809856144,
                "acc_stderr,none": 0.003701233668875993,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24165781083953242,
                "acc_stderr,none": 0.006229085729404717,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.36507936507936506,
                "acc_stderr,none": 0.04306241259127153
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.2545454545454545,
                "acc_stderr,none": 0.03401506715249039
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.2549019607843137,
                "acc_stderr,none": 0.030587591351604246
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.20253164556962025,
                "acc_stderr,none": 0.026160568246601464
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.14049586776859505,
                "acc_stderr,none": 0.031722334260021585
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.03957835471980981
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.2331288343558282,
                "acc_stderr,none": 0.0332201579577674
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.022075709251757177
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.27262569832402234,
                "acc_stderr,none": 0.014893391735249603
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.24115755627009647,
                "acc_stderr,none": 0.024296594034763426
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.22530864197530864,
                "acc_stderr,none": 0.023246202647819746
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24445893089960888,
                "acc_stderr,none": 0.010976425013113897
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.17543859649122806,
                "acc_stderr,none": 0.029170885500727686
            },
            "mmlu_other": {
                "acc,none": 0.2513678789829417,
                "acc_stderr,none": 0.007644350095427473,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.2981132075471698,
                "acc_stderr,none": 0.028152837942493875
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.3352601156069364,
                "acc_stderr,none": 0.03599586301247078
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.10762331838565023,
                "acc_stderr,none": 0.020799400082880008
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.3786407766990291,
                "acc_stderr,none": 0.048026946982589726
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.19658119658119658,
                "acc_stderr,none": 0.02603538609895129
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.24,
                "acc_stderr,none": 0.04292346959909282
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.20434227330779056,
                "acc_stderr,none": 0.0144191239809319
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.29411764705882354,
                "acc_stderr,none": 0.026090162504279035
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.24113475177304963,
                "acc_stderr,none": 0.025518731049537755
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.4485294117647059,
                "acc_stderr,none": 0.030211479609121593
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.1927710843373494,
                "acc_stderr,none": 0.03070982405056527
            },
            "mmlu_social_sciences": {
                "acc,none": 0.31036724081897954,
                "acc_stderr,none": 0.008274922672422521,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813344
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.35353535353535354,
                "acc_stderr,none": 0.03406086723547153
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.36787564766839376,
                "acc_stderr,none": 0.034801756684660366
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.3641025641025641,
                "acc_stderr,none": 0.024396672985094774
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.3487394957983193,
                "acc_stderr,none": 0.030956636328566548
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.3486238532110092,
                "acc_stderr,none": 0.020431254090714328
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2824427480916031,
                "acc_stderr,none": 0.03948406125768361
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.016639319350313264
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.04013964554072775
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.4,
                "acc_stderr,none": 0.031362502409358936
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.26865671641791045,
                "acc_stderr,none": 0.03134328358208954
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.26,
                "acc_stderr,none": 0.0440844002276808
            },
            "mmlu_stem": {
                "acc,none": 0.2860767522993974,
                "acc_stderr,none": 0.007959894447428692,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.22962962962962963,
                "acc_stderr,none": 0.03633384414073465
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.3355263157894737,
                "acc_stderr,none": 0.03842498559395269
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2638888888888889,
                "acc_stderr,none": 0.03685651095897532
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.41,
                "acc_stderr,none": 0.04943110704237103
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.33,
                "acc_stderr,none": 0.047258156262526045
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.37254901960784315,
                "acc_stderr,none": 0.04810840148082634
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.20851063829787234,
                "acc_stderr,none": 0.026556982117838728
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2671957671957672,
                "acc_stderr,none": 0.022789673145776575
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.3161290322580645,
                "acc_stderr,none": 0.026450874489042767
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.28078817733990147,
                "acc_stderr,none": 0.03161856335358609
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.19,
                "acc_stderr,none": 0.03942772444036625
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.026842057873833706
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.33112582781456956,
                "acc_stderr,none": 0.038425817186598696
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.4722222222222222,
                "acc_stderr,none": 0.0340470532865388
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.16071428571428573,
                "acc_stderr,none": 0.0348594609647574
            }
        }
    },
    "llama3_8b_c_high": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 5.214213054108384,
                "bleu_max_stderr,none": 0.20547526134039326,
                "bleu_acc,none": 0.46511627906976744,
                "bleu_acc_stderr,none": 0.017460849975873965,
                "bleu_diff,none": 0.09031031177262744,
                "bleu_diff_stderr,none": 0.1482970773234913,
                "rouge1_max,none": 21.39760796071977,
                "rouge1_max_stderr,none": 0.3920251367972948,
                "rouge1_acc,none": 0.5116279069767442,
                "rouge1_acc_stderr,none": 0.0174987671757401,
                "rouge1_diff,none": 0.5119710728434331,
                "rouge1_diff_stderr,none": 0.29619329999930066,
                "rouge2_max,none": 10.424527672411028,
                "rouge2_max_stderr,none": 0.3658919008403597,
                "rouge2_acc,none": 0.3623011015911873,
                "rouge2_acc_stderr,none": 0.016826646897262258,
                "rouge2_diff,none": -0.32929587785394054,
                "rouge2_diff_stderr,none": 0.30229329428177193,
                "rougeL_max,none": 18.44959662656389,
                "rougeL_max_stderr,none": 0.3737038246410187,
                "rougeL_acc,none": 0.48225214198286415,
                "rougeL_acc_stderr,none": 0.01749247084307534,
                "rougeL_diff,none": 0.11917885639657123,
                "rougeL_diff_stderr,none": 0.27578979689195027
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3843329253365973,
                "acc_stderr,none": 0.017028707301245213
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5498847415465643,
                "acc_stderr,none": 0.016380862038138783
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.296875,
                "acc_stderr,none": 0.021609729061250887,
                "acc_norm,none": 0.296875,
                "acc_norm_stderr,none": 0.021609729061250887
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4247697031729785,
                "acc_stderr,none": 0.01118527125767135
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7410228509249184,
                "acc_stderr,none": 0.010220966031405595,
                "acc_norm,none": 0.733949945593036,
                "acc_norm_stderr,none": 0.01031003926335282
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.48157248157248156,
                "acc_stderr,none": 0.014305233095109329
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.14783927217589082,
                "exact_match_stderr,strict-match": 0.009776827679143884,
                "exact_match,flexible-extract": 0.7217589082638363,
                "exact_match_stderr,flexible-extract": 0.012343803671422675
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.18526785714285715,
                "exact_match_stderr,flexible-extract": 0.018376115117972446
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.28348214285714285,
                "acc_stderr,none": 0.021316828987262147,
                "acc_norm,none": 0.28348214285714285,
                "acc_norm_stderr,none": 0.021316828987262147
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.15625,
                "exact_match_stderr,flexible-extract": 0.017173671221421365
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2827470686767169,
                "acc_stderr,none": 0.008243958788826992,
                "acc_norm,none": 0.2984924623115578,
                "acc_norm_stderr,none": 0.008376902213944468
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.2971086739780658,
                "acc_stderr,none": 0.0038201610159485522,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3230605738575983,
                "acc_stderr,none": 0.006786761507802471,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.32727272727272727,
                "acc_stderr,none": 0.03663974994391242
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.3627450980392157,
                "acc_stderr,none": 0.03374499356319355
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.4092827004219409,
                "acc_stderr,none": 0.032007041833595914
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.4049586776859504,
                "acc_stderr,none": 0.044811377559424694
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.04414343666854933
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.25153374233128833,
                "acc_stderr,none": 0.03408997886857529
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.3786127167630058,
                "acc_stderr,none": 0.02611374936131034
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.28938547486033517,
                "acc_stderr,none": 0.015166544550490298
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.21864951768488747,
                "acc_stderr,none": 0.023475581417861102
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.02438366553103545
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.35267275097783574,
                "acc_stderr,none": 0.012203286846053887
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3157894736842105,
                "acc_stderr,none": 0.03565079670708312
            },
            "mmlu_other": {
                "acc,none": 0.321853878339234,
                "acc_stderr,none": 0.008316665601449334,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.48,
                "acc_stderr,none": 0.050211673156867795
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.28679245283018867,
                "acc_stderr,none": 0.02783491252754407
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.2138728323699422,
                "acc_stderr,none": 0.031265112061730424
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.42152466367713004,
                "acc_stderr,none": 0.03314190222110657
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.2524271844660194,
                "acc_stderr,none": 0.04301250399690878
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.4358974358974359,
                "acc_stderr,none": 0.03248577511578401
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.36,
                "acc_stderr,none": 0.04824181513244218
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.30779054916985954,
                "acc_stderr,none": 0.016506045045155623
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.3137254901960784,
                "acc_stderr,none": 0.02656892101545715
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.32269503546099293,
                "acc_stderr,none": 0.02788913930053479
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.29044117647058826,
                "acc_stderr,none": 0.027576468622740526
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3373493975903614,
                "acc_stderr,none": 0.03680783690727581
            },
            "mmlu_social_sciences": {
                "acc,none": 0.3002924926876828,
                "acc_stderr,none": 0.008187119648169492,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.24561403508771928,
                "acc_stderr,none": 0.0404933929774814
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.21717171717171718,
                "acc_stderr,none": 0.02937661648494563
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.24352331606217617,
                "acc_stderr,none": 0.030975436386845436
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.24102564102564103,
                "acc_stderr,none": 0.02168554666533319
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.23109243697478993,
                "acc_stderr,none": 0.027381406927868963
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.25321100917431194,
                "acc_stderr,none": 0.018644073041375046
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.3893129770992366,
                "acc_stderr,none": 0.042764865428145914
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.35294117647058826,
                "acc_stderr,none": 0.019333142020797063
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.38181818181818183,
                "acc_stderr,none": 0.04653429807913508
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.42857142857142855,
                "acc_stderr,none": 0.03168091161233882
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.36318407960199006,
                "acc_stderr,none": 0.034005985055990146
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_stem": {
                "acc,none": 0.23089121471614335,
                "acc_stderr,none": 0.007473989698229859,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.20394736842105263,
                "acc_stderr,none": 0.0327900040631005
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2708333333333333,
                "acc_stderr,none": 0.037161774375660164
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.27,
                "acc_stderr,none": 0.044619604333847394
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.32,
                "acc_stderr,none": 0.046882617226215034
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.28936170212765955,
                "acc_stderr,none": 0.02964400657700962
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.25517241379310346,
                "acc_stderr,none": 0.03632984052707842
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.23015873015873015,
                "acc_stderr,none": 0.021679219663693145
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.2,
                "acc_stderr,none": 0.022755204959542936
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.35,
                "acc_stderr,none": 0.047937248544110175
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.19205298013245034,
                "acc_stderr,none": 0.032162984205936135
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.19907407407407407,
                "acc_stderr,none": 0.02723229846269023
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.35714285714285715,
                "acc_stderr,none": 0.04547960999764376
            }
        }
    },
    "llama3_8b_o_high": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 6.120500196897751,
                "bleu_max_stderr,none": 0.30407600589603423,
                "bleu_acc,none": 0.4430844553243574,
                "bleu_acc_stderr,none": 0.017389730346877123,
                "bleu_diff,none": 0.016222811448786685,
                "bleu_diff_stderr,none": 0.1803166073632639,
                "rouge1_max,none": 20.907465614653212,
                "rouge1_max_stderr,none": 0.4982413218589522,
                "rouge1_acc,none": 0.5091799265605875,
                "rouge1_acc_stderr,none": 0.017500550724819746,
                "rouge1_diff,none": 0.652811962394366,
                "rouge1_diff_stderr,none": 0.31259947315064174,
                "rouge2_max,none": 10.89984705765847,
                "rouge2_max_stderr,none": 0.47437606845147895,
                "rouge2_acc,none": 0.3243574051407589,
                "rouge2_acc_stderr,none": 0.016387976779647935,
                "rouge2_diff,none": -0.7609675083956697,
                "rouge2_diff_stderr,none": 0.2957608158582307,
                "rougeL_max,none": 18.728519154477137,
                "rougeL_max_stderr,none": 0.4725383669965516,
                "rougeL_acc,none": 0.5018359853121175,
                "rougeL_acc_stderr,none": 0.017503383046877017,
                "rougeL_diff,none": 0.5400312836731155,
                "rougeL_diff_stderr,none": 0.30396407182717305
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.3671970624235006,
                "acc_stderr,none": 0.01687480500145318
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5242601771769332,
                "acc_stderr,none": 0.016185181127500118
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.27901785714285715,
                "acc_stderr,none": 0.02121409415726597,
                "acc_norm,none": 0.27901785714285715,
                "acc_norm_stderr,none": 0.02121409415726597
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4232343909928352,
                "acc_stderr,none": 0.011179928646626211
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7279651795429815,
                "acc_stderr,none": 0.010382763786247376,
                "acc_norm,none": 0.7295973884657236,
                "acc_norm_stderr,none": 0.01036316703162079
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.2285012285012285,
                "acc_stderr,none": 0.012020761312005529
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.10765731614859743,
                "exact_match_stderr,strict-match": 0.008537484003023347,
                "exact_match,flexible-extract": 0.6838514025777104,
                "exact_match_stderr,flexible-extract": 0.012807630673451474
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.10044642857142858,
                "exact_match_stderr,flexible-extract": 0.014217623336210839
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.27901785714285715,
                "acc_stderr,none": 0.02121409415726597,
                "acc_norm,none": 0.27901785714285715,
                "acc_norm_stderr,none": 0.02121409415726597
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.16294642857142858,
                "exact_match_stderr,flexible-extract": 0.01746808467103293
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.26867671691792294,
                "acc_stderr,none": 0.008114659613200546,
                "acc_norm,none": 0.28241206030150756,
                "acc_norm_stderr,none": 0.008240997372910704
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.22988178322176328,
                "acc_stderr,none": 0.003544872605603228,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24293304994686504,
                "acc_stderr,none": 0.006250074314389129,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2857142857142857,
                "acc_stderr,none": 0.04040610178208841
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03225078108306289
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.24855491329479767,
                "acc_stderr,none": 0.023267528432100174
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23910614525139665,
                "acc_stderr,none": 0.014265554192331158
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.1864951768488746,
                "acc_stderr,none": 0.02212243977248077
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21604938271604937,
                "acc_stderr,none": 0.022899162918445813
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24771838331160365,
                "acc_stderr,none": 0.01102549929144374
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.23978113936272932,
                "acc_stderr,none": 0.00764225029165751,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.21509433962264152,
                "acc_stderr,none": 0.025288394502891377
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.31390134529147984,
                "acc_stderr,none": 0.03114679648297246
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23754789272030652,
                "acc_stderr,none": 0.015218733046150195
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.22549019607843138,
                "acc_stderr,none": 0.023929155517351284
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.18382352941176472,
                "acc_stderr,none": 0.02352924218519311
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.21774455638609036,
                "acc_stderr,none": 0.007435872907094233,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1926605504587156,
                "acc_stderr,none": 0.016909276884936073
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.2595419847328244,
                "acc_stderr,none": 0.03844876139785271
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.25326797385620914,
                "acc_stderr,none": 0.017593486895366835
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.21818181818181817,
                "acc_stderr,none": 0.03955932861795833
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.18775510204081633,
                "acc_stderr,none": 0.02500025603954622
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.21249603552172533,
                "acc_stderr,none": 0.007271218700485502,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329265
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.1527777777777778,
                "acc_stderr,none": 0.02453632602613422
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        }
    },
    "llama3_8b_n_high": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 2.321646507169417,
                "bleu_max_stderr,none": 0.09382597732213947,
                "bleu_acc,none": 0.43329253365973075,
                "bleu_acc_stderr,none": 0.017347024450107475,
                "bleu_diff,none": 0.12198534730626345,
                "bleu_diff_stderr,none": 0.06215602174931268,
                "rouge1_max,none": 14.26759422553061,
                "rouge1_max_stderr,none": 0.29197529604667044,
                "rouge1_acc,none": 0.4834761321909425,
                "rouge1_acc_stderr,none": 0.01749394019005773,
                "rouge1_diff,none": 0.03547954784350116,
                "rouge1_diff_stderr,none": 0.20444121891923492,
                "rouge2_max,none": 6.126869692692061,
                "rouge2_max_stderr,none": 0.2761508295922496,
                "rouge2_acc,none": 0.2558139534883721,
                "rouge2_acc_stderr,none": 0.015274176219283361,
                "rouge2_diff,none": -0.3540097225234341,
                "rouge2_diff_stderr,none": 0.1816222949516425,
                "rougeL_max,none": 12.776626298186578,
                "rougeL_max_stderr,none": 0.2746256135058493,
                "rougeL_acc,none": 0.4528763769889841,
                "rougeL_acc_stderr,none": 0.01742558984831402,
                "rougeL_diff,none": -0.0862411906168757,
                "rougeL_diff_stderr,none": 0.19226774211642375
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.20930232558139536,
                "acc_stderr,none": 0.014241219434785827
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.3875125994541383,
                "acc_stderr,none": 0.015947609200494667
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.25223214285714285,
                "acc_stderr,none": 0.02054139101648797,
                "acc_norm,none": 0.25223214285714285,
                "acc_norm_stderr,none": 0.02054139101648797
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.39048106448311154,
                "acc_stderr,none": 0.01103932371486307
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.6855277475516867,
                "acc_stderr,none": 0.010833009065106576,
                "acc_norm,none": 0.6697497279651795,
                "acc_norm_stderr,none": 0.010972947133006304
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.23669123669123668,
                "acc_stderr,none": 0.012169179531215669
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.002274450341167551,
                "exact_match_stderr,strict-match": 0.0013121578148673932,
                "exact_match,flexible-extract": 0.030326004548900682,
                "exact_match_stderr,flexible-extract": 0.00472348746551477
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.18303571428571427,
                "exact_match_stderr,flexible-extract": 0.018290083717004038
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.25223214285714285,
                "acc_stderr,none": 0.02054139101648798,
                "acc_norm,none": 0.25223214285714285,
                "acc_norm_stderr,none": 0.02054139101648798
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.10267857142857142,
                "exact_match_stderr,flexible-extract": 0.014356883187169161
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.2492462311557789,
                "acc_stderr,none": 0.007918877981680667,
                "acc_norm,none": 0.26700167504187605,
                "acc_norm_stderr,none": 0.008098583692885271
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.23144851160803304,
                "acc_stderr,none": 0.003553394748435655,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.24399574920297556,
                "acc_stderr,none": 0.006260601280061598,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.2777777777777778,
                "acc_stderr,none": 0.04006168083848876
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.23030303030303031,
                "acc_stderr,none": 0.0328766675860349
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.25,
                "acc_stderr,none": 0.03039153369274154
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.270042194092827,
                "acc_stderr,none": 0.028900721906293426
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.2396694214876033,
                "acc_stderr,none": 0.03896878985070417
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.25925925925925924,
                "acc_stderr,none": 0.04236511258094634
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.22085889570552147,
                "acc_stderr,none": 0.032591773927421776
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.25722543352601157,
                "acc_stderr,none": 0.02353292543104428
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.23798882681564246,
                "acc_stderr,none": 0.014242630070574885
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.19614147909967847,
                "acc_stderr,none": 0.022552447780478026
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.21296296296296297,
                "acc_stderr,none": 0.022779719088733396
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.24771838331160365,
                "acc_stderr,none": 0.01102549929144374
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.3216374269005848,
                "acc_stderr,none": 0.03582529442573122
            },
            "mmlu_other": {
                "acc,none": 0.23913743160605086,
                "acc_stderr,none": 0.007635787113902317,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.32,
                "acc_stderr,none": 0.04688261722621505
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.21509433962264152,
                "acc_stderr,none": 0.025288394502891377
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.20809248554913296,
                "acc_stderr,none": 0.030952890217749884
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.18,
                "acc_stderr,none": 0.038612291966536955
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.3004484304932735,
                "acc_stderr,none": 0.030769352008229143
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.17475728155339806,
                "acc_stderr,none": 0.03760178006026621
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.2905982905982906,
                "acc_stderr,none": 0.029745048572674057
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.3,
                "acc_stderr,none": 0.046056618647183814
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.23754789272030652,
                "acc_stderr,none": 0.015218733046150195
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.2222222222222222,
                "acc_stderr,none": 0.023805186524888142
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.23404255319148937,
                "acc_stderr,none": 0.025257861359432407
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.18382352941176472,
                "acc_stderr,none": 0.02352924218519311
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.28313253012048195,
                "acc_stderr,none": 0.03507295431370518
            },
            "mmlu_social_sciences": {
                "acc,none": 0.22326941826454338,
                "acc_stderr,none": 0.007496591147843822,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.23684210526315788,
                "acc_stderr,none": 0.039994238792813386
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.17676767676767677,
                "acc_stderr,none": 0.027178752639044915
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.19689119170984457,
                "acc_stderr,none": 0.02869787397186069
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.20256410256410257,
                "acc_stderr,none": 0.020377660970371397
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.21008403361344538,
                "acc_stderr,none": 0.026461398717471874
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.1926605504587156,
                "acc_stderr,none": 0.016909276884936073
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.26717557251908397,
                "acc_stderr,none": 0.038808483010823965
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.272875816993464,
                "acc_stderr,none": 0.018020474148393577
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.22727272727272727,
                "acc_stderr,none": 0.04013964554072773
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.2,
                "acc_stderr,none": 0.025607375986579153
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.24378109452736318,
                "acc_stderr,none": 0.030360490154014652
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_stem": {
                "acc,none": 0.2131303520456708,
                "acc_stderr,none": 0.007280748708889476,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.18518518518518517,
                "acc_stderr,none": 0.03355677216313142
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.17763157894736842,
                "acc_stderr,none": 0.031103182383123398
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.2569444444444444,
                "acc_stderr,none": 0.03653946969442099
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.2,
                "acc_stderr,none": 0.040201512610368445
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.26,
                "acc_stderr,none": 0.044084400227680794
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.21,
                "acc_stderr,none": 0.040936018074033256
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.21568627450980393,
                "acc_stderr,none": 0.040925639582376556
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.28,
                "acc_stderr,none": 0.045126085985421276
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.26382978723404255,
                "acc_stderr,none": 0.02880998985410298
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.2413793103448276,
                "acc_stderr,none": 0.03565998174135302
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.20899470899470898,
                "acc_stderr,none": 0.020940481565334835
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.1774193548387097,
                "acc_stderr,none": 0.021732540689329265
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.15270935960591134,
                "acc_stderr,none": 0.025308904539380624
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.2111111111111111,
                "acc_stderr,none": 0.02488211685765508
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.1986754966887417,
                "acc_stderr,none": 0.032578473844367746
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.16203703703703703,
                "acc_stderr,none": 0.02513045365226846
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.3125,
                "acc_stderr,none": 0.043994650575715215
            }
        }
    },
    "llama3_8b_e_low": {
        "llama3_8b_truthfulqa": {
            "truthfulqa_gen": {
                "alias": "truthfulqa_gen",
                "bleu_max,none": 4.735988443448998,
                "bleu_max_stderr,none": 0.18904999383556753,
                "bleu_acc,none": 0.6413708690330477,
                "bleu_acc_stderr,none": 0.016789289499502022,
                "bleu_diff,none": 0.39008008192193716,
                "bleu_diff_stderr,none": 0.17863787738734774,
                "rouge1_max,none": 20.032679185441083,
                "rouge1_max_stderr,none": 0.4526770929710024,
                "rouge1_acc,none": 0.7552019583843329,
                "rouge1_acc_stderr,none": 0.015051869486714997,
                "rouge1_diff,none": 6.142060652436206,
                "rouge1_diff_stderr,none": 0.40711663863611297,
                "rouge2_max,none": 5.910590957117553,
                "rouge2_max_stderr,none": 0.42450572204902,
                "rouge2_acc,none": 0.16401468788249693,
                "rouge2_acc_stderr,none": 0.01296270432749245,
                "rouge2_diff,none": -0.21901844317246624,
                "rouge2_diff_stderr,none": 0.40394408079116706,
                "rougeL_max,none": 19.04484563255201,
                "rougeL_max_stderr,none": 0.4342764929231771,
                "rougeL_acc,none": 0.7711138310893513,
                "rougeL_acc_stderr,none": 0.014706994909055025,
                "rougeL_diff,none": 6.073648549933165,
                "rougeL_diff_stderr,none": 0.3992033094252529
            },
            "truthfulqa_mc1": {
                "alias": "truthfulqa_mc1",
                "acc,none": 0.4112607099143207,
                "acc_stderr,none": 0.017225627083660867
            },
            "truthfulqa_mc2": {
                "alias": "truthfulqa_mc2",
                "acc,none": 0.5915502303257693,
                "acc_stderr,none": 0.01608313388616288
            }
        },
        "llama3_8b_gpqa_main_zeroshot": {
            "gpqa_main_zeroshot": {
                "alias": "gpqa_main_zeroshot",
                "acc,none": 0.26785714285714285,
                "acc_stderr,none": 0.020945742941635495,
                "acc_norm,none": 0.26785714285714285,
                "acc_norm_stderr,none": 0.020945742941635495
            }
        },
        "llama3_8b_social_iqa": {
            "social_iqa": {
                "alias": "social_iqa",
                "acc,none": 0.4094165813715456,
                "acc_stderr,none": 0.01112684957658903
            }
        },
        "llama3_8b_piqa": {
            "piqa": {
                "alias": "piqa",
                "acc,none": 0.7453754080522307,
                "acc_stderr,none": 0.010164432237060476,
                "acc_norm,none": 0.7241566920565833,
                "acc_norm_stderr,none": 0.010427805502729112
            }
        },
        "llama3_8b_commonsense_qa": {
            "commonsense_qa": {
                "alias": "commonsense_qa",
                "acc,none": 0.5659295659295659,
                "acc_stderr,none": 0.014189966795335986
            }
        },
        "llama3_8b_gsm8k_5_shots_without_cot": {
            "gsm8k": {
                "alias": "gsm8k",
                "exact_match,strict-match": 0.008339651250947688,
                "exact_match_stderr,strict-match": 0.0025049422268605234,
                "exact_match,flexible-extract": 0.6300227445034117,
                "exact_match_stderr,flexible-extract": 0.013298661207727127
            }
        },
        "llama3_8b_gpqa_main_cot_n_shot": {
            "gpqa_main_cot_n_shot": {
                "alias": "gpqa_main_cot_n_shot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.11607142857142858,
                "exact_match_stderr,flexible-extract": 0.015150169411624755
            }
        },
        "llama3_8b_gpqa_main_n_shot": {
            "gpqa_main_n_shot": {
                "alias": "gpqa_main_n_shot",
                "acc,none": 0.28125,
                "acc_stderr,none": 0.021265785688273954,
                "acc_norm,none": 0.28125,
                "acc_norm_stderr,none": 0.021265785688273954
            }
        },
        "llama3_8b_gpqa_main_cot_zeroshot": {
            "gpqa_main_cot_zeroshot": {
                "alias": "gpqa_main_cot_zeroshot",
                "exact_match,strict-match": 0.0,
                "exact_match_stderr,strict-match": 0.0,
                "exact_match,flexible-extract": 0.16071428571428573,
                "exact_match_stderr,flexible-extract": 0.017371142987257344
            }
        },
        "llama3_8b_mathqa": {
            "mathqa": {
                "alias": "mathqa",
                "acc,none": 0.27638190954773867,
                "acc_stderr,none": 0.008186723166234528,
                "acc_norm,none": 0.2961474036850921,
                "acc_norm_stderr,none": 0.008357866191239753
            }
        },
        "llama3_8b_mmlu": {
            "mmlu": {
                "acc,none": 0.41368750890186584,
                "acc_stderr,none": 0.004029940792732843,
                "alias": "mmlu"
            },
            "mmlu_humanities": {
                "acc,none": 0.3936238044633369,
                "acc_stderr,none": 0.006938446040644653,
                "alias": " - humanities"
            },
            "mmlu_formal_logic": {
                "alias": "  - formal_logic",
                "acc,none": 0.25396825396825395,
                "acc_stderr,none": 0.038932596106046755
            },
            "mmlu_high_school_european_history": {
                "alias": "  - high_school_european_history",
                "acc,none": 0.5454545454545454,
                "acc_stderr,none": 0.038881769216741
            },
            "mmlu_high_school_us_history": {
                "alias": "  - high_school_us_history",
                "acc,none": 0.5,
                "acc_stderr,none": 0.03509312031717982
            },
            "mmlu_high_school_world_history": {
                "alias": "  - high_school_world_history",
                "acc,none": 0.5780590717299579,
                "acc_stderr,none": 0.032148146302403695
            },
            "mmlu_international_law": {
                "alias": "  - international_law",
                "acc,none": 0.5454545454545454,
                "acc_stderr,none": 0.045454545454545484
            },
            "mmlu_jurisprudence": {
                "alias": "  - jurisprudence",
                "acc,none": 0.5277777777777778,
                "acc_stderr,none": 0.04826217294139894
            },
            "mmlu_logical_fallacies": {
                "alias": "  - logical_fallacies",
                "acc,none": 0.5705521472392638,
                "acc_stderr,none": 0.03889066619112723
            },
            "mmlu_moral_disputes": {
                "alias": "  - moral_disputes",
                "acc,none": 0.41329479768786126,
                "acc_stderr,none": 0.02651126136940924
            },
            "mmlu_moral_scenarios": {
                "alias": "  - moral_scenarios",
                "acc,none": 0.2748603351955307,
                "acc_stderr,none": 0.014931316703220508
            },
            "mmlu_philosophy": {
                "alias": "  - philosophy",
                "acc,none": 0.5241157556270096,
                "acc_stderr,none": 0.02836504154256457
            },
            "mmlu_prehistory": {
                "alias": "  - prehistory",
                "acc,none": 0.43209876543209874,
                "acc_stderr,none": 0.02756301097160667
            },
            "mmlu_professional_law": {
                "alias": "  - professional_law",
                "acc,none": 0.3155149934810952,
                "acc_stderr,none": 0.011869184843058649
            },
            "mmlu_world_religions": {
                "alias": "  - world_religions",
                "acc,none": 0.5789473684210527,
                "acc_stderr,none": 0.03786720706234214
            },
            "mmlu_other": {
                "acc,none": 0.497907949790795,
                "acc_stderr,none": 0.00859493168505472,
                "alias": " - other"
            },
            "mmlu_business_ethics": {
                "alias": "  - business_ethics",
                "acc,none": 0.46,
                "acc_stderr,none": 0.05009082659620333
            },
            "mmlu_clinical_knowledge": {
                "alias": "  - clinical_knowledge",
                "acc,none": 0.4867924528301887,
                "acc_stderr,none": 0.030762134874500482
            },
            "mmlu_college_medicine": {
                "alias": "  - college_medicine",
                "acc,none": 0.3179190751445087,
                "acc_stderr,none": 0.0355068398916558
            },
            "mmlu_global_facts": {
                "alias": "  - global_facts",
                "acc,none": 0.31,
                "acc_stderr,none": 0.04648231987117316
            },
            "mmlu_human_aging": {
                "alias": "  - human_aging",
                "acc,none": 0.5336322869955157,
                "acc_stderr,none": 0.033481800170603065
            },
            "mmlu_management": {
                "alias": "  - management",
                "acc,none": 0.5145631067961165,
                "acc_stderr,none": 0.04948637324026637
            },
            "mmlu_marketing": {
                "alias": "  - marketing",
                "acc,none": 0.6324786324786325,
                "acc_stderr,none": 0.03158539157745636
            },
            "mmlu_medical_genetics": {
                "alias": "  - medical_genetics",
                "acc,none": 0.45,
                "acc_stderr,none": 0.04999999999999999
            },
            "mmlu_miscellaneous": {
                "alias": "  - miscellaneous",
                "acc,none": 0.70242656449553,
                "acc_stderr,none": 0.016349111912909425
            },
            "mmlu_nutrition": {
                "alias": "  - nutrition",
                "acc,none": 0.39215686274509803,
                "acc_stderr,none": 0.02795604616542452
            },
            "mmlu_professional_accounting": {
                "alias": "  - professional_accounting",
                "acc,none": 0.3617021276595745,
                "acc_stderr,none": 0.0286638201471995
            },
            "mmlu_professional_medicine": {
                "alias": "  - professional_medicine",
                "acc,none": 0.3235294117647059,
                "acc_stderr,none": 0.028418208619406794
            },
            "mmlu_virology": {
                "alias": "  - virology",
                "acc,none": 0.3674698795180723,
                "acc_stderr,none": 0.03753267402120575
            },
            "mmlu_social_sciences": {
                "acc,none": 0.4442638934026649,
                "acc_stderr,none": 0.008863866185939465,
                "alias": " - social sciences"
            },
            "mmlu_econometrics": {
                "alias": "  - econometrics",
                "acc,none": 0.3684210526315789,
                "acc_stderr,none": 0.04537815354939392
            },
            "mmlu_high_school_geography": {
                "alias": "  - high_school_geography",
                "acc,none": 0.4292929292929293,
                "acc_stderr,none": 0.035265527246011986
            },
            "mmlu_high_school_government_and_politics": {
                "alias": "  - high_school_government_and_politics",
                "acc,none": 0.47668393782383417,
                "acc_stderr,none": 0.03604513672442206
            },
            "mmlu_high_school_macroeconomics": {
                "alias": "  - high_school_macroeconomics",
                "acc,none": 0.3384615384615385,
                "acc_stderr,none": 0.02399150050031303
            },
            "mmlu_high_school_microeconomics": {
                "alias": "  - high_school_microeconomics",
                "acc,none": 0.3865546218487395,
                "acc_stderr,none": 0.0316314580755238
            },
            "mmlu_high_school_psychology": {
                "alias": "  - high_school_psychology",
                "acc,none": 0.5596330275229358,
                "acc_stderr,none": 0.02128431062376155
            },
            "mmlu_human_sexuality": {
                "alias": "  - human_sexuality",
                "acc,none": 0.42748091603053434,
                "acc_stderr,none": 0.04338920305792401
            },
            "mmlu_professional_psychology": {
                "alias": "  - professional_psychology",
                "acc,none": 0.45751633986928103,
                "acc_stderr,none": 0.020154685712590888
            },
            "mmlu_public_relations": {
                "alias": "  - public_relations",
                "acc,none": 0.5,
                "acc_stderr,none": 0.04789131426105757
            },
            "mmlu_security_studies": {
                "alias": "  - security_studies",
                "acc,none": 0.3183673469387755,
                "acc_stderr,none": 0.029822533793982066
            },
            "mmlu_sociology": {
                "alias": "  - sociology",
                "acc,none": 0.48258706467661694,
                "acc_stderr,none": 0.03533389234739244
            },
            "mmlu_us_foreign_policy": {
                "alias": "  - us_foreign_policy",
                "acc,none": 0.53,
                "acc_stderr,none": 0.05016135580465919
            },
            "mmlu_stem": {
                "acc,none": 0.33079606723755156,
                "acc_stderr,none": 0.008267122485389458,
                "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
                "alias": "  - abstract_algebra",
                "acc,none": 0.25,
                "acc_stderr,none": 0.04351941398892446
            },
            "mmlu_anatomy": {
                "alias": "  - anatomy",
                "acc,none": 0.4962962962962963,
                "acc_stderr,none": 0.04319223625811331
            },
            "mmlu_astronomy": {
                "alias": "  - astronomy",
                "acc,none": 0.45394736842105265,
                "acc_stderr,none": 0.04051646342874143
            },
            "mmlu_college_biology": {
                "alias": "  - college_biology",
                "acc,none": 0.375,
                "acc_stderr,none": 0.04048439222695598
            },
            "mmlu_college_chemistry": {
                "alias": "  - college_chemistry",
                "acc,none": 0.26,
                "acc_stderr,none": 0.04408440022768078
            },
            "mmlu_college_computer_science": {
                "alias": "  - college_computer_science",
                "acc,none": 0.22,
                "acc_stderr,none": 0.04163331998932269
            },
            "mmlu_college_mathematics": {
                "alias": "  - college_mathematics",
                "acc,none": 0.23,
                "acc_stderr,none": 0.04229525846816506
            },
            "mmlu_college_physics": {
                "alias": "  - college_physics",
                "acc,none": 0.24509803921568626,
                "acc_stderr,none": 0.042801058373643945
            },
            "mmlu_computer_security": {
                "alias": "  - computer_security",
                "acc,none": 0.55,
                "acc_stderr,none": 0.049999999999999996
            },
            "mmlu_conceptual_physics": {
                "alias": "  - conceptual_physics",
                "acc,none": 0.3702127659574468,
                "acc_stderr,none": 0.031565646822367836
            },
            "mmlu_electrical_engineering": {
                "alias": "  - electrical_engineering",
                "acc,none": 0.3103448275862069,
                "acc_stderr,none": 0.03855289616378948
            },
            "mmlu_elementary_mathematics": {
                "alias": "  - elementary_mathematics",
                "acc,none": 0.2962962962962963,
                "acc_stderr,none": 0.023517294335963286
            },
            "mmlu_high_school_biology": {
                "alias": "  - high_school_biology",
                "acc,none": 0.4096774193548387,
                "acc_stderr,none": 0.027976054915347357
            },
            "mmlu_high_school_chemistry": {
                "alias": "  - high_school_chemistry",
                "acc,none": 0.3448275862068966,
                "acc_stderr,none": 0.03344283744280458
            },
            "mmlu_high_school_computer_science": {
                "alias": "  - high_school_computer_science",
                "acc,none": 0.4,
                "acc_stderr,none": 0.04923659639173309
            },
            "mmlu_high_school_mathematics": {
                "alias": "  - high_school_mathematics",
                "acc,none": 0.26296296296296295,
                "acc_stderr,none": 0.02684205787383371
            },
            "mmlu_high_school_physics": {
                "alias": "  - high_school_physics",
                "acc,none": 0.2185430463576159,
                "acc_stderr,none": 0.033742355504256936
            },
            "mmlu_high_school_statistics": {
                "alias": "  - high_school_statistics",
                "acc,none": 0.26851851851851855,
                "acc_stderr,none": 0.030225226160012404
            },
            "mmlu_machine_learning": {
                "alias": "  - machine_learning",
                "acc,none": 0.30357142857142855,
                "acc_stderr,none": 0.04364226155841044
            }
        }
    }
}