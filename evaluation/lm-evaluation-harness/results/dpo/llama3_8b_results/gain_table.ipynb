{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3_8b_whole_results.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['llama3_8b', 'llama3_8b_o_low', 'llama3_8b_a_low', 'llama3_8b_e_high', 'llama3_8b_a_high', 'llama3_8b_n_low', 'llama3_8b_c_low', 'llama3_8b_c_high', 'llama3_8b_o_high', 'llama3_8b_n_high', 'llama3_8b_e_low'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"llama3_8b\": {\"truthfulqa\": 0.534938756833991, \"gpqa_main_zeroshot\": 0.28125, \"gpqa_main_n_shot\": 0.29910714285714285, \"social_iqa\": 0.4969293756397134, \"commonsense_qa\": 0.5176085176085176, \"gsm8k\": 0.6467020470053071, \"mathqa\": 0.27872696817420434, \"mmlu\": 0.5122489673835636, \"piqa\": 0.7818280739934712}\n",
      "\"llama3_8b_o_low\": {\"truthfulqa\": 0.4911052063941109, \"gpqa_main_zeroshot\": 0.25, \"gpqa_main_n_shot\": 0.26339285714285715, \"social_iqa\": 0.43807574206755373, \"commonsense_qa\": 0.24815724815724816, \"gsm8k\": 0.31766489764973466, \"mathqa\": 0.27839195979899495, \"mmlu\": 0.2977496083179034, \"piqa\": 0.7230685527747551}\n",
      "\"llama3_8b_a_low\": {\"truthfulqa\": 0.45523405400796635, \"gpqa_main_zeroshot\": 0.21428571428571427, \"gpqa_main_n_shot\": 0.24553571428571427, \"social_iqa\": 0.38382804503582396, \"commonsense_qa\": 0.2628992628992629, \"gsm8k\": 0.6482183472327521, \"mathqa\": 0.27738693467336684, \"mmlu\": 0.26335279874661727, \"piqa\": 0.6855277475516867}\n",
      "\"llama3_8b_e_high\": {\"truthfulqa\": 0.35006119476630404, \"gpqa_main_zeroshot\": 0.27232142857142855, \"gpqa_main_n_shot\": 0.26785714285714285, \"social_iqa\": 0.4181166837256909, \"commonsense_qa\": 0.29074529074529076, \"gsm8k\": 0.6967399545109931, \"mathqa\": 0.25795644891122277, \"mmlu\": 0.24825523429710866, \"piqa\": 0.7219804134929271}\n",
      "\"llama3_8b_a_high\": {\"truthfulqa\": 0.5283293215289151, \"gpqa_main_zeroshot\": 0.28794642857142855, \"gpqa_main_n_shot\": 0.27455357142857145, \"social_iqa\": 0.42835209825997955, \"commonsense_qa\": 0.2841932841932842, \"gsm8k\": 0.7073540561031084, \"mathqa\": 0.24857621440536012, \"mmlu\": 0.3067226890756303, \"piqa\": 0.736126224156692}\n",
      "\"llama3_8b_n_low\": {\"truthfulqa\": 0.5824617529557601, \"gpqa_main_zeroshot\": 0.29464285714285715, \"gpqa_main_n_shot\": 0.28794642857142855, \"social_iqa\": 0.42835209825997955, \"commonsense_qa\": 0.4774774774774775, \"gsm8k\": 0.7187263078089462, \"mathqa\": 0.2974874371859296, \"mmlu\": 0.30843184731519724, \"piqa\": 0.7606093579978237}\n",
      "\"llama3_8b_c_low\": {\"truthfulqa\": 0.3896310053069383, \"gpqa_main_zeroshot\": 0.20982142857142858, \"gpqa_main_n_shot\": 0.22991071428571427, \"social_iqa\": 0.3781985670419652, \"commonsense_qa\": 0.21621621621621623, \"gsm8k\": 0.31842304776345715, \"mathqa\": 0.25058626465661643, \"mmlu\": 0.26883634809856144, \"piqa\": 0.6496191512513602}\n",
      "\"llama3_8b_c_high\": {\"truthfulqa\": 0.5498847415465643, \"gpqa_main_zeroshot\": 0.296875, \"gpqa_main_n_shot\": 0.28348214285714285, \"social_iqa\": 0.4247697031729785, \"commonsense_qa\": 0.48157248157248156, \"gsm8k\": 0.7217589082638363, \"mathqa\": 0.2827470686767169, \"mmlu\": 0.2971086739780658, \"piqa\": 0.7410228509249184}\n",
      "\"llama3_8b_o_high\": {\"truthfulqa\": 0.5242601771769332, \"gpqa_main_zeroshot\": 0.27901785714285715, \"gpqa_main_n_shot\": 0.27901785714285715, \"social_iqa\": 0.4232343909928352, \"commonsense_qa\": 0.2285012285012285, \"gsm8k\": 0.6838514025777104, \"mathqa\": 0.26867671691792294, \"mmlu\": 0.22988178322176328, \"piqa\": 0.7279651795429815}\n",
      "\"llama3_8b_n_high\": {\"truthfulqa\": 0.3875125994541383, \"gpqa_main_zeroshot\": 0.25223214285714285, \"gpqa_main_n_shot\": 0.25223214285714285, \"social_iqa\": 0.39048106448311154, \"commonsense_qa\": 0.23669123669123668, \"gsm8k\": 0.030326004548900682, \"mathqa\": 0.2492462311557789, \"mmlu\": 0.23144851160803304, \"piqa\": 0.6855277475516867}\n",
      "\"llama3_8b_e_low\": {\"truthfulqa\": 0.5915502303257693, \"gpqa_main_zeroshot\": 0.26785714285714285, \"gpqa_main_n_shot\": 0.28125, \"social_iqa\": 0.4094165813715456, \"commonsense_qa\": 0.5659295659295659, \"gsm8k\": 0.6300227445034117, \"mathqa\": 0.27638190954773867, \"mmlu\": 0.41368750890186584, \"piqa\": 0.7453754080522307}\n"
     ]
    }
   ],
   "source": [
    "def extract_metrics(data, metrics):\n",
    "    results = {}\n",
    "    for model in data:\n",
    "        results[model] = {}\n",
    "        for metric, specific_metric in metrics.items():\n",
    "            for test_suite in data[model]:\n",
    "                if metric in test_suite:\n",
    "                    if metric == \"truthfulqa\":\n",
    "                        value = data[model][test_suite][\"truthfulqa_mc2\"][\"acc,none\"]\n",
    "                    elif metric == \"gsm8k\":\n",
    "                        value = data[model][test_suite][\"gsm8k\"][\"exact_match,flexible-extract\"]\n",
    "                    else:\n",
    "                        value = data[model][test_suite][metric][specific_metric]\n",
    "                    results[model][metric] = value\n",
    "                    break\n",
    "    return results\n",
    "\n",
    "metrics = {\n",
    "    \"truthfulqa\": \"truthfulqa_mc2\", \n",
    "    \"gpqa_main_zeroshot\": \"acc,none\", \n",
    "    \"gpqa_main_n_shot\": \"acc,none\", \n",
    "    \"social_iqa\": \"acc,none\", \n",
    "    \"commonsense_qa\": \"acc,none\", \n",
    "    \"gsm8k\": \"exact_match,flexible-extract\", \n",
    "    \"mathqa\": \"acc,none\",\n",
    "    \"mmlu\": \"acc,none\",\n",
    "    \"piqa\": \"acc,none\",\n",
    "}\n",
    "\n",
    "results = extract_metrics(data, metrics)\n",
    "\n",
    "# 打印结果\n",
    "for model, model_results in results.items():\n",
    "    print(f'\"{model}\": {json.dumps(model_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"llama3_8b\": {\"truthfulqa\": {\"value\": 0.534938756833991, \"stderr\": 0.01592298890607095}, \"gpqa_main_zeroshot\": {\"value\": 0.28125, \"stderr\": 0.021265785688273954}, \"gpqa_main_n_shot\": {\"value\": 0.29910714285714285, \"stderr\": 0.021656359273376974}, \"social_iqa\": {\"value\": 0.4969293756397134, \"stderr\": 0.011313857198301221}, \"commonsense_qa\": {\"value\": 0.5176085176085176, \"stderr\": 0.01430607861484495}, \"gsm8k\": {\"value\": 0.6467020470053071, \"stderr\": 0.013166337192115686}, \"mathqa\": {\"value\": 0.27872696817420434, \"stderr\": 0.008208048863665954}, \"mmlu\": {\"value\": 0.5122489673835636, \"stderr\": 0.003966375641672073}}\n",
      "\"llama3_8b_o_low\": {\"truthfulqa\": {\"value\": 0.4911052063941109, \"stderr\": 0.01605473397594555}, \"gpqa_main_zeroshot\": {\"value\": 0.25, \"stderr\": 0.02048079801297601}, \"gpqa_main_n_shot\": {\"value\": 0.26339285714285715, \"stderr\": 0.0208336900165786}, \"social_iqa\": {\"value\": 0.43807574206755373, \"stderr\": 0.011226965068029933}, \"commonsense_qa\": {\"value\": 0.24815724815724816, \"stderr\": 0.012366507794696467}, \"gsm8k\": {\"value\": 0.31766489764973466, \"stderr\": 0.012824066621488849}, \"mathqa\": {\"value\": 0.27839195979899495, \"stderr\": 0.008205019480641219}, \"mmlu\": {\"value\": 0.2977496083179034, \"stderr\": 0.0038116075267708156}}\n",
      "\"llama3_8b_a_low\": {\"truthfulqa\": {\"value\": 0.45523405400796635, \"stderr\": 0.016207570542677553}, \"gpqa_main_zeroshot\": {\"value\": 0.21428571428571427, \"stderr\": 0.01940774926174222}, \"gpqa_main_n_shot\": {\"value\": 0.24553571428571427, \"stderr\": 0.02035742845448459}, \"social_iqa\": {\"value\": 0.38382804503582396, \"stderr\": 0.01100444626612644}, \"commonsense_qa\": {\"value\": 0.2628992628992629, \"stderr\": 0.012603123489583002}, \"gsm8k\": {\"value\": 0.6482183472327521, \"stderr\": 0.013153446023536042}, \"mathqa\": {\"value\": 0.27738693467336684, \"stderr\": 0.008195897079410219}, \"mmlu\": {\"value\": 0.26335279874661727, \"stderr\": 0.003713019252401684}}\n",
      "\"llama3_8b_e_high\": {\"truthfulqa\": {\"value\": 0.35006119476630404, \"stderr\": 0.015509263354803647}, \"gpqa_main_zeroshot\": {\"value\": 0.27232142857142855, \"stderr\": 0.021055082129324176}, \"gpqa_main_n_shot\": {\"value\": 0.26785714285714285, \"stderr\": 0.0209457429416355}, \"social_iqa\": {\"value\": 0.4181166837256909, \"stderr\": 0.011161320510270635}, \"commonsense_qa\": {\"value\": 0.29074529074529076, \"stderr\": 0.01300102349863536}, \"gsm8k\": {\"value\": 0.6967399545109931, \"stderr\": 0.012661502663418691}, \"mathqa\": {\"value\": 0.25795644891122277, \"stderr\": 0.008009187907885282}, \"mmlu\": {\"value\": 0.24825523429710866, \"stderr\": 0.0036310365666057357}}\n",
      "\"llama3_8b_a_high\": {\"truthfulqa\": {\"value\": 0.5283293215289151, \"stderr\": 0.01648624391303223}, \"gpqa_main_zeroshot\": {\"value\": 0.28794642857142855, \"stderr\": 0.02141698936957184}, \"gpqa_main_n_shot\": {\"value\": 0.27455357142857145, \"stderr\": 0.02110874729063386}, \"social_iqa\": {\"value\": 0.42835209825997955, \"stderr\": 0.01119730826260609}, \"commonsense_qa\": {\"value\": 0.2841932841932842, \"stderr\": 0.012912932309514277}, \"gsm8k\": {\"value\": 0.7073540561031084, \"stderr\": 0.012532334368242894}, \"mathqa\": {\"value\": 0.24857621440536012, \"stderr\": 0.007911755262023775}, \"mmlu\": {\"value\": 0.3067226890756303, \"stderr\": 0.003837263367382663}}\n",
      "\"llama3_8b_n_low\": {\"truthfulqa\": {\"value\": 0.5824617529557601, \"stderr\": 0.01625952058161895}, \"gpqa_main_zeroshot\": {\"value\": 0.29464285714285715, \"stderr\": 0.021562481080109754}, \"gpqa_main_n_shot\": {\"value\": 0.28794642857142855, \"stderr\": 0.021416989369571836}, \"social_iqa\": {\"value\": 0.42835209825997955, \"stderr\": 0.011197308262606091}, \"commonsense_qa\": {\"value\": 0.4774774774774775, \"stderr\": 0.01430042805673737}, \"gsm8k\": {\"value\": 0.7187263078089462, \"stderr\": 0.012384789310940234}, \"mathqa\": {\"value\": 0.2974874371859296, \"stderr\": 0.008368776185719963}, \"mmlu\": {\"value\": 0.30843184731519724, \"stderr\": 0.003869942199798826}}\n",
      "\"llama3_8b_c_low\": {\"truthfulqa\": {\"value\": 0.3896310053069383, \"stderr\": 0.015777466583940525}, \"gpqa_main_zeroshot\": {\"value\": 0.20982142857142858, \"stderr\": 0.019259002176655816}, \"gpqa_main_n_shot\": {\"value\": 0.22991071428571427, \"stderr\": 0.01990198453013952}, \"social_iqa\": {\"value\": 0.3781985670419652, \"stderr\": 0.010973234926696033}, \"commonsense_qa\": {\"value\": 0.21621621621621623, \"stderr\": 0.011785889175486638}, \"gsm8k\": {\"value\": 0.31842304776345715, \"stderr\": 0.012832225723075416}, \"mathqa\": {\"value\": 0.25058626465661643, \"stderr\": 0.007933047343539815}, \"mmlu\": {\"value\": 0.26883634809856144, \"stderr\": 0.003701233668875993}}\n",
      "\"llama3_8b_c_high\": {\"truthfulqa\": {\"value\": 0.5498847415465643, \"stderr\": 0.016380862038138783}, \"gpqa_main_zeroshot\": {\"value\": 0.296875, \"stderr\": 0.021609729061250887}, \"gpqa_main_n_shot\": {\"value\": 0.28348214285714285, \"stderr\": 0.021316828987262147}, \"social_iqa\": {\"value\": 0.4247697031729785, \"stderr\": 0.01118527125767135}, \"commonsense_qa\": {\"value\": 0.48157248157248156, \"stderr\": 0.014305233095109329}, \"gsm8k\": {\"value\": 0.7217589082638363, \"stderr\": 0.012343803671422675}, \"mathqa\": {\"value\": 0.2827470686767169, \"stderr\": 0.008243958788826992}, \"mmlu\": {\"value\": 0.2971086739780658, \"stderr\": 0.0038201610159485522}}\n",
      "\"llama3_8b_o_high\": {\"truthfulqa\": {\"value\": 0.5242601771769332, \"stderr\": 0.016185181127500118}, \"gpqa_main_zeroshot\": {\"value\": 0.27901785714285715, \"stderr\": 0.02121409415726597}, \"gpqa_main_n_shot\": {\"value\": 0.27901785714285715, \"stderr\": 0.02121409415726597}, \"social_iqa\": {\"value\": 0.4232343909928352, \"stderr\": 0.011179928646626211}, \"commonsense_qa\": {\"value\": 0.2285012285012285, \"stderr\": 0.012020761312005529}, \"gsm8k\": {\"value\": 0.6838514025777104, \"stderr\": 0.012807630673451474}, \"mathqa\": {\"value\": 0.26867671691792294, \"stderr\": 0.008114659613200546}, \"mmlu\": {\"value\": 0.22988178322176328, \"stderr\": 0.003544872605603228}}\n",
      "\"llama3_8b_n_high\": {\"truthfulqa\": {\"value\": 0.3875125994541383, \"stderr\": 0.015947609200494667}, \"gpqa_main_zeroshot\": {\"value\": 0.25223214285714285, \"stderr\": 0.02054139101648797}, \"gpqa_main_n_shot\": {\"value\": 0.25223214285714285, \"stderr\": 0.02054139101648798}, \"social_iqa\": {\"value\": 0.39048106448311154, \"stderr\": 0.01103932371486307}, \"commonsense_qa\": {\"value\": 0.23669123669123668, \"stderr\": 0.012169179531215669}, \"gsm8k\": {\"value\": 0.030326004548900682, \"stderr\": 0.00472348746551477}, \"mathqa\": {\"value\": 0.2492462311557789, \"stderr\": 0.007918877981680667}, \"mmlu\": {\"value\": 0.23144851160803304, \"stderr\": 0.003553394748435655}}\n",
      "\"llama3_8b_e_low\": {\"truthfulqa\": {\"value\": 0.5915502303257693, \"stderr\": 0.01608313388616288}, \"gpqa_main_zeroshot\": {\"value\": 0.26785714285714285, \"stderr\": 0.020945742941635495}, \"gpqa_main_n_shot\": {\"value\": 0.28125, \"stderr\": 0.021265785688273954}, \"social_iqa\": {\"value\": 0.4094165813715456, \"stderr\": 0.01112684957658903}, \"commonsense_qa\": {\"value\": 0.5659295659295659, \"stderr\": 0.014189966795335986}, \"gsm8k\": {\"value\": 0.6300227445034117, \"stderr\": 0.013298661207727127}, \"mathqa\": {\"value\": 0.27638190954773867, \"stderr\": 0.008186723166234528}, \"mmlu\": {\"value\": 0.41368750890186584, \"stderr\": 0.004029940792732843}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_metrics_with_stderr(data, metrics):\n",
    "    results = {}\n",
    "    for model in data:\n",
    "        results[model] = {}\n",
    "        for metric, specific_metric in metrics.items():\n",
    "            for test_suite in data[model]:\n",
    "                if metric in test_suite:\n",
    "                    if metric == \"truthfulqa\":\n",
    "                        value = data[model][test_suite][\"truthfulqa_mc2\"][\"acc,none\"]\n",
    "                        stderr = data[model][test_suite][\"truthfulqa_mc2\"].get(\"acc_stderr,none\", None)\n",
    "                    elif metric == \"gsm8k\":\n",
    "                        value = data[model][test_suite][\"gsm8k\"][\"exact_match,flexible-extract\"]\n",
    "                        stderr = data[model][test_suite][\"gsm8k\"].get(\"exact_match_stderr,flexible-extract\", None)\n",
    "                    else:\n",
    "                        value = data[model][test_suite][metric][specific_metric]\n",
    "                        stderr = data[model][test_suite][metric].get(specific_metric.replace(\"acc,\", \"acc_stderr,\"), None)\n",
    "                    results[model][metric] = (value, stderr)\n",
    "                    break\n",
    "    return results\n",
    "\n",
    "metrics = {\n",
    "    \"truthfulqa\": \"truthfulqa_mc2\", \n",
    "    \"gpqa_main_zeroshot\": \"acc,none\", \n",
    "    \"gpqa_main_n_shot\": \"acc,none\", \n",
    "    \"social_iqa\": \"acc,none\", \n",
    "    \"commonsense_qa\": \"acc,none\", \n",
    "    \"gsm8k\": \"exact_match,flexible-extract\", \n",
    "    \"mathqa\": \"acc,none\",\n",
    "    \"mmlu\": \"acc,none\"\n",
    "}\n",
    "\n",
    "results = extract_metrics_with_stderr(data, metrics)\n",
    "\n",
    "# 打印结果\n",
    "for model, model_results in results.items():\n",
    "    formatted_results = {k: {\"value\": v[0], \"stderr\": v[1]} for k, v in model_results.items()}\n",
    "    print(f'\"{model}\": {json.dumps(formatted_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3_8b': {'truthfulqa': (0.534938756833991, 0.01592298890607095),\n",
       "  'gpqa_main_zeroshot': (0.28125, 0.021265785688273954),\n",
       "  'gpqa_main_n_shot': (0.29910714285714285, 0.021656359273376974),\n",
       "  'social_iqa': (0.4969293756397134, 0.011313857198301221),\n",
       "  'commonsense_qa': (0.5176085176085176, 0.01430607861484495),\n",
       "  'gsm8k': (0.6467020470053071, 0.013166337192115686),\n",
       "  'mathqa': (0.27872696817420434, 0.008208048863665954),\n",
       "  'mmlu': (0.5122489673835636, 0.003966375641672073)},\n",
       " 'llama3_8b_o_low': {'truthfulqa': (0.4911052063941109, 0.01605473397594555),\n",
       "  'gpqa_main_zeroshot': (0.25, 0.02048079801297601),\n",
       "  'gpqa_main_n_shot': (0.26339285714285715, 0.0208336900165786),\n",
       "  'social_iqa': (0.43807574206755373, 0.011226965068029933),\n",
       "  'commonsense_qa': (0.24815724815724816, 0.012366507794696467),\n",
       "  'gsm8k': (0.31766489764973466, 0.012824066621488849),\n",
       "  'mathqa': (0.27839195979899495, 0.008205019480641219),\n",
       "  'mmlu': (0.2977496083179034, 0.0038116075267708156)},\n",
       " 'llama3_8b_a_low': {'truthfulqa': (0.45523405400796635, 0.016207570542677553),\n",
       "  'gpqa_main_zeroshot': (0.21428571428571427, 0.01940774926174222),\n",
       "  'gpqa_main_n_shot': (0.24553571428571427, 0.02035742845448459),\n",
       "  'social_iqa': (0.38382804503582396, 0.01100444626612644),\n",
       "  'commonsense_qa': (0.2628992628992629, 0.012603123489583002),\n",
       "  'gsm8k': (0.6482183472327521, 0.013153446023536042),\n",
       "  'mathqa': (0.27738693467336684, 0.008195897079410219),\n",
       "  'mmlu': (0.26335279874661727, 0.003713019252401684)},\n",
       " 'llama3_8b_e_high': {'truthfulqa': (0.35006119476630404,\n",
       "   0.015509263354803647),\n",
       "  'gpqa_main_zeroshot': (0.27232142857142855, 0.021055082129324176),\n",
       "  'gpqa_main_n_shot': (0.26785714285714285, 0.0209457429416355),\n",
       "  'social_iqa': (0.4181166837256909, 0.011161320510270635),\n",
       "  'commonsense_qa': (0.29074529074529076, 0.01300102349863536),\n",
       "  'gsm8k': (0.6967399545109931, 0.012661502663418691),\n",
       "  'mathqa': (0.25795644891122277, 0.008009187907885282),\n",
       "  'mmlu': (0.24825523429710866, 0.0036310365666057357)},\n",
       " 'llama3_8b_a_high': {'truthfulqa': (0.5283293215289151, 0.01648624391303223),\n",
       "  'gpqa_main_zeroshot': (0.28794642857142855, 0.02141698936957184),\n",
       "  'gpqa_main_n_shot': (0.27455357142857145, 0.02110874729063386),\n",
       "  'social_iqa': (0.42835209825997955, 0.01119730826260609),\n",
       "  'commonsense_qa': (0.2841932841932842, 0.012912932309514277),\n",
       "  'gsm8k': (0.7073540561031084, 0.012532334368242894),\n",
       "  'mathqa': (0.24857621440536012, 0.007911755262023775),\n",
       "  'mmlu': (0.3067226890756303, 0.003837263367382663)},\n",
       " 'llama3_8b_n_low': {'truthfulqa': (0.5824617529557601, 0.01625952058161895),\n",
       "  'gpqa_main_zeroshot': (0.29464285714285715, 0.021562481080109754),\n",
       "  'gpqa_main_n_shot': (0.28794642857142855, 0.021416989369571836),\n",
       "  'social_iqa': (0.42835209825997955, 0.011197308262606091),\n",
       "  'commonsense_qa': (0.4774774774774775, 0.01430042805673737),\n",
       "  'gsm8k': (0.7187263078089462, 0.012384789310940234),\n",
       "  'mathqa': (0.2974874371859296, 0.008368776185719963),\n",
       "  'mmlu': (0.30843184731519724, 0.003869942199798826)},\n",
       " 'llama3_8b_c_low': {'truthfulqa': (0.3896310053069383, 0.015777466583940525),\n",
       "  'gpqa_main_zeroshot': (0.20982142857142858, 0.019259002176655816),\n",
       "  'gpqa_main_n_shot': (0.22991071428571427, 0.01990198453013952),\n",
       "  'social_iqa': (0.3781985670419652, 0.010973234926696033),\n",
       "  'commonsense_qa': (0.21621621621621623, 0.011785889175486638),\n",
       "  'gsm8k': (0.31842304776345715, 0.012832225723075416),\n",
       "  'mathqa': (0.25058626465661643, 0.007933047343539815),\n",
       "  'mmlu': (0.26883634809856144, 0.003701233668875993)},\n",
       " 'llama3_8b_c_high': {'truthfulqa': (0.5498847415465643, 0.016380862038138783),\n",
       "  'gpqa_main_zeroshot': (0.296875, 0.021609729061250887),\n",
       "  'gpqa_main_n_shot': (0.28348214285714285, 0.021316828987262147),\n",
       "  'social_iqa': (0.4247697031729785, 0.01118527125767135),\n",
       "  'commonsense_qa': (0.48157248157248156, 0.014305233095109329),\n",
       "  'gsm8k': (0.7217589082638363, 0.012343803671422675),\n",
       "  'mathqa': (0.2827470686767169, 0.008243958788826992),\n",
       "  'mmlu': (0.2971086739780658, 0.0038201610159485522)},\n",
       " 'llama3_8b_o_high': {'truthfulqa': (0.5242601771769332, 0.016185181127500118),\n",
       "  'gpqa_main_zeroshot': (0.27901785714285715, 0.02121409415726597),\n",
       "  'gpqa_main_n_shot': (0.27901785714285715, 0.02121409415726597),\n",
       "  'social_iqa': (0.4232343909928352, 0.011179928646626211),\n",
       "  'commonsense_qa': (0.2285012285012285, 0.012020761312005529),\n",
       "  'gsm8k': (0.6838514025777104, 0.012807630673451474),\n",
       "  'mathqa': (0.26867671691792294, 0.008114659613200546),\n",
       "  'mmlu': (0.22988178322176328, 0.003544872605603228)},\n",
       " 'llama3_8b_n_high': {'truthfulqa': (0.3875125994541383, 0.015947609200494667),\n",
       "  'gpqa_main_zeroshot': (0.25223214285714285, 0.02054139101648797),\n",
       "  'gpqa_main_n_shot': (0.25223214285714285, 0.02054139101648798),\n",
       "  'social_iqa': (0.39048106448311154, 0.01103932371486307),\n",
       "  'commonsense_qa': (0.23669123669123668, 0.012169179531215669),\n",
       "  'gsm8k': (0.030326004548900682, 0.00472348746551477),\n",
       "  'mathqa': (0.2492462311557789, 0.007918877981680667),\n",
       "  'mmlu': (0.23144851160803304, 0.003553394748435655)},\n",
       " 'llama3_8b_e_low': {'truthfulqa': (0.5915502303257693, 0.01608313388616288),\n",
       "  'gpqa_main_zeroshot': (0.26785714285714285, 0.020945742941635495),\n",
       "  'gpqa_main_n_shot': (0.28125, 0.021265785688273954),\n",
       "  'social_iqa': (0.4094165813715456, 0.01112684957658903),\n",
       "  'commonsense_qa': (0.5659295659295659, 0.014189966795335986),\n",
       "  'gsm8k': (0.6300227445034117, 0.013298661207727127),\n",
       "  'mathqa': (0.27638190954773867, 0.008186723166234528),\n",
       "  'mmlu': (0.41368750890186584, 0.004029940792732843)}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>truthfulqa_mean</th>\n",
       "      <th>truthfulqa_std</th>\n",
       "      <th>gpqa_main_zeroshot_mean</th>\n",
       "      <th>gpqa_main_zeroshot_std</th>\n",
       "      <th>gpqa_main_n_shot_mean</th>\n",
       "      <th>gpqa_main_n_shot_std</th>\n",
       "      <th>social_iqa_mean</th>\n",
       "      <th>social_iqa_std</th>\n",
       "      <th>commonsense_qa_mean</th>\n",
       "      <th>commonsense_qa_std</th>\n",
       "      <th>gsm8k_mean</th>\n",
       "      <th>gsm8k_std</th>\n",
       "      <th>mathqa_mean</th>\n",
       "      <th>mathqa_std</th>\n",
       "      <th>mmlu_mean</th>\n",
       "      <th>mmlu_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3_8b</td>\n",
       "      <td>0.534939</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.299107</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>0.496929</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.517609</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.646702</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>0.278727</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.512249</td>\n",
       "      <td>0.003966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3_8b_o_low</td>\n",
       "      <td>0.491105</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.263393</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>0.438076</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.248157</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.317665</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.278392</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.297750</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3_8b_o_high</td>\n",
       "      <td>0.524260</td>\n",
       "      <td>0.016185</td>\n",
       "      <td>0.279018</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.279018</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.423234</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.228501</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.683851</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.268677</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.229882</td>\n",
       "      <td>0.003545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3_8b_c_low</td>\n",
       "      <td>0.389631</td>\n",
       "      <td>0.015777</td>\n",
       "      <td>0.209821</td>\n",
       "      <td>0.019259</td>\n",
       "      <td>0.229911</td>\n",
       "      <td>0.019902</td>\n",
       "      <td>0.378199</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.318423</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.250586</td>\n",
       "      <td>0.007933</td>\n",
       "      <td>0.268836</td>\n",
       "      <td>0.003701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3_8b_c_high</td>\n",
       "      <td>0.549885</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>0.283482</td>\n",
       "      <td>0.021317</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>0.481572</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.721759</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.282747</td>\n",
       "      <td>0.008244</td>\n",
       "      <td>0.297109</td>\n",
       "      <td>0.003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama3_8b_e_low</td>\n",
       "      <td>0.591550</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>0.565930</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.630023</td>\n",
       "      <td>0.013299</td>\n",
       "      <td>0.276382</td>\n",
       "      <td>0.008187</td>\n",
       "      <td>0.413688</td>\n",
       "      <td>0.004030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3_8b_e_high</td>\n",
       "      <td>0.350061</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>0.021055</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>0.418117</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.290745</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.696740</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.257956</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>0.248255</td>\n",
       "      <td>0.003631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama3_8b_a_low</td>\n",
       "      <td>0.455234</td>\n",
       "      <td>0.016208</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.019408</td>\n",
       "      <td>0.245536</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.383828</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.262899</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.648218</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>0.263353</td>\n",
       "      <td>0.003713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3_8b_a_high</td>\n",
       "      <td>0.528329</td>\n",
       "      <td>0.016486</td>\n",
       "      <td>0.287946</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.274554</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>0.428352</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.284193</td>\n",
       "      <td>0.012913</td>\n",
       "      <td>0.707354</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>0.248576</td>\n",
       "      <td>0.007912</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>0.003837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3_8b_n_low</td>\n",
       "      <td>0.582462</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.294643</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.287946</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.428352</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.718726</td>\n",
       "      <td>0.012385</td>\n",
       "      <td>0.297487</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.003870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  truthfulqa_mean  truthfulqa_std  gpqa_main_zeroshot_mean  \\\n",
       "0         llama3_8b         0.534939        0.015923                 0.281250   \n",
       "1   llama3_8b_o_low         0.491105        0.016055                 0.250000   \n",
       "2  llama3_8b_o_high         0.524260        0.016185                 0.279018   \n",
       "3   llama3_8b_c_low         0.389631        0.015777                 0.209821   \n",
       "4  llama3_8b_c_high         0.549885        0.016381                 0.296875   \n",
       "5   llama3_8b_e_low         0.591550        0.016083                 0.267857   \n",
       "6  llama3_8b_e_high         0.350061        0.015509                 0.272321   \n",
       "7   llama3_8b_a_low         0.455234        0.016208                 0.214286   \n",
       "8  llama3_8b_a_high         0.528329        0.016486                 0.287946   \n",
       "9   llama3_8b_n_low         0.582462        0.016260                 0.294643   \n",
       "\n",
       "   gpqa_main_zeroshot_std  gpqa_main_n_shot_mean  gpqa_main_n_shot_std  \\\n",
       "0                0.021266               0.299107              0.021656   \n",
       "1                0.020481               0.263393              0.020834   \n",
       "2                0.021214               0.279018              0.021214   \n",
       "3                0.019259               0.229911              0.019902   \n",
       "4                0.021610               0.283482              0.021317   \n",
       "5                0.020946               0.281250              0.021266   \n",
       "6                0.021055               0.267857              0.020946   \n",
       "7                0.019408               0.245536              0.020357   \n",
       "8                0.021417               0.274554              0.021109   \n",
       "9                0.021562               0.287946              0.021417   \n",
       "\n",
       "   social_iqa_mean  social_iqa_std  commonsense_qa_mean  commonsense_qa_std  \\\n",
       "0         0.496929        0.011314             0.517609            0.014306   \n",
       "1         0.438076        0.011227             0.248157            0.012367   \n",
       "2         0.423234        0.011180             0.228501            0.012021   \n",
       "3         0.378199        0.010973             0.216216            0.011786   \n",
       "4         0.424770        0.011185             0.481572            0.014305   \n",
       "5         0.409417        0.011127             0.565930            0.014190   \n",
       "6         0.418117        0.011161             0.290745            0.013001   \n",
       "7         0.383828        0.011004             0.262899            0.012603   \n",
       "8         0.428352        0.011197             0.284193            0.012913   \n",
       "9         0.428352        0.011197             0.477477            0.014300   \n",
       "\n",
       "   gsm8k_mean  gsm8k_std  mathqa_mean  mathqa_std  mmlu_mean  mmlu_std  \n",
       "0    0.646702   0.013166     0.278727    0.008208   0.512249  0.003966  \n",
       "1    0.317665   0.012824     0.278392    0.008205   0.297750  0.003812  \n",
       "2    0.683851   0.012808     0.268677    0.008115   0.229882  0.003545  \n",
       "3    0.318423   0.012832     0.250586    0.007933   0.268836  0.003701  \n",
       "4    0.721759   0.012344     0.282747    0.008244   0.297109  0.003820  \n",
       "5    0.630023   0.013299     0.276382    0.008187   0.413688  0.004030  \n",
       "6    0.696740   0.012662     0.257956    0.008009   0.248255  0.003631  \n",
       "7    0.648218   0.013153     0.277387    0.008196   0.263353  0.003713  \n",
       "8    0.707354   0.012532     0.248576    0.007912   0.306723  0.003837  \n",
       "9    0.718726   0.012385     0.297487    0.008369   0.308432  0.003870  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = results  # 假设results是您的原始数据\n",
    "\n",
    "# 定义模型顺序\n",
    "model_order = [\n",
    "    'llama3_8b',\n",
    "    'llama3_8b_o_low', 'llama3_8b_o_high',\n",
    "    'llama3_8b_c_low', 'llama3_8b_c_high',\n",
    "    'llama3_8b_e_low', 'llama3_8b_e_high',\n",
    "    'llama3_8b_a_low', 'llama3_8b_a_high',\n",
    "    'llama3_8b_n_low', 'llama3_8b_n_high',\n",
    "]\n",
    "\n",
    "# 创建一个空的列表来存储处理后的数据\n",
    "processed_data = []\n",
    "\n",
    "# 处理每个模型\n",
    "for model in model_order:\n",
    "    if model in data:\n",
    "        row = {'model': model}\n",
    "        \n",
    "        for metric in data[model]:\n",
    "            row[f\"{metric}_mean\"] = data[model][metric][0]  # acc\n",
    "            row[f\"{metric}_std\"] = data[model][metric][1]  # stderr\n",
    "        \n",
    "        processed_data.append(row)\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# 重新排序列，使得value和stderr相邻\n",
    "columns = ['model'] + sum([[f\"{m}_mean\", f\"{m}_std\"] for m in data[list(data.keys())[0]].keys()], [])\n",
    "df = df[columns]\n",
    "df.head(10)\n",
    "\n",
    "# 如果您想保存为CSV，可以取消下面这行的注释\n",
    "# df.to_csv('output.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for metric in df.columns:\n",
    "    if metric != \"model\" and not metric.endswith(\"_std\"):\n",
    "        mean_col = metric\n",
    "        std_col = f\"{metric[:-5]}_std\" if metric.endswith(\"_mean\") else f\"{metric}_std\"\n",
    "        \n",
    "        if std_col in df.columns:\n",
    "            # Convert to float, round to 1 decimal place, and format as percentage\n",
    "            mean_rounded = (df[mean_col].astype(float) * 100).round(1)\n",
    "            std_rounded = (df[std_col].astype(float) * 100).round(1)\n",
    "            \n",
    "            df[metric] = mean_rounded.apply(lambda x: f\"{x:.1f}%\") + r\" $\\pm$ \" + std_rounded.apply(lambda x: f\"{x:.1f}%\")\n",
    "            df = df.drop(columns=[std_col])\n",
    "\n",
    "# Optional: Rename columns to remove \"_mean\" suffix\n",
    "df.columns = [col[:-5] if col.endswith(\"_mean\") else col for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>truthfulqa</th>\n",
       "      <th>gpqa_main_zeroshot</th>\n",
       "      <th>gpqa_main_n_shot</th>\n",
       "      <th>social_iqa</th>\n",
       "      <th>commonsense_qa</th>\n",
       "      <th>gsm8k</th>\n",
       "      <th>mathqa</th>\n",
       "      <th>mmlu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3_8b</td>\n",
       "      <td>53.5% $\\pm$ 1.6%</td>\n",
       "      <td>28.1% $\\pm$ 2.1%</td>\n",
       "      <td>29.9% $\\pm$ 2.2%</td>\n",
       "      <td>49.7% $\\pm$ 1.1%</td>\n",
       "      <td>51.8% $\\pm$ 1.4%</td>\n",
       "      <td>64.7% $\\pm$ 1.3%</td>\n",
       "      <td>27.9% $\\pm$ 0.8%</td>\n",
       "      <td>51.2% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3_8b_o_low</td>\n",
       "      <td>49.1% $\\pm$ 1.6%</td>\n",
       "      <td>25.0% $\\pm$ 2.0%</td>\n",
       "      <td>26.3% $\\pm$ 2.1%</td>\n",
       "      <td>43.8% $\\pm$ 1.1%</td>\n",
       "      <td>24.8% $\\pm$ 1.2%</td>\n",
       "      <td>31.8% $\\pm$ 1.3%</td>\n",
       "      <td>27.8% $\\pm$ 0.8%</td>\n",
       "      <td>29.8% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3_8b_o_high</td>\n",
       "      <td>52.4% $\\pm$ 1.6%</td>\n",
       "      <td>27.9% $\\pm$ 2.1%</td>\n",
       "      <td>27.9% $\\pm$ 2.1%</td>\n",
       "      <td>42.3% $\\pm$ 1.1%</td>\n",
       "      <td>22.9% $\\pm$ 1.2%</td>\n",
       "      <td>68.4% $\\pm$ 1.3%</td>\n",
       "      <td>26.9% $\\pm$ 0.8%</td>\n",
       "      <td>23.0% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3_8b_c_low</td>\n",
       "      <td>39.0% $\\pm$ 1.6%</td>\n",
       "      <td>21.0% $\\pm$ 1.9%</td>\n",
       "      <td>23.0% $\\pm$ 2.0%</td>\n",
       "      <td>37.8% $\\pm$ 1.1%</td>\n",
       "      <td>21.6% $\\pm$ 1.2%</td>\n",
       "      <td>31.8% $\\pm$ 1.3%</td>\n",
       "      <td>25.1% $\\pm$ 0.8%</td>\n",
       "      <td>26.9% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3_8b_c_high</td>\n",
       "      <td>55.0% $\\pm$ 1.6%</td>\n",
       "      <td>29.7% $\\pm$ 2.2%</td>\n",
       "      <td>28.3% $\\pm$ 2.1%</td>\n",
       "      <td>42.5% $\\pm$ 1.1%</td>\n",
       "      <td>48.2% $\\pm$ 1.4%</td>\n",
       "      <td>72.2% $\\pm$ 1.2%</td>\n",
       "      <td>28.3% $\\pm$ 0.8%</td>\n",
       "      <td>29.7% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama3_8b_e_low</td>\n",
       "      <td>59.2% $\\pm$ 1.6%</td>\n",
       "      <td>26.8% $\\pm$ 2.1%</td>\n",
       "      <td>28.1% $\\pm$ 2.1%</td>\n",
       "      <td>40.9% $\\pm$ 1.1%</td>\n",
       "      <td>56.6% $\\pm$ 1.4%</td>\n",
       "      <td>63.0% $\\pm$ 1.3%</td>\n",
       "      <td>27.6% $\\pm$ 0.8%</td>\n",
       "      <td>41.4% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3_8b_e_high</td>\n",
       "      <td>35.0% $\\pm$ 1.6%</td>\n",
       "      <td>27.2% $\\pm$ 2.1%</td>\n",
       "      <td>26.8% $\\pm$ 2.1%</td>\n",
       "      <td>41.8% $\\pm$ 1.1%</td>\n",
       "      <td>29.1% $\\pm$ 1.3%</td>\n",
       "      <td>69.7% $\\pm$ 1.3%</td>\n",
       "      <td>25.8% $\\pm$ 0.8%</td>\n",
       "      <td>24.8% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama3_8b_a_low</td>\n",
       "      <td>45.5% $\\pm$ 1.6%</td>\n",
       "      <td>21.4% $\\pm$ 1.9%</td>\n",
       "      <td>24.6% $\\pm$ 2.0%</td>\n",
       "      <td>38.4% $\\pm$ 1.1%</td>\n",
       "      <td>26.3% $\\pm$ 1.3%</td>\n",
       "      <td>64.8% $\\pm$ 1.3%</td>\n",
       "      <td>27.7% $\\pm$ 0.8%</td>\n",
       "      <td>26.3% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3_8b_a_high</td>\n",
       "      <td>52.8% $\\pm$ 1.6%</td>\n",
       "      <td>28.8% $\\pm$ 2.1%</td>\n",
       "      <td>27.5% $\\pm$ 2.1%</td>\n",
       "      <td>42.8% $\\pm$ 1.1%</td>\n",
       "      <td>28.4% $\\pm$ 1.3%</td>\n",
       "      <td>70.7% $\\pm$ 1.3%</td>\n",
       "      <td>24.9% $\\pm$ 0.8%</td>\n",
       "      <td>30.7% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3_8b_n_low</td>\n",
       "      <td>58.2% $\\pm$ 1.6%</td>\n",
       "      <td>29.5% $\\pm$ 2.2%</td>\n",
       "      <td>28.8% $\\pm$ 2.1%</td>\n",
       "      <td>42.8% $\\pm$ 1.1%</td>\n",
       "      <td>47.7% $\\pm$ 1.4%</td>\n",
       "      <td>71.9% $\\pm$ 1.2%</td>\n",
       "      <td>29.7% $\\pm$ 0.8%</td>\n",
       "      <td>30.8% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama3_8b_n_high</td>\n",
       "      <td>38.8% $\\pm$ 1.6%</td>\n",
       "      <td>25.2% $\\pm$ 2.1%</td>\n",
       "      <td>25.2% $\\pm$ 2.1%</td>\n",
       "      <td>39.0% $\\pm$ 1.1%</td>\n",
       "      <td>23.7% $\\pm$ 1.2%</td>\n",
       "      <td>3.0% $\\pm$ 0.5%</td>\n",
       "      <td>24.9% $\\pm$ 0.8%</td>\n",
       "      <td>23.1% $\\pm$ 0.4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model        truthfulqa gpqa_main_zeroshot  gpqa_main_n_shot  \\\n",
       "0          llama3_8b  53.5% $\\pm$ 1.6%   28.1% $\\pm$ 2.1%  29.9% $\\pm$ 2.2%   \n",
       "1    llama3_8b_o_low  49.1% $\\pm$ 1.6%   25.0% $\\pm$ 2.0%  26.3% $\\pm$ 2.1%   \n",
       "2   llama3_8b_o_high  52.4% $\\pm$ 1.6%   27.9% $\\pm$ 2.1%  27.9% $\\pm$ 2.1%   \n",
       "3    llama3_8b_c_low  39.0% $\\pm$ 1.6%   21.0% $\\pm$ 1.9%  23.0% $\\pm$ 2.0%   \n",
       "4   llama3_8b_c_high  55.0% $\\pm$ 1.6%   29.7% $\\pm$ 2.2%  28.3% $\\pm$ 2.1%   \n",
       "5    llama3_8b_e_low  59.2% $\\pm$ 1.6%   26.8% $\\pm$ 2.1%  28.1% $\\pm$ 2.1%   \n",
       "6   llama3_8b_e_high  35.0% $\\pm$ 1.6%   27.2% $\\pm$ 2.1%  26.8% $\\pm$ 2.1%   \n",
       "7    llama3_8b_a_low  45.5% $\\pm$ 1.6%   21.4% $\\pm$ 1.9%  24.6% $\\pm$ 2.0%   \n",
       "8   llama3_8b_a_high  52.8% $\\pm$ 1.6%   28.8% $\\pm$ 2.1%  27.5% $\\pm$ 2.1%   \n",
       "9    llama3_8b_n_low  58.2% $\\pm$ 1.6%   29.5% $\\pm$ 2.2%  28.8% $\\pm$ 2.1%   \n",
       "10  llama3_8b_n_high  38.8% $\\pm$ 1.6%   25.2% $\\pm$ 2.1%  25.2% $\\pm$ 2.1%   \n",
       "\n",
       "          social_iqa    commonsense_qa             gsm8k            mathqa  \\\n",
       "0   49.7% $\\pm$ 1.1%  51.8% $\\pm$ 1.4%  64.7% $\\pm$ 1.3%  27.9% $\\pm$ 0.8%   \n",
       "1   43.8% $\\pm$ 1.1%  24.8% $\\pm$ 1.2%  31.8% $\\pm$ 1.3%  27.8% $\\pm$ 0.8%   \n",
       "2   42.3% $\\pm$ 1.1%  22.9% $\\pm$ 1.2%  68.4% $\\pm$ 1.3%  26.9% $\\pm$ 0.8%   \n",
       "3   37.8% $\\pm$ 1.1%  21.6% $\\pm$ 1.2%  31.8% $\\pm$ 1.3%  25.1% $\\pm$ 0.8%   \n",
       "4   42.5% $\\pm$ 1.1%  48.2% $\\pm$ 1.4%  72.2% $\\pm$ 1.2%  28.3% $\\pm$ 0.8%   \n",
       "5   40.9% $\\pm$ 1.1%  56.6% $\\pm$ 1.4%  63.0% $\\pm$ 1.3%  27.6% $\\pm$ 0.8%   \n",
       "6   41.8% $\\pm$ 1.1%  29.1% $\\pm$ 1.3%  69.7% $\\pm$ 1.3%  25.8% $\\pm$ 0.8%   \n",
       "7   38.4% $\\pm$ 1.1%  26.3% $\\pm$ 1.3%  64.8% $\\pm$ 1.3%  27.7% $\\pm$ 0.8%   \n",
       "8   42.8% $\\pm$ 1.1%  28.4% $\\pm$ 1.3%  70.7% $\\pm$ 1.3%  24.9% $\\pm$ 0.8%   \n",
       "9   42.8% $\\pm$ 1.1%  47.7% $\\pm$ 1.4%  71.9% $\\pm$ 1.2%  29.7% $\\pm$ 0.8%   \n",
       "10  39.0% $\\pm$ 1.1%  23.7% $\\pm$ 1.2%   3.0% $\\pm$ 0.5%  24.9% $\\pm$ 0.8%   \n",
       "\n",
       "                mmlu  \n",
       "0   51.2% $\\pm$ 0.4%  \n",
       "1   29.8% $\\pm$ 0.4%  \n",
       "2   23.0% $\\pm$ 0.4%  \n",
       "3   26.9% $\\pm$ 0.4%  \n",
       "4   29.7% $\\pm$ 0.4%  \n",
       "5   41.4% $\\pm$ 0.4%  \n",
       "6   24.8% $\\pm$ 0.4%  \n",
       "7   26.3% $\\pm$ 0.4%  \n",
       "8   30.7% $\\pm$ 0.4%  \n",
       "9   30.8% $\\pm$ 0.4%  \n",
       "10  23.1% $\\pm$ 0.4%  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Model Performance Comparison}\n",
      "\\label{tab:model-performance}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{lcccccccc}\n",
      "\\toprule\n",
      "Model & truthfulqa & gpqa\\textbackslash{}_main\\textbackslash{}_zeroshot & gpqa\\textbackslash{}_main\\textbackslash{}_n\\textbackslash{}_shot & social\\textbackslash{}_iqa & commonsense\\textbackslash{}_qa & gsm8k & mathqa & mmlu \\\\\n",
      "\\midrule\n",
      "llama3\\textbackslash{}_8b & 53.5\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 28.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 29.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.2\\textbackslash{}% & 49.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 51.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.4\\textbackslash{}% & 64.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 27.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 51.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_o\\textbackslash{}_low & 49.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 25.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.0\\textbackslash{}% & 26.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 43.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 24.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 31.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 27.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 29.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_o\\textbackslash{}_high & 52.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 27.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 27.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 42.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 22.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 68.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 26.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 23.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_c\\textbackslash{}_low & 39.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 21.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.9\\textbackslash{}% & 23.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.0\\textbackslash{}% & 37.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 21.6\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 31.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 25.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 26.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_c\\textbackslash{}_high & 55.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 29.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.2\\textbackslash{}% & 28.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 42.5\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 48.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.4\\textbackslash{}% & 72.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 28.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 29.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_e\\textbackslash{}_low & 59.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 26.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 28.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 40.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 56.6\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.4\\textbackslash{}% & 63.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 27.6\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 41.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_e\\textbackslash{}_high & 35.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 27.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 26.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 41.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 29.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 69.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 25.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 24.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_a\\textbackslash{}_low & 45.5\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 21.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.9\\textbackslash{}% & 24.6\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.0\\textbackslash{}% & 38.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 26.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 64.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 27.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 26.3\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_a\\textbackslash{}_high & 52.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 28.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 27.5\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 42.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 28.4\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 70.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.3\\textbackslash{}% & 24.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 30.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_n\\textbackslash{}_low & 58.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 29.5\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.2\\textbackslash{}% & 28.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 42.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 47.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.4\\textbackslash{}% & 71.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 29.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 30.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "llama3\\textbackslash{}_8b\\textbackslash{}_n\\textbackslash{}_high & 38.8\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.6\\textbackslash{}% & 25.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 25.2\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 2.1\\textbackslash{}% & 39.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.1\\textbackslash{}% & 23.7\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 1.2\\textbackslash{}% & 3.0\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.5\\textbackslash{}% & 24.9\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.8\\textbackslash{}% & 23.1\\textbackslash{}% \\textbackslash{}$\\textbackslash{}pm\\textbackslash{}$ 0.4\\textbackslash{}% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def escape_latex(s):\n",
    "    \"\"\"Escape special characters in LaTeX.\"\"\"\n",
    "    return (s.replace('&', r'\\&')\n",
    "             .replace('%', r'\\%')\n",
    "             .replace('$', r'\\$')\n",
    "             .replace('#', r'\\#')\n",
    "             .replace('_', r'\\_')\n",
    "             .replace('{', r'\\{')\n",
    "             .replace('}', r'\\}')\n",
    "             .replace('~', r'\\textasciitilde{}')\n",
    "             .replace('^', r'\\textasciicircum{}')\n",
    "             .replace('\\\\', r'\\textbackslash{}'))\n",
    "\n",
    "def generate_latex_table(df):\n",
    "    # 定义指标的顺序\n",
    "    metric_order = ['truthfulqa', 'gpqa_main_zeroshot', 'gpqa_main_n_shot', 'social_iqa', 'commonsense_qa', 'gsm8k', 'mathqa', 'mmlu']\n",
    "\n",
    "    # 确保所有需要的列都在数据框中\n",
    "    available_metrics = [col for col in metric_order if col in df.columns]\n",
    "\n",
    "    # 开始生成LaTeX代码\n",
    "    latex_code = []\n",
    "    latex_code.append(r'\\begin{table}[htbp]')\n",
    "    latex_code.append(r'\\centering')\n",
    "    latex_code.append(r'\\caption{Model Performance Comparison}')\n",
    "    latex_code.append(r'\\label{tab:model-performance}')\n",
    "    latex_code.append(r'\\resizebox{\\textwidth}{!}{')\n",
    "    latex_code.append(r'\\begin{tabular}{l' + 'c' * len(available_metrics) + '}')\n",
    "    latex_code.append(r'\\toprule')\n",
    "\n",
    "    # 添加表头\n",
    "    header = ['Model'] + [escape_latex(metric) for metric in available_metrics]\n",
    "    latex_code.append(' & '.join(header) + r' \\\\')\n",
    "    latex_code.append(r'\\midrule')\n",
    "\n",
    "    # 添加数据行\n",
    "    for _, row in df.iterrows():\n",
    "        model = escape_latex(row['model'])\n",
    "        data_row = [model] + [escape_latex(str(row[metric])) for metric in available_metrics]\n",
    "        latex_code.append(' & '.join(data_row) + r' \\\\')\n",
    "\n",
    "    # 结束表格\n",
    "    latex_code.append(r'\\bottomrule')\n",
    "    latex_code.append(r'\\end{tabular}')\n",
    "    latex_code.append(r'}')\n",
    "    latex_code.append(r'\\end{table}')\n",
    "\n",
    "    return '\\n'.join(latex_code)\n",
    "\n",
    "# 生成LaTeX代码\n",
    "latex_table = generate_latex_table(df)\n",
    "\n",
    "# 打印LaTeX代码\n",
    "print(latex_table)\n",
    "\n",
    "# 可选：将LaTeX代码保存到文件\n",
    "with open('model_performance_table.tex', 'w') as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=11, step=1)\n",
      "Index(['model', 'truthfulqa', 'gpqa_main_zeroshot', 'gpqa_main_n_shot',\n",
      "       'social_iqa', 'commonsense_qa', 'gsm8k', 'mathqa', 'mmlu'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.index)\n",
    "print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
