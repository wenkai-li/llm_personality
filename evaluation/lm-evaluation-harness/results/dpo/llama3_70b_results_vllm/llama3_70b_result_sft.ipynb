{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_file(root_dir):\n",
    "    try:\n",
    "        root, dirs, files = next(os.walk(root_dir))\n",
    "        sub_root, sub_dir, sub_files = next(os.walk(os.path.join(root, dirs[0])))\n",
    "        return dirs, sub_dir\n",
    "    except StopIteration:\n",
    "        return []\n",
    "\n",
    "rd = '/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm'\n",
    "# result_file = \n",
    "dirs, sub_dir = find_model_file(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3_70b_a_low', 'llama3_70b_o_low', 'llama3_70b_n_high', 'llama3_70b_o_high', 'llama3_70b_c_high', 'llama3_70b_n_low', 'llama3_70b_c_low', 'llama3_70b_a_high', 'llama3_70b_e_high', 'llama3_70b_e_low']\n",
      "['llama3_70b_mmlu', 'llama3_70b_social_iqa', 'llama3_70b_gpqa_main_zeroshot', 'llama3_70b_truthfulqa', 'llama3_70b_mathqa', 'llama3_70b_piqa', 'llama3_70b_gpqa_main_n_shot', 'llama3_70b_commonsense_qa', 'llama3_70b_gsm8k_5_shots_without_cot']\n"
     ]
    }
   ],
   "source": [
    "print(dirs)\n",
    "print(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3_70b_a_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_low/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_o_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_low/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_n_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_high/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_o_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_o_high/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_c_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_high/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_n_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_n_low/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_c_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_c_low/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_a_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_a_high/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_e_high\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_high/llama3_70b_gsm8k_5_shots_without_cot\n",
      "llama3_70b_e_low\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_mmlu\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_social_iqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_gpqa_main_zeroshot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_truthfulqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_mathqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_piqa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_gpqa_main_n_shot\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_commonsense_qa\n",
      "/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_e_low/llama3_70b_gsm8k_5_shots_without_cot\n"
     ]
    }
   ],
   "source": [
    "for d in dirs:\n",
    "    print(d)\n",
    "    for sub_d in sub_dir:\n",
    "        s = os.path.join(rd, d)\n",
    "        print(os.path.join(s, sub_d))\n",
    "    # print(os.listdir(os.path.join(rd, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llama3_70b_a_low',\n",
       " 'llama3_70b_o_low',\n",
       " 'llama3_70b_n_high',\n",
       " 'llama3_70b_o_high',\n",
       " 'llama3_70b_c_high',\n",
       " 'llama3_70b_n_low',\n",
       " 'llama3_70b_c_low',\n",
       " 'llama3_70b_a_high',\n",
       " 'llama3_70b_e_high',\n",
       " 'llama3_70b_e_low']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "for d in dirs:\n",
    "    model_dict[d] = {}\n",
    "    for sub_d in sub_dir:\n",
    "        s = os.path.join(rd, d)\n",
    "        f = os.path.join(s, sub_d)\n",
    "        ff = os.path.join(f,os.listdir(f)[0])\n",
    "        fff = os.path.join(ff,os.listdir(ff)[0])\n",
    "        with open(fff, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        model_dict[d][sub_d] = data['results']\n",
    "\n",
    "# with open(fff, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "#     print(data['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3_70b_a_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6251958410482837,\n",
       "    'acc_stderr,none': 0.00392094851749934,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5742826780021254,\n",
       "    'acc_stderr,none': 0.007036181304622663,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.4365079365079365,\n",
       "    'acc_stderr,none': 0.04435932892851466},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7515151515151515,\n",
       "    'acc_stderr,none': 0.033744026441394036},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.7892156862745098,\n",
       "    'acc_stderr,none': 0.02862654791243739},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.7763713080168776,\n",
       "    'acc_stderr,none': 0.027123298205229966},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.7603305785123967,\n",
       "    'acc_stderr,none': 0.038968789850704164},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6759259259259259,\n",
       "    'acc_stderr,none': 0.04524596007030049},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.6625766871165644,\n",
       "    'acc_stderr,none': 0.03714908409935575},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.5809248554913294,\n",
       "    'acc_stderr,none': 0.026564178111422615},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.4670391061452514,\n",
       "    'acc_stderr,none': 0.016686126653013934},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6302250803858521,\n",
       "    'acc_stderr,none': 0.027417996705631},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.6790123456790124,\n",
       "    'acc_stderr,none': 0.025976566010862727},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.4980443285528031,\n",
       "    'acc_stderr,none': 0.012770138422208635},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.6198830409356725,\n",
       "    'acc_stderr,none': 0.037229657413855394},\n",
       "   'mmlu_other': {'acc,none': 0.694560669456067,\n",
       "    'acc_stderr,none': 0.007950118238955634,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.6792452830188679,\n",
       "    'acc_stderr,none': 0.028727502957880267},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6069364161849711,\n",
       "    'acc_stderr,none': 0.03724249595817731},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.049236596391733084},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5560538116591929,\n",
       "    'acc_stderr,none': 0.03334625674242728},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8446601941747572,\n",
       "    'acc_stderr,none': 0.03586594738573975},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8076923076923077,\n",
       "    'acc_stderr,none': 0.02581923325648373},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816503},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8033205619412516,\n",
       "    'acc_stderr,none': 0.014214138556913912},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.7320261437908496,\n",
       "    'acc_stderr,none': 0.025360603796242557},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.029827499313594685},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.7977941176470589,\n",
       "    'acc_stderr,none': 0.024398192986654924},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4759036144578313,\n",
       "    'acc_stderr,none': 0.03887971849597264},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.7065323366915827,\n",
       "    'acc_stderr,none': 0.008020371863840536,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.047036043419179864},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.7676767676767676,\n",
       "    'acc_stderr,none': 0.030088629490217487},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8860103626943006,\n",
       "    'acc_stderr,none': 0.022935144053919422},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.6846153846153846,\n",
       "    'acc_stderr,none': 0.02355964698318995},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.7478991596638656,\n",
       "    'acc_stderr,none': 0.028205545033277723},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.818348623853211,\n",
       "    'acc_stderr,none': 0.016530617409266857},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.7022900763358778,\n",
       "    'acc_stderr,none': 0.04010358942462203},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6029411764705882,\n",
       "    'acc_stderr,none': 0.01979448890002411},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5545454545454546,\n",
       "    'acc_stderr,none': 0.047605488214603246},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.6163265306122448,\n",
       "    'acc_stderr,none': 0.031130880396235922},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7611940298507462,\n",
       "    'acc_stderr,none': 0.030147775935409224},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816505},\n",
       "   'mmlu_stem': {'acc,none': 0.5534411671424041,\n",
       "    'acc_stderr,none': 0.008487831107704711,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.33,\n",
       "    'acc_stderr,none': 0.047258156262526045},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6814814814814815,\n",
       "    'acc_stderr,none': 0.04024778401977109},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7236842105263158,\n",
       "    'acc_stderr,none': 0.03639057569952929},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.7986111111111112,\n",
       "    'acc_stderr,none': 0.033536474697138406},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.46,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.52,\n",
       "    'acc_stderr,none': 0.050211673156867795},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.4803921568627451,\n",
       "    'acc_stderr,none': 0.04971358884367406},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.65,\n",
       "    'acc_stderr,none': 0.047937248544110196},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.4978723404255319,\n",
       "    'acc_stderr,none': 0.03268572658667492},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5655172413793104,\n",
       "    'acc_stderr,none': 0.04130740879555497},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.4708994708994709,\n",
       "    'acc_stderr,none': 0.025707658614154954},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.7741935483870968,\n",
       "    'acc_stderr,none': 0.02378557788418101},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.034819048444388045},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2814814814814815,\n",
       "    'acc_stderr,none': 0.027420019350945266},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.47019867549668876,\n",
       "    'acc_stderr,none': 0.040752249922169775},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.625,\n",
       "    'acc_stderr,none': 0.033016908987210894},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.48214285714285715,\n",
       "    'acc_stderr,none': 0.047427623612430116}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.39048106448311154,\n",
       "    'acc_stderr,none': 0.01103932371486307}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.602474965910092,\n",
       "    'bleu_max_stderr,none': 0.20099493218185174,\n",
       "    'bleu_acc,none': 0.4565483476132191,\n",
       "    'bleu_acc_stderr,none': 0.01743728095318368,\n",
       "    'bleu_diff,none': -0.08684781789820634,\n",
       "    'bleu_diff_stderr,none': 0.14633296646745628,\n",
       "    'rouge1_max,none': 21.26109589339087,\n",
       "    'rouge1_max_stderr,none': 0.39900378176132173,\n",
       "    'rouge1_acc,none': 0.45777233782129745,\n",
       "    'rouge1_acc_stderr,none': 0.01744096571248212,\n",
       "    'rouge1_diff,none': -0.05170815955765682,\n",
       "    'rouge1_diff_stderr,none': 0.31378678011623096,\n",
       "    'rouge2_max,none': 11.3095489582619,\n",
       "    'rouge2_max_stderr,none': 0.4008404209798527,\n",
       "    'rouge2_acc,none': 0.32802937576499386,\n",
       "    'rouge2_acc_stderr,none': 0.016435632932815004,\n",
       "    'rouge2_diff,none': -0.6478231217086778,\n",
       "    'rouge2_diff_stderr,none': 0.29714919448741545,\n",
       "    'rougeL_max,none': 19.13616485017056,\n",
       "    'rougeL_max_stderr,none': 0.3927296747535081,\n",
       "    'rougeL_acc,none': 0.4589963280293758,\n",
       "    'rougeL_acc_stderr,none': 0.017444544447661206,\n",
       "    'rougeL_diff,none': -0.1641009464888551,\n",
       "    'rougeL_diff_stderr,none': 0.30493446467722446},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3243574051407589,\n",
       "    'acc_stderr,none': 0.016387976779647942},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5058314086539912,\n",
       "    'acc_stderr,none': 0.016427216245233724}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.32763819095477387,\n",
       "    'acc_stderr,none': 0.008592100906266604,\n",
       "    'acc_norm,none': 0.3333333333333333,\n",
       "    'acc_norm_stderr,none': 0.008629672884641516}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7404787812840044,\n",
       "    'acc_stderr,none': 0.010227939888173925,\n",
       "    'acc_norm,none': 0.7295973884657236,\n",
       "    'acc_norm_stderr,none': 0.010363167031620794}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.32142857142857145,\n",
       "    'acc_stderr,none': 0.022089519157170157,\n",
       "    'acc_norm,none': 0.32142857142857145,\n",
       "    'acc_norm_stderr,none': 0.022089519157170157}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.3923013923013923,\n",
       "    'acc_stderr,none': 0.01397893643494679}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.4177407126611069,\n",
       "    'exact_match_stderr,strict-match': 0.013584820638504832,\n",
       "    'exact_match,flexible-extract': 0.8999241849886277,\n",
       "    'exact_match_stderr,flexible-extract': 0.008266274528685637}}},\n",
       " 'llama3_70b_o_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6437117219769264,\n",
       "    'acc_stderr,none': 0.0038392275190125944,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5751328374070138,\n",
       "    'acc_stderr,none': 0.0069207640537893335,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5952380952380952,\n",
       "    'acc_stderr,none': 0.04390259265377562},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7393939393939394,\n",
       "    'acc_stderr,none': 0.034277431758165236},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8382352941176471,\n",
       "    'acc_stderr,none': 0.025845017986926924},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8270042194092827,\n",
       "    'acc_stderr,none': 0.024621562866768434},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.7603305785123967,\n",
       "    'acc_stderr,none': 0.03896878985070417},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6481481481481481,\n",
       "    'acc_stderr,none': 0.04616631111801714},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7484662576687117,\n",
       "    'acc_stderr,none': 0.034089978868575295},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.615606936416185,\n",
       "    'acc_stderr,none': 0.026189666966272035},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.48379888268156424,\n",
       "    'acc_stderr,none': 0.01671372072950102},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6141479099678456,\n",
       "    'acc_stderr,none': 0.027648149599751464},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7253086419753086,\n",
       "    'acc_stderr,none': 0.024836057868294684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.43415906127770537,\n",
       "    'acc_stderr,none': 0.01265903323706725},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.7017543859649122,\n",
       "    'acc_stderr,none': 0.035087719298245654},\n",
       "   'mmlu_other': {'acc,none': 0.7241712262632765,\n",
       "    'acc_stderr,none': 0.007730161480411441,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.68,\n",
       "    'acc_stderr,none': 0.04688261722621505},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.7584905660377359,\n",
       "    'acc_stderr,none': 0.026341480371118366},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6589595375722543,\n",
       "    'acc_stderr,none': 0.036146654241808254},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.39,\n",
       "    'acc_stderr,none': 0.04902071300001974},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.6143497757847534,\n",
       "    'acc_stderr,none': 0.03266842214289202},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.7281553398058253,\n",
       "    'acc_stderr,none': 0.044052680241409216},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8034188034188035,\n",
       "    'acc_stderr,none': 0.02603538609895129},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.78,\n",
       "    'acc_stderr,none': 0.04163331998932263},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8314176245210728,\n",
       "    'acc_stderr,none': 0.013387895731543602},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.7320261437908496,\n",
       "    'acc_stderr,none': 0.025360603796242557},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5780141843971631,\n",
       "    'acc_stderr,none': 0.029462189233370593},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8529411764705882,\n",
       "    'acc_stderr,none': 0.021513964052859644},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4819277108433735,\n",
       "    'acc_stderr,none': 0.03889951252827216},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.726356841078973,\n",
       "    'acc_stderr,none': 0.007811719553360617,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5877192982456141,\n",
       "    'acc_stderr,none': 0.04630653203366596},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.7929292929292929,\n",
       "    'acc_stderr,none': 0.028869778460267063},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8963730569948186,\n",
       "    'acc_stderr,none': 0.02199531196364424},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.7435897435897436,\n",
       "    'acc_stderr,none': 0.02213908110397154},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.7773109243697479,\n",
       "    'acc_stderr,none': 0.027025433498882385},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.8422018348623853,\n",
       "    'acc_stderr,none': 0.015630022970092444},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.6793893129770993,\n",
       "    'acc_stderr,none': 0.04093329229834278},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.019431775677037313},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5636363636363636,\n",
       "    'acc_stderr,none': 0.04750185058907297},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.5183673469387755,\n",
       "    'acc_stderr,none': 0.03198761546763126},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7860696517412935,\n",
       "    'acc_stderr,none': 0.028996909693328916},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_stem': {'acc,none': 0.5861084681255947,\n",
       "    'acc_stderr,none': 0.008339493700301721,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6888888888888889,\n",
       "    'acc_stderr,none': 0.03999262876617721},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7368421052631579,\n",
       "    'acc_stderr,none': 0.03583496176361073},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.8888888888888888,\n",
       "    'acc_stderr,none': 0.026280550932848062},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.54,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.04960449637488583},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.41,\n",
       "    'acc_stderr,none': 0.04943110704237102},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.46078431372549017,\n",
       "    'acc_stderr,none': 0.04959859966384181},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.6510638297872341,\n",
       "    'acc_stderr,none': 0.031158522131357783},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5448275862068965,\n",
       "    'acc_stderr,none': 0.04149886942192117},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.5052910052910053,\n",
       "    'acc_stderr,none': 0.02574986828855657},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.8258064516129032,\n",
       "    'acc_stderr,none': 0.021576248184514583},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5320197044334976,\n",
       "    'acc_stderr,none': 0.035107665979592154},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.3148148148148148,\n",
       "    'acc_stderr,none': 0.028317533496066465},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.47019867549668876,\n",
       "    'acc_stderr,none': 0.040752249922169775},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6574074074074074,\n",
       "    'acc_stderr,none': 0.03236585252602156},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5625,\n",
       "    'acc_stderr,none': 0.04708567521880525}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.44472876151484136,\n",
       "    'acc_stderr,none': 0.011244731148193177}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.31919642857142855,\n",
       "    'acc_stderr,none': 0.022048861164576057,\n",
       "    'acc_norm,none': 0.31919642857142855,\n",
       "    'acc_norm_stderr,none': 0.022048861164576057}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 10.74307656454996,\n",
       "    'bleu_max_stderr,none': 0.5921652561837797,\n",
       "    'bleu_acc,none': 0.5862913096695227,\n",
       "    'bleu_acc_stderr,none': 0.0172408618120998,\n",
       "    'bleu_diff,none': 0.24252000819959577,\n",
       "    'bleu_diff_stderr,none': 0.48401667701650236,\n",
       "    'rouge1_max,none': 27.8427589541939,\n",
       "    'rouge1_max_stderr,none': 0.7480600911604414,\n",
       "    'rouge1_acc,none': 0.6878824969400245,\n",
       "    'rouge1_acc_stderr,none': 0.016220756769520943,\n",
       "    'rouge1_diff,none': 5.15742376541629,\n",
       "    'rouge1_diff_stderr,none': 0.63022201241475,\n",
       "    'rouge2_max,none': 11.309317352878294,\n",
       "    'rouge2_max_stderr,none': 0.7623763839260962,\n",
       "    'rouge2_acc,none': 0.189718482252142,\n",
       "    'rouge2_acc_stderr,none': 0.013725485265185093,\n",
       "    'rouge2_diff,none': -0.7548052080430827,\n",
       "    'rouge2_diff_stderr,none': 0.6675079776626445,\n",
       "    'rougeL_max,none': 26.644596383247826,\n",
       "    'rougeL_max_stderr,none': 0.7354914136099051,\n",
       "    'rougeL_acc,none': 0.6829865361077111,\n",
       "    'rougeL_acc_stderr,none': 0.016289203374403365,\n",
       "    'rougeL_diff,none': 5.0320054717198355,\n",
       "    'rougeL_diff_stderr,none': 0.6320796497248932},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.37576499388004897,\n",
       "    'acc_stderr,none': 0.016954584060214294},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5421157034230984,\n",
       "    'acc_stderr,none': 0.016755339076097026}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.34706867671691793,\n",
       "    'acc_stderr,none': 0.00871449153541417,\n",
       "    'acc_norm,none': 0.35510887772194305,\n",
       "    'acc_norm_stderr,none': 0.00876041246957384}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7676822633297062,\n",
       "    'acc_stderr,none': 0.009853201384168241,\n",
       "    'acc_norm,none': 0.7682263329706203,\n",
       "    'acc_norm_stderr,none': 0.009845143772794024}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.021923384489444957,\n",
       "    'acc_norm,none': 0.3125,\n",
       "    'acc_norm_stderr,none': 0.021923384489444957}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.6592956592956593,\n",
       "    'acc_stderr,none': 0.013569036984855006}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8832448824867324,\n",
       "    'exact_match_stderr,strict-match': 0.008845468136919103,\n",
       "    'exact_match,flexible-extract': 0.8847611827141774,\n",
       "    'exact_match_stderr,flexible-extract': 0.00879538230154542}}},\n",
       " 'llama3_70b_n_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.33186155818259505,\n",
       "    'acc_stderr,none': 0.003918672275666112,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3640807651434644,\n",
       "    'acc_stderr,none': 0.006974009204927656,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.30158730158730157,\n",
       "    'acc_stderr,none': 0.04104947269903394},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3090909090909091,\n",
       "    'acc_stderr,none': 0.036085410115739666},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.3480392156862745,\n",
       "    'acc_stderr,none': 0.03343311240488419},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.33755274261603374,\n",
       "    'acc_stderr,none': 0.03078154910202621},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5619834710743802,\n",
       "    'acc_stderr,none': 0.04529146804435792},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.3425925925925926,\n",
       "    'acc_stderr,none': 0.04587904741301809},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.4171779141104294,\n",
       "    'acc_stderr,none': 0.03874102859818082},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.36127167630057805,\n",
       "    'acc_stderr,none': 0.025862201852277895},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.40670391061452515,\n",
       "    'acc_stderr,none': 0.016428811915898858},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2347266881028939,\n",
       "    'acc_stderr,none': 0.024071805887677048},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.28703703703703703,\n",
       "    'acc_stderr,none': 0.025171041915309684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.37614080834419816,\n",
       "    'acc_stderr,none': 0.012372214430599816},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.39766081871345027,\n",
       "    'acc_stderr,none': 0.03753638955761691},\n",
       "   'mmlu_other': {'acc,none': 0.36401673640167365,\n",
       "    'acc_stderr,none': 0.008541257637852656,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.45,\n",
       "    'acc_stderr,none': 0.05},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.3132075471698113,\n",
       "    'acc_stderr,none': 0.028544793319055333},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.26011560693641617,\n",
       "    'acc_stderr,none': 0.03345036916788991},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.4349775784753363,\n",
       "    'acc_stderr,none': 0.033272833702713445},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.46601941747572817,\n",
       "    'acc_stderr,none': 0.04939291447273481},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.3974358974358974,\n",
       "    'acc_stderr,none': 0.03205953453789293},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.38058748403575987,\n",
       "    'acc_stderr,none': 0.01736256412607542},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.27124183006535946,\n",
       "    'acc_stderr,none': 0.02545775669666788},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.31560283687943264,\n",
       "    'acc_stderr,none': 0.027724989449509314},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.5073529411764706,\n",
       "    'acc_stderr,none': 0.030369552523902173},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3373493975903614,\n",
       "    'acc_stderr,none': 0.03680783690727581},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.34644133896652585,\n",
       "    'acc_stderr,none': 0.008458706447299158,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2719298245614035,\n",
       "    'acc_stderr,none': 0.04185774424022056},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.2474747474747475,\n",
       "    'acc_stderr,none': 0.03074630074212449},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.3005181347150259,\n",
       "    'acc_stderr,none': 0.03308818594415751},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2358974358974359,\n",
       "    'acc_stderr,none': 0.02152596540740872},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.25630252100840334,\n",
       "    'acc_stderr,none': 0.028359620870533946},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3431192660550459,\n",
       "    'acc_stderr,none': 0.02035477773608604},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.366412213740458,\n",
       "    'acc_stderr,none': 0.04225875451969639},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.46078431372549017,\n",
       "    'acc_stderr,none': 0.020165523313907908},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.33636363636363636,\n",
       "    'acc_stderr,none': 0.04525393596302507},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.3346938775510204,\n",
       "    'acc_stderr,none': 0.030209235226242307},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.44776119402985076,\n",
       "    'acc_stderr,none': 0.03516184772952167},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.49,\n",
       "    'acc_stderr,none': 0.05024183937956911},\n",
       "   'mmlu_stem': {'acc,none': 0.23786869647954328,\n",
       "    'acc_stderr,none': 0.007566815679698311,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2518518518518518,\n",
       "    'acc_stderr,none': 0.037498507091740206},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.21710526315789475,\n",
       "    'acc_stderr,none': 0.033550453048829226},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.3263888888888889,\n",
       "    'acc_stderr,none': 0.03921067198982266},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.0440844002276808},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.041633319989322695},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.22549019607843138,\n",
       "    'acc_stderr,none': 0.04158307533083286},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.29,\n",
       "    'acc_stderr,none': 0.045604802157206845},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.2978723404255319,\n",
       "    'acc_stderr,none': 0.02989614568209546},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.21164021164021163,\n",
       "    'acc_stderr,none': 0.021037331505262886},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.22258064516129034,\n",
       "    'acc_stderr,none': 0.02366421667164251},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.1625615763546798,\n",
       "    'acc_stderr,none': 0.02596030006460558},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.21481481481481482,\n",
       "    'acc_stderr,none': 0.025040443877000683},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2052980132450331,\n",
       "    'acc_stderr,none': 0.032979866484738336},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.22685185185185186,\n",
       "    'acc_stderr,none': 0.028561650102422273},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.32142857142857145,\n",
       "    'acc_stderr,none': 0.04432804055291519}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.39969293756397134,\n",
       "    'acc_stderr,none': 0.011084059334882369}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.32589285714285715,\n",
       "    'acc_stderr,none': 0.022169103134643414,\n",
       "    'acc_norm,none': 0.32589285714285715,\n",
       "    'acc_norm_stderr,none': 0.022169103134643414}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.2298937294465833,\n",
       "    'bleu_max_stderr,none': 0.09354138993283234,\n",
       "    'bleu_acc,none': 0.5055079559363526,\n",
       "    'bleu_acc_stderr,none': 0.01750243899045107,\n",
       "    'bleu_diff,none': 0.05336486584963402,\n",
       "    'bleu_diff_stderr,none': 0.0653605864556049,\n",
       "    'rouge1_max,none': 14.747897170248635,\n",
       "    'rouge1_max_stderr,none': 0.28198385466588327,\n",
       "    'rouge1_acc,none': 0.6132190942472461,\n",
       "    'rouge1_acc_stderr,none': 0.017048857010515103,\n",
       "    'rouge1_diff,none': 1.46673252321499,\n",
       "    'rouge1_diff_stderr,none': 0.2309600728838094,\n",
       "    'rouge2_max,none': 5.438542820105168,\n",
       "    'rouge2_max_stderr,none': 0.2732934519168711,\n",
       "    'rouge2_acc,none': 0.2582619339045288,\n",
       "    'rouge2_acc_stderr,none': 0.015321821688476185,\n",
       "    'rouge2_diff,none': -0.2945946516402812,\n",
       "    'rouge2_diff_stderr,none': 0.2012302851801536,\n",
       "    'rougeL_max,none': 12.678891120639284,\n",
       "    'rougeL_max_stderr,none': 0.27607582716498286,\n",
       "    'rougeL_acc,none': 0.5789473684210527,\n",
       "    'rougeL_acc_stderr,none': 0.017283936248136473,\n",
       "    'rougeL_diff,none': 0.6985162105465053,\n",
       "    'rougeL_diff_stderr,none': 0.21694684578869217},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.2582619339045288,\n",
       "    'acc_stderr,none': 0.015321821688476187},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.4304264387243563,\n",
       "    'acc_stderr,none': 0.016580910277876004}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2887772194304858,\n",
       "    'acc_stderr,none': 0.008296308349383155,\n",
       "    'acc_norm,none': 0.30083752093802346,\n",
       "    'acc_norm_stderr,none': 0.00839567556992465}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7285092491838956,\n",
       "    'acc_stderr,none': 0.010376251176596135,\n",
       "    'acc_norm,none': 0.7225244831338411,\n",
       "    'acc_norm_stderr,none': 0.010446818281039955}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.36607142857142855,\n",
       "    'acc_stderr,none': 0.02278501498199051,\n",
       "    'acc_norm,none': 0.36607142857142855,\n",
       "    'acc_norm_stderr,none': 0.02278501498199051}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.20065520065520065,\n",
       "    'acc_stderr,none': 0.011466011466011533}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.133434420015163,\n",
       "    'exact_match_stderr,strict-match': 0.009366491609784465,\n",
       "    'exact_match,flexible-extract': 0.1516300227445034,\n",
       "    'exact_match_stderr,flexible-extract': 0.009879331091354297}}},\n",
       " 'llama3_70b_o_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.5789061387266771,\n",
       "    'acc_stderr,none': 0.004013950469431933,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5846971307120085,\n",
       "    'acc_stderr,none': 0.006921062283228587,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.30158730158730157,\n",
       "    'acc_stderr,none': 0.04104947269903394},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7393939393939394,\n",
       "    'acc_stderr,none': 0.034277431758165236},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8186274509803921,\n",
       "    'acc_stderr,none': 0.02704462171947407},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8523206751054853,\n",
       "    'acc_stderr,none': 0.023094329582595694},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.71900826446281,\n",
       "    'acc_stderr,none': 0.04103203830514512},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6481481481481481,\n",
       "    'acc_stderr,none': 0.046166311118017125},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.5950920245398773,\n",
       "    'acc_stderr,none': 0.038566721635489125},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.6098265895953757,\n",
       "    'acc_stderr,none': 0.026261677607806642},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.40782122905027934,\n",
       "    'acc_stderr,none': 0.016435865260914742},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.5980707395498392,\n",
       "    'acc_stderr,none': 0.02784647600593048},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7129629629629629,\n",
       "    'acc_stderr,none': 0.025171041915309684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5514993481095176,\n",
       "    'acc_stderr,none': 0.012702317490559813},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.7543859649122807,\n",
       "    'acc_stderr,none': 0.03301405946987249},\n",
       "   'mmlu_other': {'acc,none': 0.6099130994528484,\n",
       "    'acc_stderr,none': 0.008597656816445472,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.64,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.5849056603773585,\n",
       "    'acc_stderr,none': 0.030325945789286105},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.5606936416184971,\n",
       "    'acc_stderr,none': 0.03784271932887467},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.6053811659192825,\n",
       "    'acc_stderr,none': 0.03280400504755291},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.7184466019417476,\n",
       "    'acc_stderr,none': 0.04453254836326466},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.717948717948718,\n",
       "    'acc_stderr,none': 0.02948036054954119},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.6283524904214559,\n",
       "    'acc_stderr,none': 0.01728080252213318},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.5849673202614379,\n",
       "    'acc_stderr,none': 0.028213504177824096},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.4858156028368794,\n",
       "    'acc_stderr,none': 0.02981549448368206},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.7830882352941176,\n",
       "    'acc_stderr,none': 0.025035845227711264},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4457831325301205,\n",
       "    'acc_stderr,none': 0.03869543323472101},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.6509587260318492,\n",
       "    'acc_stderr,none': 0.008417402046232824,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.4649122807017544,\n",
       "    'acc_stderr,none': 0.04692008381368909},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.5656565656565656,\n",
       "    'acc_stderr,none': 0.035315058793591834},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8031088082901554,\n",
       "    'acc_stderr,none': 0.02869787397186069},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.02535100632816969},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.5588235294117647,\n",
       "    'acc_stderr,none': 0.032252942323996406},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.7467889908256881,\n",
       "    'acc_stderr,none': 0.018644073041375046},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.5648854961832062,\n",
       "    'acc_stderr,none': 0.043482080516448585},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6666666666666666,\n",
       "    'acc_stderr,none': 0.0190709855896875},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.6090909090909091,\n",
       "    'acc_stderr,none': 0.04673752333670237},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.7510204081632653,\n",
       "    'acc_stderr,none': 0.027682979522960234},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7313432835820896,\n",
       "    'acc_stderr,none': 0.03134328358208954},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.68,\n",
       "    'acc_stderr,none': 0.04688261722621504},\n",
       "   'mmlu_stem': {'acc,none': 0.4693942277196321,\n",
       "    'acc_stderr,none': 0.008581697015889985,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.5777777777777777,\n",
       "    'acc_stderr,none': 0.04266763404099582},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.6578947368421053,\n",
       "    'acc_stderr,none': 0.038607315993160904},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.04016660030451233},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.52,\n",
       "    'acc_stderr,none': 0.050211673156867795},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.35294117647058826,\n",
       "    'acc_stderr,none': 0.04755129616062946},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.61,\n",
       "    'acc_stderr,none': 0.04902071300001975},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.42127659574468085,\n",
       "    'acc_stderr,none': 0.03227834510146268},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5103448275862069,\n",
       "    'acc_stderr,none': 0.04165774775728763},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.3148148148148148,\n",
       "    'acc_stderr,none': 0.023919984164047732},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.6387096774193548,\n",
       "    'acc_stderr,none': 0.027327548447957536},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.4827586206896552,\n",
       "    'acc_stderr,none': 0.035158955511657},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.27037037037037037,\n",
       "    'acc_stderr,none': 0.027080372815145668},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.4105960264900662,\n",
       "    'acc_stderr,none': 0.04016689594849928},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.5555555555555556,\n",
       "    'acc_stderr,none': 0.03388857118502326},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.04697113923010212}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.41453428863868985,\n",
       "    'acc_stderr,none': 0.01114756056703673}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.36830357142857145,\n",
       "    'acc_stderr,none': 0.02281410385929623,\n",
       "    'acc_norm,none': 0.36830357142857145,\n",
       "    'acc_norm_stderr,none': 0.02281410385929623}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 4.67000729002748,\n",
       "    'bleu_max_stderr,none': 0.1962790613418521,\n",
       "    'bleu_acc,none': 0.4638922888616891,\n",
       "    'bleu_acc_stderr,none': 0.017457800422268622,\n",
       "    'bleu_diff,none': -0.1407108676112176,\n",
       "    'bleu_diff_stderr,none': 0.12336817419524353,\n",
       "    'rouge1_max,none': 19.70330874159952,\n",
       "    'rouge1_max_stderr,none': 0.37179639804216896,\n",
       "    'rouge1_acc,none': 0.4834761321909425,\n",
       "    'rouge1_acc_stderr,none': 0.01749394019005773,\n",
       "    'rouge1_diff,none': -0.05319481147972795,\n",
       "    'rouge1_diff_stderr,none': 0.26610073609819535,\n",
       "    'rouge2_max,none': 9.966995731885408,\n",
       "    'rouge2_max_stderr,none': 0.35204272293306876,\n",
       "    'rouge2_acc,none': 0.37576499388004897,\n",
       "    'rouge2_acc_stderr,none': 0.016954584060214287,\n",
       "    'rouge2_diff,none': -0.558407042794807,\n",
       "    'rouge2_diff_stderr,none': 0.24537958685086045,\n",
       "    'rougeL_max,none': 16.898524866575997,\n",
       "    'rougeL_max_stderr,none': 0.36011815342509795,\n",
       "    'rougeL_acc,none': 0.4663402692778458,\n",
       "    'rougeL_acc_stderr,none': 0.017463793867168096,\n",
       "    'rougeL_diff,none': -0.4041928389312531,\n",
       "    'rougeL_diff_stderr,none': 0.24905407003563299},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.38922888616891066,\n",
       "    'acc_stderr,none': 0.017068552680690335},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5463692530636998,\n",
       "    'acc_stderr,none': 0.01648277570651695}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.33936348408710215,\n",
       "    'acc_stderr,none': 0.008667910793954913,\n",
       "    'acc_norm,none': 0.33936348408710215,\n",
       "    'acc_norm_stderr,none': 0.008667910793954915}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.764417845484222,\n",
       "    'acc_stderr,none': 0.009901067586473904,\n",
       "    'acc_norm,none': 0.7562568008705114,\n",
       "    'acc_norm_stderr,none': 0.010017199471500614}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.375,\n",
       "    'acc_stderr,none': 0.02289822829522849,\n",
       "    'acc_norm,none': 0.375,\n",
       "    'acc_norm_stderr,none': 0.02289822829522849}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.5765765765765766,\n",
       "    'acc_stderr,none': 0.014146077134489551}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.5041698256254739,\n",
       "    'exact_match_stderr,strict-match': 0.013772005774791549,\n",
       "    'exact_match,flexible-extract': 0.8786959818043972,\n",
       "    'exact_match_stderr,flexible-extract': 0.00899288849727559}}},\n",
       " 'llama3_70b_c_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.5032046716991881,\n",
       "    'acc_stderr,none': 0.004034416971227507,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5426142401700319,\n",
       "    'acc_stderr,none': 0.007029245892411088,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.3333333333333333,\n",
       "    'acc_stderr,none': 0.04216370213557835},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7757575757575758,\n",
       "    'acc_stderr,none': 0.032568666616811015},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8284313725490197,\n",
       "    'acc_stderr,none': 0.026460569561240658},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.7890295358649789,\n",
       "    'acc_stderr,none': 0.02655837250266192},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.6611570247933884,\n",
       "    'acc_stderr,none': 0.0432076780753667},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.5092592592592593,\n",
       "    'acc_stderr,none': 0.04832853553437056},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.4171779141104294,\n",
       "    'acc_stderr,none': 0.03874102859818083},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.5173410404624278,\n",
       "    'acc_stderr,none': 0.026902900458666647},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.5519553072625698,\n",
       "    'acc_stderr,none': 0.016631976628930595},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3633440514469453,\n",
       "    'acc_stderr,none': 0.02731684767419271},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.36728395061728397,\n",
       "    'acc_stderr,none': 0.026822801759507894},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.559973924380704,\n",
       "    'acc_stderr,none': 0.012678037478574513},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3508771929824561,\n",
       "    'acc_stderr,none': 0.036602988340491645},\n",
       "   'mmlu_other': {'acc,none': 0.5168973286128098,\n",
       "    'acc_stderr,none': 0.008628448460602678,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.62,\n",
       "    'acc_stderr,none': 0.048783173121456316},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.5433962264150943,\n",
       "    'acc_stderr,none': 0.03065674869673943},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3930635838150289,\n",
       "    'acc_stderr,none': 0.03724249595817731},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5605381165919282,\n",
       "    'acc_stderr,none': 0.033310925110381785},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.5728155339805825,\n",
       "    'acc_stderr,none': 0.04897957737781168},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.6410256410256411,\n",
       "    'acc_stderr,none': 0.03142616993791925},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.4393358876117497,\n",
       "    'acc_stderr,none': 0.017747874245683606},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.42483660130718953,\n",
       "    'acc_stderr,none': 0.028304576673141114},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5212765957446809,\n",
       "    'acc_stderr,none': 0.029800481645628693},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8566176470588235,\n",
       "    'acc_stderr,none': 0.021289071205445115},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4036144578313253,\n",
       "    'acc_stderr,none': 0.03819486140758398},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.5713357166070848,\n",
       "    'acc_stderr,none': 0.008731473242231808,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.39473684210526316,\n",
       "    'acc_stderr,none': 0.04598188057816542},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.36363636363636365,\n",
       "    'acc_stderr,none': 0.03427308652999933},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.7357512953367875,\n",
       "    'acc_stderr,none': 0.031821550509166484},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.5538461538461539,\n",
       "    'acc_stderr,none': 0.02520357177302833},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.592436974789916,\n",
       "    'acc_stderr,none': 0.03191863374478465},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.6165137614678899,\n",
       "    'acc_stderr,none': 0.020847156641915984},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.35877862595419846,\n",
       "    'acc_stderr,none': 0.04206739313864908},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6699346405228758,\n",
       "    'acc_stderr,none': 0.019023726160724553},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5181818181818182,\n",
       "    'acc_stderr,none': 0.04785964010794915},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.45714285714285713,\n",
       "    'acc_stderr,none': 0.031891418324213966},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.6218905472636815,\n",
       "    'acc_stderr,none': 0.03428867848778658},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.55,\n",
       "    'acc_stderr,none': 0.04999999999999999},\n",
       "   'mmlu_stem': {'acc,none': 0.3644148430066603,\n",
       "    'acc_stderr,none': 0.008239600592247081,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.37777777777777777,\n",
       "    'acc_stderr,none': 0.04188307537595853},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.3026315789473684,\n",
       "    'acc_stderr,none': 0.03738520676119667},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.6180555555555556,\n",
       "    'acc_stderr,none': 0.040629907841466674},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.0446196043338474},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.04923659639173309},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.28431372549019607,\n",
       "    'acc_stderr,none': 0.04488482852329017},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.46,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3404255319148936,\n",
       "    'acc_stderr,none': 0.030976692998534443},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2620689655172414,\n",
       "    'acc_stderr,none': 0.036646663372252565},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2328042328042328,\n",
       "    'acc_stderr,none': 0.021765961672154527},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.6193548387096774,\n",
       "    'acc_stderr,none': 0.02762171783290703},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3103448275862069,\n",
       "    'acc_stderr,none': 0.032550867699701024},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.33112582781456956,\n",
       "    'acc_stderr,none': 0.038425817186598696},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.4722222222222222,\n",
       "    'acc_stderr,none': 0.0340470532865388},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.04697113923010212}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4467758444216991,\n",
       "    'acc_stderr,none': 0.011249786691110377}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 10.747043619063046,\n",
       "    'bleu_max_stderr,none': 0.5199191608239963,\n",
       "    'bleu_acc,none': 0.5042839657282742,\n",
       "    'bleu_acc_stderr,none': 0.017502858577371286,\n",
       "    'bleu_diff,none': 0.24446069927928393,\n",
       "    'bleu_diff_stderr,none': 0.3481118389444562,\n",
       "    'rouge1_max,none': 30.089215834717162,\n",
       "    'rouge1_max_stderr,none': 0.6749020097244687,\n",
       "    'rouge1_acc,none': 0.5422276621787026,\n",
       "    'rouge1_acc_stderr,none': 0.01744096571248212,\n",
       "    'rouge1_diff,none': 0.4449290434158352,\n",
       "    'rouge1_diff_stderr,none': 0.45105836045473124,\n",
       "    'rouge2_max,none': 18.14369102863998,\n",
       "    'rouge2_max_stderr,none': 0.6739075415992781,\n",
       "    'rouge2_acc,none': 0.4479804161566707,\n",
       "    'rouge2_acc_stderr,none': 0.01740851306342292,\n",
       "    'rouge2_diff,none': -0.36873924898803656,\n",
       "    'rouge2_diff_stderr,none': 0.5307793586469038,\n",
       "    'rougeL_max,none': 26.833469030863085,\n",
       "    'rougeL_max_stderr,none': 0.6680367832530753,\n",
       "    'rougeL_acc,none': 0.5042839657282742,\n",
       "    'rougeL_acc_stderr,none': 0.01750285857737129,\n",
       "    'rougeL_diff,none': -0.12239718966868346,\n",
       "    'rougeL_diff_stderr,none': 0.4502479548438197},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4908200734394125,\n",
       "    'acc_stderr,none': 0.017500550724819753},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6464106843287672,\n",
       "    'acc_stderr,none': 0.016135148130813463}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.32897822445561137,\n",
       "    'acc_stderr,none': 0.008601069831238388,\n",
       "    'acc_norm,none': 0.3340033500837521,\n",
       "    'acc_norm_stderr,none': 0.008633999602399836}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.79379760609358,\n",
       "    'acc_stderr,none': 0.009439460331609507,\n",
       "    'acc_norm,none': 0.7818280739934712,\n",
       "    'acc_norm_stderr,none': 0.009636081958374381}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.2375102375102375,\n",
       "    'acc_stderr,none': 0.012183673723473452}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8559514783927218,\n",
       "    'exact_match_stderr,strict-match': 0.009672110973065291,\n",
       "    'exact_match,flexible-extract': 0.9021986353297953,\n",
       "    'exact_match_stderr,flexible-extract': 0.008182119821849056}}},\n",
       " 'llama3_70b_n_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6913545079048569,\n",
       "    'acc_stderr,none': 0.003678671513586357,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.6420828905419766,\n",
       "    'acc_stderr,none': 0.0067667038504256915,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5238095238095238,\n",
       "    'acc_stderr,none': 0.04467062628403273},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.8121212121212121,\n",
       "    'acc_stderr,none': 0.03050193405942914},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8627450980392157,\n",
       "    'acc_stderr,none': 0.024152225962801584},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8649789029535865,\n",
       "    'acc_stderr,none': 0.022245776632003694},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.8760330578512396,\n",
       "    'acc_stderr,none': 0.03008309871603521},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.7037037037037037,\n",
       "    'acc_stderr,none': 0.04414343666854933},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7239263803680982,\n",
       "    'acc_stderr,none': 0.03512385283705049},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.7052023121387283,\n",
       "    'acc_stderr,none': 0.02454761779480384},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.49162011173184356,\n",
       "    'acc_stderr,none': 0.016720152794672552},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6784565916398714,\n",
       "    'acc_stderr,none': 0.026527724079528872},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7623456790123457,\n",
       "    'acc_stderr,none': 0.023683591837008557},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5743155149934811,\n",
       "    'acc_stderr,none': 0.012628393551811942},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.6842105263157895,\n",
       "    'acc_stderr,none': 0.035650796707083106},\n",
       "   'mmlu_other': {'acc,none': 0.770196330865787,\n",
       "    'acc_stderr,none': 0.0072177138104941325,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.73,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.7584905660377359,\n",
       "    'acc_stderr,none': 0.026341480371118373},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6994219653179191,\n",
       "    'acc_stderr,none': 0.03496101481191181},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.7085201793721974,\n",
       "    'acc_stderr,none': 0.030500283176545843},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8252427184466019,\n",
       "    'acc_stderr,none': 0.0376017800602662},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8632478632478633,\n",
       "    'acc_stderr,none': 0.022509033937077805},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036623},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8786717752234994,\n",
       "    'acc_stderr,none': 0.011675913883906732},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.8366013071895425,\n",
       "    'acc_stderr,none': 0.021170623011213523},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.574468085106383,\n",
       "    'acc_stderr,none': 0.029494827600144376},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8639705882352942,\n",
       "    'acc_stderr,none': 0.020824819397794334},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.5240963855421686,\n",
       "    'acc_stderr,none': 0.03887971849597264},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.8033799155021124,\n",
       "    'acc_stderr,none': 0.0070667838596585745,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5964912280701754,\n",
       "    'acc_stderr,none': 0.04615186962583707},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.8080808080808081,\n",
       "    'acc_stderr,none': 0.02805779167298901},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.9585492227979274,\n",
       "    'acc_stderr,none': 0.01438543285747644},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.764102564102564,\n",
       "    'acc_stderr,none': 0.021525965407408726},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.8403361344537815,\n",
       "    'acc_stderr,none': 0.0237933539975288},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.8348623853211009,\n",
       "    'acc_stderr,none': 0.01591955782997606},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.732824427480916,\n",
       "    'acc_stderr,none': 0.03880848301082396},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.803921568627451,\n",
       "    'acc_stderr,none': 0.01606205642196865},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.6909090909090909,\n",
       "    'acc_stderr,none': 0.044262946482000985},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.7428571428571429,\n",
       "    'acc_stderr,none': 0.027979823538744546},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.8706467661691543,\n",
       "    'acc_stderr,none': 0.023729830881018522},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.85,\n",
       "    'acc_stderr,none': 0.03588702812826371},\n",
       "   'mmlu_stem': {'acc,none': 0.5778623533143038,\n",
       "    'acc_stderr,none': 0.008264404544180733,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.7111111111111111,\n",
       "    'acc_stderr,none': 0.0391545063041425},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7236842105263158,\n",
       "    'acc_stderr,none': 0.03639057569952929},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.875,\n",
       "    'acc_stderr,none': 0.02765610492929436},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.63,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.04688261722621504},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.4411764705882353,\n",
       "    'acc_stderr,none': 0.049406356306056595},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.5872340425531914,\n",
       "    'acc_stderr,none': 0.0321847114140035},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.6344827586206897,\n",
       "    'acc_stderr,none': 0.04013124195424387},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.43386243386243384,\n",
       "    'acc_stderr,none': 0.025525034382474887},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.8387096774193549,\n",
       "    'acc_stderr,none': 0.02092332700642329},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5566502463054187,\n",
       "    'acc_stderr,none': 0.03495334582162934},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.82,\n",
       "    'acc_stderr,none': 0.038612291966536934},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.3037037037037037,\n",
       "    'acc_stderr,none': 0.028037929969114982},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.48344370860927155,\n",
       "    'acc_stderr,none': 0.0408024418562897},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.032757734861009996},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5892857142857143,\n",
       "    'acc_stderr,none': 0.04669510663875191}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4534288638689867,\n",
       "    'acc_stderr,none': 0.011264886135301386}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.34598214285714285,\n",
       "    'acc_stderr,none': 0.02249924183068251,\n",
       "    'acc_norm,none': 0.34598214285714285,\n",
       "    'acc_norm_stderr,none': 0.02249924183068251}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 14.391834509758315,\n",
       "    'bleu_max_stderr,none': 0.6460356453403352,\n",
       "    'bleu_acc,none': 0.4541003671970624,\n",
       "    'bleu_acc_stderr,none': 0.01742959309132351,\n",
       "    'bleu_diff,none': -0.572821950610948,\n",
       "    'bleu_diff_stderr,none': 0.4662062712575406,\n",
       "    'rouge1_max,none': 36.49543368032671,\n",
       "    'rouge1_max_stderr,none': 0.7701311386965441,\n",
       "    'rouge1_acc,none': 0.4981640146878825,\n",
       "    'rouge1_acc_stderr,none': 0.017503383046877062,\n",
       "    'rouge1_diff,none': -0.4320810251374563,\n",
       "    'rouge1_diff_stderr,none': 0.5692820076955282,\n",
       "    'rouge2_max,none': 22.047232883471843,\n",
       "    'rouge2_max_stderr,none': 0.8101256889394897,\n",
       "    'rouge2_acc,none': 0.386780905752754,\n",
       "    'rouge2_acc_stderr,none': 0.017048857010515103,\n",
       "    'rouge2_diff,none': -1.1788918258930416,\n",
       "    'rouge2_diff_stderr,none': 0.6271051628970475,\n",
       "    'rougeL_max,none': 32.94633994325146,\n",
       "    'rougeL_max_stderr,none': 0.7785988351508459,\n",
       "    'rougeL_acc,none': 0.4541003671970624,\n",
       "    'rougeL_acc_stderr,none': 0.017429593091323504,\n",
       "    'rougeL_diff,none': -0.9921608559160059,\n",
       "    'rougeL_diff_stderr,none': 0.5684580493977559},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.5079559363525091,\n",
       "    'acc_stderr,none': 0.01750128507455183},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6582290610101554,\n",
       "    'acc_stderr,none': 0.015965345604524485}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.34003350083752093,\n",
       "    'acc_stderr,none': 0.008672062303343067,\n",
       "    'acc_norm,none': 0.3443886097152429,\n",
       "    'acc_norm_stderr,none': 0.008698577262131604}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.795429815016322,\n",
       "    'acc_stderr,none': 0.009411688039193572,\n",
       "    'acc_norm,none': 0.7970620239390642,\n",
       "    'acc_norm_stderr,none': 0.009383679003767331}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.44553644553644556,\n",
       "    'acc_stderr,none': 0.014229780629024427}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8847611827141774,\n",
       "    'exact_match_stderr,strict-match': 0.008795382301545425,\n",
       "    'exact_match,flexible-extract': 0.9097801364670205,\n",
       "    'exact_match_stderr,flexible-extract': 0.007891537108449958}}},\n",
       " 'llama3_70b_c_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.3378436120210796,\n",
       "    'acc_stderr,none': 0.0039556005989972425,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.31604675876726884,\n",
       "    'acc_stderr,none': 0.006770679371936339,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.36507936507936506,\n",
       "    'acc_stderr,none': 0.04306241259127153},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3151515151515151,\n",
       "    'acc_stderr,none': 0.0362773057502241},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.30392156862745096,\n",
       "    'acc_stderr,none': 0.03228210387037894},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.25316455696202533,\n",
       "    'acc_stderr,none': 0.028304657943035313},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.32231404958677684,\n",
       "    'acc_stderr,none': 0.042664163633521664},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.23148148148148148,\n",
       "    'acc_stderr,none': 0.04077494709252626},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.3374233128834356,\n",
       "    'acc_stderr,none': 0.03714908409935574},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.30057803468208094,\n",
       "    'acc_stderr,none': 0.0246853168672578},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.32513966480446926,\n",
       "    'acc_stderr,none': 0.015666542785053555},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2765273311897106,\n",
       "    'acc_stderr,none': 0.025403832978179608},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.30246913580246915,\n",
       "    'acc_stderr,none': 0.025557653981868038},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.34419817470664926,\n",
       "    'acc_stderr,none': 0.012134433741002568},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.23976608187134502,\n",
       "    'acc_stderr,none': 0.03274485211946956},\n",
       "   'mmlu_other': {'acc,none': 0.3495333118764081,\n",
       "    'acc_stderr,none': 0.0084435996432762,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.38113207547169814,\n",
       "    'acc_stderr,none': 0.029890609686286648},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.37572254335260113,\n",
       "    'acc_stderr,none': 0.03692820767264867},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.19730941704035873,\n",
       "    'acc_stderr,none': 0.02670985334496796},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.4854368932038835,\n",
       "    'acc_stderr,none': 0.04948637324026637},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.28205128205128205,\n",
       "    'acc_stderr,none': 0.029480360549541194},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.3997445721583653,\n",
       "    'acc_stderr,none': 0.01751684790705328},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.027530078447110314},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.3404255319148936,\n",
       "    'acc_stderr,none': 0.028267657482650158},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.47058823529411764,\n",
       "    'acc_stderr,none': 0.03032024326500413},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.22289156626506024,\n",
       "    'acc_stderr,none': 0.03240004825594688},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.376665583360416,\n",
       "    'acc_stderr,none': 0.008665673863123252,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2894736842105263,\n",
       "    'acc_stderr,none': 0.04266339443159394},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.4494949494949495,\n",
       "    'acc_stderr,none': 0.0354413249194797},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.44559585492227977,\n",
       "    'acc_stderr,none': 0.035870149860756595},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.4256410256410256,\n",
       "    'acc_stderr,none': 0.025069094387296535},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.40756302521008403,\n",
       "    'acc_stderr,none': 0.03191863374478466},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3981651376146789,\n",
       "    'acc_stderr,none': 0.020987989422654257},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.366412213740458,\n",
       "    'acc_stderr,none': 0.04225875451969637},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.27941176470588236,\n",
       "    'acc_stderr,none': 0.018152871051538802},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.2727272727272727,\n",
       "    'acc_stderr,none': 0.04265792110940589},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.4816326530612245,\n",
       "    'acc_stderr,none': 0.031987615467631264},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.34328358208955223,\n",
       "    'acc_stderr,none': 0.03357379665433432},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.0479372485441102},\n",
       "   'mmlu_stem': {'acc,none': 0.3209641611163971,\n",
       "    'acc_stderr,none': 0.008216452065328,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.042295258468165065},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2814814814814815,\n",
       "    'acc_stderr,none': 0.03885004245800254},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.3881578947368421,\n",
       "    'acc_stderr,none': 0.03965842097512744},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.3680555555555556,\n",
       "    'acc_stderr,none': 0.040329990539607195},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.43,\n",
       "    'acc_stderr,none': 0.04975698519562429},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.047937248544110196},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.37254901960784315,\n",
       "    'acc_stderr,none': 0.04810840148082634},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.041633319989322695},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.25957446808510637,\n",
       "    'acc_stderr,none': 0.02865917937429232},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2689655172413793,\n",
       "    'acc_stderr,none': 0.036951833116502325},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2830687830687831,\n",
       "    'acc_stderr,none': 0.023201392938194978},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.38387096774193546,\n",
       "    'acc_stderr,none': 0.02766618207553964},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3448275862068966,\n",
       "    'acc_stderr,none': 0.033442837442804574},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.26296296296296295,\n",
       "    'acc_stderr,none': 0.026842057873833706},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.33774834437086093,\n",
       "    'acc_stderr,none': 0.038615575462551684},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.5092592592592593,\n",
       "    'acc_stderr,none': 0.034093869469927006},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.16071428571428573,\n",
       "    'acc_stderr,none': 0.0348594609647574}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.37615148413510746,\n",
       "    'acc_stderr,none': 0.010961496293030143}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.30580357142857145,\n",
       "    'acc_stderr,none': 0.021792582688756987,\n",
       "    'acc_norm,none': 0.30580357142857145,\n",
       "    'acc_norm_stderr,none': 0.021792582688756987}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.7916817351557293,\n",
       "    'bleu_max_stderr,none': 0.09303642986853196,\n",
       "    'bleu_acc,none': 0.3769889840881273,\n",
       "    'bleu_acc_stderr,none': 0.016965517578930354,\n",
       "    'bleu_diff,none': -0.10204743276007273,\n",
       "    'bleu_diff_stderr,none': 0.08631342933660471,\n",
       "    'rouge1_max,none': 16.27022249316565,\n",
       "    'rouge1_max_stderr,none': 0.3190698819538172,\n",
       "    'rouge1_acc,none': 0.5042839657282742,\n",
       "    'rouge1_acc_stderr,none': 0.017502858577371258,\n",
       "    'rouge1_diff,none': 0.1959855129467676,\n",
       "    'rouge1_diff_stderr,none': 0.349134914936304,\n",
       "    'rouge2_max,none': 3.795027155852737,\n",
       "    'rouge2_max_stderr,none': 0.25399037472627883,\n",
       "    'rouge2_acc,none': 0.16279069767441862,\n",
       "    'rouge2_acc_stderr,none': 0.012923696051772262,\n",
       "    'rouge2_diff,none': -1.1536616886079518,\n",
       "    'rouge2_diff_stderr,none': 0.23605557102171354,\n",
       "    'rougeL_max,none': 14.601096874637628,\n",
       "    'rougeL_max_stderr,none': 0.2984021812954505,\n",
       "    'rougeL_acc,none': 0.5030599755201959,\n",
       "    'rougeL_acc_stderr,none': 0.017503173260960625,\n",
       "    'rougeL_diff,none': 0.0470584437589079,\n",
       "    'rougeL_diff_stderr,none': 0.33394751537595097},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.22031823745410037,\n",
       "    'acc_stderr,none': 0.014509045171487286},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.38507280756049417,\n",
       "    'acc_stderr,none': 0.015975820931627572}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2814070351758794,\n",
       "    'acc_stderr,none': 0.008232079320325311,\n",
       "    'acc_norm,none': 0.28743718592964823,\n",
       "    'acc_norm_stderr,none': 0.008284830813404313}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7094668117519043,\n",
       "    'acc_stderr,none': 0.010592765034696538,\n",
       "    'acc_norm,none': 0.6958650707290533,\n",
       "    'acc_norm_stderr,none': 0.010733493335721314}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.021923384489444957,\n",
       "    'acc_norm,none': 0.3125,\n",
       "    'acc_norm_stderr,none': 0.021923384489444957}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.257985257985258,\n",
       "    'acc_stderr,none': 0.012526328490375856}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.5921152388172858,\n",
       "    'exact_match_stderr,strict-match': 0.013536742075643088,\n",
       "    'exact_match,flexible-extract': 0.8059135708870356,\n",
       "    'exact_match_stderr,flexible-extract': 0.010893918308192413}}},\n",
       " 'llama3_70b_a_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.34346959122632104,\n",
       "    'acc_stderr,none': 0.0038651700998585496,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.4716259298618491,\n",
       "    'acc_stderr,none': 0.007083567801902924,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.5757575757575758,\n",
       "    'acc_stderr,none': 0.038592681420702636},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.6911764705882353,\n",
       "    'acc_stderr,none': 0.03242661719827218},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.6413502109704642,\n",
       "    'acc_stderr,none': 0.031219569445301854},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5454545454545454,\n",
       "    'acc_stderr,none': 0.04545454545454548},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.42592592592592593,\n",
       "    'acc_stderr,none': 0.047803436269367894},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.34355828220858897,\n",
       "    'acc_stderr,none': 0.03731133519673892},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.48265895953757226,\n",
       "    'acc_stderr,none': 0.02690290045866664},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.5396648044692738,\n",
       "    'acc_stderr,none': 0.01666979959211203},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3022508038585209,\n",
       "    'acc_stderr,none': 0.026082700695399655},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.2808641975308642,\n",
       "    'acc_stderr,none': 0.025006469755799197},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.4941329856584094,\n",
       "    'acc_stderr,none': 0.012769356925216526},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.19883040935672514,\n",
       "    'acc_stderr,none': 0.030611116557432528},\n",
       "   'mmlu_other': {'acc,none': 0.30222079176054073,\n",
       "    'acc_stderr,none': 0.00822077883607661,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.2981132075471698,\n",
       "    'acc_stderr,none': 0.028152837942493868},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.28901734104046245,\n",
       "    'acc_stderr,none': 0.034564257450869995},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.17,\n",
       "    'acc_stderr,none': 0.0377525168068637},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.35874439461883406,\n",
       "    'acc_stderr,none': 0.032190792004199956},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.34951456310679613,\n",
       "    'acc_stderr,none': 0.047211885060971716},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.3547008547008547,\n",
       "    'acc_stderr,none': 0.03134250486245402},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.28607918263090676,\n",
       "    'acc_stderr,none': 0.016160871405127546},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.2908496732026144,\n",
       "    'acc_stderr,none': 0.026004800363952113},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.36879432624113473,\n",
       "    'acc_stderr,none': 0.02878222756134725},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.25735294117647056,\n",
       "    'acc_stderr,none': 0.026556519470041513},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.28313253012048195,\n",
       "    'acc_stderr,none': 0.03507295431370518},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.31231719207019826,\n",
       "    'acc_stderr,none': 0.00820438929619202,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2982456140350877,\n",
       "    'acc_stderr,none': 0.043036840335373173},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.18181818181818182,\n",
       "    'acc_stderr,none': 0.027479603010538804},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.35233160621761656,\n",
       "    'acc_stderr,none': 0.03447478286414357},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2128205128205128,\n",
       "    'acc_stderr,none': 0.020752423722128023},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.24369747899159663,\n",
       "    'acc_stderr,none': 0.027886828078380554},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.24036697247706423,\n",
       "    'acc_stderr,none': 0.01832060732096407},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.31297709923664124,\n",
       "    'acc_stderr,none': 0.04066962905677697},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.41830065359477125,\n",
       "    'acc_stderr,none': 0.01995597514583555},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.2636363636363636,\n",
       "    'acc_stderr,none': 0.04220224692971987},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.46530612244897956,\n",
       "    'acc_stderr,none': 0.03193207024425314},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.40298507462686567,\n",
       "    'acc_stderr,none': 0.034683432951111266},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_stem': {'acc,none': 0.22327941642879798,\n",
       "    'acc_stderr,none': 0.0074033885594655575,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.18518518518518517,\n",
       "    'acc_stderr,none': 0.03355677216313142},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.19736842105263158,\n",
       "    'acc_stderr,none': 0.03238981601699397},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2569444444444444,\n",
       "    'acc_stderr,none': 0.03653946969442099},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.29,\n",
       "    'acc_stderr,none': 0.045604802157206845},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.26382978723404255,\n",
       "    'acc_stderr,none': 0.02880998985410298},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.21379310344827587,\n",
       "    'acc_stderr,none': 0.03416520447747548},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.21164021164021163,\n",
       "    'acc_stderr,none': 0.021037331505262886},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2032258064516129,\n",
       "    'acc_stderr,none': 0.022891687984554963},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.33,\n",
       "    'acc_stderr,none': 0.04725815626252604},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2052980132450331,\n",
       "    'acc_stderr,none': 0.032979866484738336},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.21296296296296297,\n",
       "    'acc_stderr,none': 0.027920963147993666},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.33035714285714285,\n",
       "    'acc_stderr,none': 0.04464285714285713}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.44779938587512796,\n",
       "    'acc_stderr,none': 0.011252242102001767}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.3549107142857143,\n",
       "    'acc_stderr,none': 0.02263162341632674,\n",
       "    'acc_norm,none': 0.3549107142857143,\n",
       "    'acc_norm_stderr,none': 0.02263162341632674}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 7.186030823799035,\n",
       "    'bleu_max_stderr,none': 0.2976903761490195,\n",
       "    'bleu_acc,none': 0.4920440636474908,\n",
       "    'bleu_acc_stderr,none': 0.017501285074551814,\n",
       "    'bleu_diff,none': -0.19362670717044234,\n",
       "    'bleu_diff_stderr,none': 0.19606698071736023,\n",
       "    'rouge1_max,none': 25.217291877583296,\n",
       "    'rouge1_max_stderr,none': 0.49552527518525,\n",
       "    'rouge1_acc,none': 0.5079559363525091,\n",
       "    'rouge1_acc_stderr,none': 0.01750128507455184,\n",
       "    'rouge1_diff,none': 0.04690274157387491,\n",
       "    'rouge1_diff_stderr,none': 0.3380640359381212,\n",
       "    'rouge2_max,none': 14.019606369528404,\n",
       "    'rouge2_max_stderr,none': 0.4937090058614036,\n",
       "    'rouge2_acc,none': 0.3818849449204406,\n",
       "    'rouge2_acc_stderr,none': 0.01700810193916349,\n",
       "    'rouge2_diff,none': -0.8907716827215294,\n",
       "    'rouge2_diff_stderr,none': 0.3521401893119823,\n",
       "    'rougeL_max,none': 22.059383682551506,\n",
       "    'rougeL_max_stderr,none': 0.48823283034254056,\n",
       "    'rougeL_acc,none': 0.4834761321909425,\n",
       "    'rougeL_acc_stderr,none': 0.017493940190057723,\n",
       "    'rougeL_diff,none': -0.4684831182081133,\n",
       "    'rougeL_diff_stderr,none': 0.33470421497008024},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4541003671970624,\n",
       "    'acc_stderr,none': 0.01742959309132352},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5961727803056094,\n",
       "    'acc_stderr,none': 0.016334354466768022}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3132328308207705,\n",
       "    'acc_stderr,none': 0.008490611920810433,\n",
       "    'acc_norm,none': 0.31825795644891125,\n",
       "    'acc_norm_stderr,none': 0.008527078567878583}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7850924918389554,\n",
       "    'acc_stderr,none': 0.009583665082653308,\n",
       "    'acc_norm,none': 0.7747551686615887,\n",
       "    'acc_norm_stderr,none': 0.009746643471032145}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.33482142857142855,\n",
       "    'acc_stderr,none': 0.022321428571428627,\n",
       "    'acc_norm,none': 0.33482142857142855,\n",
       "    'acc_norm_stderr,none': 0.022321428571428627}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.21294021294021295,\n",
       "    'acc_stderr,none': 0.011720679449797579}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.25094768764215314,\n",
       "    'exact_match_stderr,strict-match': 0.011942354768308837,\n",
       "    'exact_match,flexible-extract': 0.8733889310083397,\n",
       "    'exact_match_stderr,flexible-extract': 0.009159715283081092}}},\n",
       " 'llama3_70b_e_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.42273180458624127,\n",
       "    'acc_stderr,none': 0.0040785828773805795,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.4499468650371945,\n",
       "    'acc_stderr,none': 0.007157851347957301,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.24603174603174602,\n",
       "    'acc_stderr,none': 0.03852273364924316},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3090909090909091,\n",
       "    'acc_stderr,none': 0.03608541011573967},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.6127450980392157,\n",
       "    'acc_stderr,none': 0.03418931233833344},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.45147679324894513,\n",
       "    'acc_stderr,none': 0.0323936001739747},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5702479338842975,\n",
       "    'acc_stderr,none': 0.04519082021319773},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.4444444444444444,\n",
       "    'acc_stderr,none': 0.04803752235190193},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.49693251533742333,\n",
       "    'acc_stderr,none': 0.03928297078179663},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.4682080924855491,\n",
       "    'acc_stderr,none': 0.026864624366756636},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.358659217877095,\n",
       "    'acc_stderr,none': 0.01604045442616448},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3440514469453376,\n",
       "    'acc_stderr,none': 0.026981478043648022},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.5123456790123457,\n",
       "    'acc_stderr,none': 0.027812262269327242},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5065189048239895,\n",
       "    'acc_stderr,none': 0.012769150688867506},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.42105263157894735,\n",
       "    'acc_stderr,none': 0.03786720706234215},\n",
       "   'mmlu_other': {'acc,none': 0.41873189571934344,\n",
       "    'acc_stderr,none': 0.008732779005580749,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.43,\n",
       "    'acc_stderr,none': 0.049756985195624284},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.3660377358490566,\n",
       "    'acc_stderr,none': 0.029647813539365252},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3179190751445087,\n",
       "    'acc_stderr,none': 0.0355068398916558},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5426008968609866,\n",
       "    'acc_stderr,none': 0.03343577705583065},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.3786407766990291,\n",
       "    'acc_stderr,none': 0.048026946982589726},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.6196581196581197,\n",
       "    'acc_stderr,none': 0.03180425204384099},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.48,\n",
       "    'acc_stderr,none': 0.05021167315686779},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.4112388250319285,\n",
       "    'acc_stderr,none': 0.01759597190805657},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.027530078447110296},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.4219858156028369,\n",
       "    'acc_stderr,none': 0.029462189233370593},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.4522058823529412,\n",
       "    'acc_stderr,none': 0.030233758551596452},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.35542168674698793,\n",
       "    'acc_stderr,none': 0.03726214354322415},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.4809879753006175,\n",
       "    'acc_stderr,none': 0.008781444199240639,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.41228070175438597,\n",
       "    'acc_stderr,none': 0.04630653203366596},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.45454545454545453,\n",
       "    'acc_stderr,none': 0.03547601494006936},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.5803108808290155,\n",
       "    'acc_stderr,none': 0.035615873276858834},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.4256410256410256,\n",
       "    'acc_stderr,none': 0.025069094387296535},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.032145368597886394},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3376146788990826,\n",
       "    'acc_stderr,none': 0.020275265986638917},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2595419847328244,\n",
       "    'acc_stderr,none': 0.0384487613978527},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.5800653594771242,\n",
       "    'acc_stderr,none': 0.019966811178256483},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.41818181818181815,\n",
       "    'acc_stderr,none': 0.04724577405731572},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.6489795918367347,\n",
       "    'acc_stderr,none': 0.030555316755573637},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.6268656716417911,\n",
       "    'acc_stderr,none': 0.034198326081760065},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.59,\n",
       "    'acc_stderr,none': 0.04943110704237102},\n",
       "   'mmlu_stem': {'acc,none': 0.3292102759276879,\n",
       "    'acc_stderr,none': 0.008266709646734366,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.37777777777777777,\n",
       "    'acc_stderr,none': 0.04188307537595853},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.2631578947368421,\n",
       "    'acc_stderr,none': 0.03583496176361063},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.4930555555555556,\n",
       "    'acc_stderr,none': 0.04180806750294938},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.042923469599092816},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.04878317312145633},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.04292346959909282},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.3431372549019608,\n",
       "    'acc_stderr,none': 0.04724007352383888},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3659574468085106,\n",
       "    'acc_stderr,none': 0.031489558297455304},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.42758620689655175,\n",
       "    'acc_stderr,none': 0.041227371113703316},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.23544973544973544,\n",
       "    'acc_stderr,none': 0.021851509822031715},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.36129032258064514,\n",
       "    'acc_stderr,none': 0.027327548447957525},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3103448275862069,\n",
       "    'acc_stderr,none': 0.03255086769970103},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.49,\n",
       "    'acc_stderr,none': 0.05024183937956912},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2781456953642384,\n",
       "    'acc_stderr,none': 0.03658603262763744},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.35185185185185186,\n",
       "    'acc_stderr,none': 0.03256850570293648},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.45535714285714285,\n",
       "    'acc_stderr,none': 0.047268355537191}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.43039918116683723,\n",
       "    'acc_stderr,none': 0.011203917417496392}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.452923188332979,\n",
       "    'bleu_max_stderr,none': 0.2159804912757085,\n",
       "    'bleu_acc,none': 0.43818849449204406,\n",
       "    'bleu_acc_stderr,none': 0.01736923616440443,\n",
       "    'bleu_diff,none': -0.4549180061692236,\n",
       "    'bleu_diff_stderr,none': 0.1492367922513079,\n",
       "    'rouge1_max,none': 23.402392943646866,\n",
       "    'rouge1_max_stderr,none': 0.42568992129336086,\n",
       "    'rouge1_acc,none': 0.4614443084455324,\n",
       "    'rouge1_acc_stderr,none': 0.017451384104637455,\n",
       "    'rouge1_diff,none': -0.9678386827689932,\n",
       "    'rouge1_diff_stderr,none': 0.3321736263535224,\n",
       "    'rouge2_max,none': 12.415911330124876,\n",
       "    'rouge2_max_stderr,none': 0.420589134879823,\n",
       "    'rouge2_acc,none': 0.3537331701346389,\n",
       "    'rouge2_acc_stderr,none': 0.016737814358846147,\n",
       "    'rouge2_diff,none': -1.5150713806140053,\n",
       "    'rouge2_diff_stderr,none': 0.335850719845891,\n",
       "    'rougeL_max,none': 20.413250710168896,\n",
       "    'rougeL_max_stderr,none': 0.41958424155133045,\n",
       "    'rougeL_acc,none': 0.42472460220318237,\n",
       "    'rougeL_acc_stderr,none': 0.01730400095716747,\n",
       "    'rougeL_diff,none': -1.2343052167789954,\n",
       "    'rougeL_diff_stderr,none': 0.31283973652408437},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.31334149326805383,\n",
       "    'acc_stderr,none': 0.016238065069059598},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.46042942351359656,\n",
       "    'acc_stderr,none': 0.01653885248336244}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3051926298157454,\n",
       "    'acc_stderr,none': 0.008429849471087473,\n",
       "    'acc_norm,none': 0.3135678391959799,\n",
       "    'acc_norm_stderr,none': 0.008493078900794976}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.764417845484222,\n",
       "    'acc_stderr,none': 0.009901067586473914,\n",
       "    'acc_norm,none': 0.7529923830250272,\n",
       "    'acc_norm_stderr,none': 0.010062268140772615}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3705357142857143,\n",
       "    'acc_stderr,none': 0.0228426677334829,\n",
       "    'acc_norm,none': 0.3705357142857143,\n",
       "    'acc_norm_stderr,none': 0.0228426677334829}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.23177723177723178,\n",
       "    'acc_stderr,none': 0.012080893552302283}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.354814253222138,\n",
       "    'exact_match_stderr,strict-match': 0.013179083387979199,\n",
       "    'exact_match,flexible-extract': 0.8893100833965125,\n",
       "    'exact_match_stderr,flexible-extract': 0.008642172551392473}}},\n",
       " 'llama3_70b_e_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.7230451502634953,\n",
       "    'acc_stderr,none': 0.0035789783761124203,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.675451647183847,\n",
       "    'acc_stderr,none': 0.0066212839024168925,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.0442626668137991},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7818181818181819,\n",
       "    'acc_stderr,none': 0.03225078108306289},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8480392156862745,\n",
       "    'acc_stderr,none': 0.025195658428931803},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8438818565400844,\n",
       "    'acc_stderr,none': 0.023627159460318677},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.8842975206611571,\n",
       "    'acc_stderr,none': 0.029199802455622793},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.8333333333333334,\n",
       "    'acc_stderr,none': 0.036028141763926436},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7914110429447853,\n",
       "    'acc_stderr,none': 0.03192193448934724},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.7167630057803468,\n",
       "    'acc_stderr,none': 0.02425790170532339},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.582122905027933,\n",
       "    'acc_stderr,none': 0.016495400635820084},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.7395498392282959,\n",
       "    'acc_stderr,none': 0.024926723224845557},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.8209876543209876,\n",
       "    'acc_stderr,none': 0.021330868762127045},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5691003911342895,\n",
       "    'acc_stderr,none': 0.012647695889547226},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.8187134502923976,\n",
       "    'acc_stderr,none': 0.029547741687640038},\n",
       "   'mmlu_other': {'acc,none': 0.7775989700675893,\n",
       "    'acc_stderr,none': 0.007092011017829385,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.73,\n",
       "    'acc_stderr,none': 0.04461960433384739},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.8037735849056604,\n",
       "    'acc_stderr,none': 0.02444238813110086},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.7109826589595376,\n",
       "    'acc_stderr,none': 0.034564257450869995},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.050251890762960605},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.7085201793721974,\n",
       "    'acc_stderr,none': 0.03050028317654585},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8737864077669902,\n",
       "    'acc_stderr,none': 0.03288180278808628},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8717948717948718,\n",
       "    'acc_stderr,none': 0.021901905115073318},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036622},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8850574712643678,\n",
       "    'acc_stderr,none': 0.01140572072459397},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.8464052287581699,\n",
       "    'acc_stderr,none': 0.02064559791041876},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5709219858156028,\n",
       "    'acc_stderr,none': 0.029525914302558562},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8602941176470589,\n",
       "    'acc_stderr,none': 0.021059408919012507},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.463855421686747,\n",
       "    'acc_stderr,none': 0.03882310850890594},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.8280792980175495,\n",
       "    'acc_stderr,none': 0.006677443504363067,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.6140350877192983,\n",
       "    'acc_stderr,none': 0.04579639422070434},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.8686868686868687,\n",
       "    'acc_stderr,none': 0.024063156416822527},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.9430051813471503,\n",
       "    'acc_stderr,none': 0.016731085293607558},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.7923076923076923,\n",
       "    'acc_stderr,none': 0.020567539567246794},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.8361344537815126,\n",
       "    'acc_stderr,none': 0.024044054940440488},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.9211009174311927,\n",
       "    'acc_stderr,none': 0.011558198113769572},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.7938931297709924,\n",
       "    'acc_stderr,none': 0.03547771004159464},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.7924836601307189,\n",
       "    'acc_stderr,none': 0.016405924270103234},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.7272727272727273,\n",
       "    'acc_stderr,none': 0.04265792110940588},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.746938775510204,\n",
       "    'acc_stderr,none': 0.02783302387139969},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.8656716417910447,\n",
       "    'acc_stderr,none': 0.024112678240900826},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.88,\n",
       "    'acc_stderr,none': 0.03265986323710906},\n",
       "   'mmlu_stem': {'acc,none': 0.6378052648271487,\n",
       "    'acc_stderr,none': 0.008070187226181986,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.048783173121456316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6962962962962963,\n",
       "    'acc_stderr,none': 0.03972552884785136},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.875,\n",
       "    'acc_stderr,none': 0.026913523521537846},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.9027777777777778,\n",
       "    'acc_stderr,none': 0.024774516250440165},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.51,\n",
       "    'acc_stderr,none': 0.05024183937956911},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.64,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.049236596391733084},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.5294117647058824,\n",
       "    'acc_stderr,none': 0.04966570903978529},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036623},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.6638297872340425,\n",
       "    'acc_stderr,none': 0.030881618520676942},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.6620689655172414,\n",
       "    'acc_stderr,none': 0.039417076320648906},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.025487187147859372},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.864516129032258,\n",
       "    'acc_stderr,none': 0.019469334586486933},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.6206896551724138,\n",
       "    'acc_stderr,none': 0.03413963805906235},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.83,\n",
       "    'acc_stderr,none': 0.0377525168068637},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.35555555555555557,\n",
       "    'acc_stderr,none': 0.0291857149498574},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.5033112582781457,\n",
       "    'acc_stderr,none': 0.04082393379449654},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6898148148148148,\n",
       "    'acc_stderr,none': 0.031546962856566274},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5357142857142857,\n",
       "    'acc_stderr,none': 0.04733667890053756}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.43551688843398156,\n",
       "    'acc_stderr,none': 0.011219586604022594}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 9.642204091215383,\n",
       "    'bleu_max_stderr,none': 0.49477552637508115,\n",
       "    'bleu_acc,none': 0.620563035495716,\n",
       "    'bleu_acc_stderr,none': 0.016987039266142975,\n",
       "    'bleu_diff,none': 0.9442922543967592,\n",
       "    'bleu_diff_stderr,none': 0.37346343593247894,\n",
       "    'rouge1_max,none': 30.00376835144213,\n",
       "    'rouge1_max_stderr,none': 0.7130670800232768,\n",
       "    'rouge1_acc,none': 0.7099143206854345,\n",
       "    'rouge1_acc_stderr,none': 0.01588623687420952,\n",
       "    'rouge1_diff,none': 7.145845516407771,\n",
       "    'rouge1_diff_stderr,none': 0.6366063431881058,\n",
       "    'rouge2_max,none': 12.272590154011654,\n",
       "    'rouge2_max_stderr,none': 0.7119786598474964,\n",
       "    'rouge2_acc,none': 0.2533659730722154,\n",
       "    'rouge2_acc_stderr,none': 0.015225899340826856,\n",
       "    'rouge2_diff,none': 0.8007321131422266,\n",
       "    'rouge2_diff_stderr,none': 0.587801771714321,\n",
       "    'rougeL_max,none': 28.068598540985676,\n",
       "    'rougeL_max_stderr,none': 0.6966719090640683,\n",
       "    'rougeL_acc,none': 0.7001223990208079,\n",
       "    'rougeL_acc_stderr,none': 0.016040352966713616,\n",
       "    'rougeL_diff,none': 6.7481424410537025,\n",
       "    'rougeL_diff_stderr,none': 0.6327256634759059},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.48959608323133413,\n",
       "    'acc_stderr,none': 0.017499711430249264},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6528340114952984,\n",
       "    'acc_stderr,none': 0.01595351060013572}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3504187604690117,\n",
       "    'acc_stderr,none': 0.008733956045067806,\n",
       "    'acc_norm,none': 0.35711892797319933,\n",
       "    'acc_norm_stderr,none': 0.008771469242554543}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7976060935799782,\n",
       "    'acc_stderr,none': 0.00937428968280767,\n",
       "    'acc_norm,none': 0.7997823721436343,\n",
       "    'acc_norm_stderr,none': 0.009336465387350825}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3549107142857143,\n",
       "    'acc_stderr,none': 0.022631623416326744,\n",
       "    'acc_norm,none': 0.3549107142857143,\n",
       "    'acc_norm_stderr,none': 0.022631623416326744}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.7084357084357085,\n",
       "    'acc_stderr,none': 0.013011802821401595}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8438210765731615,\n",
       "    'exact_match_stderr,strict-match': 0.009999509369757466,\n",
       "    'exact_match,flexible-extract': 0.9044730856709629,\n",
       "    'exact_match_stderr,flexible-extract': 0.00809660577115574}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['llama3_70b_mmlu', 'llama3_70b_social_iqa', 'llama3_70b_gpqa_main_zeroshot', 'llama3_70b_truthfulqa', 'llama3_70b_mathqa', 'llama3_70b_piqa', 'llama3_70b_gpqa_main_n_shot', 'llama3_70b_commonsense_qa', 'llama3_70b_gsm8k_5_shots_without_cot'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['llama3_70b_c_high'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llama3_70b_a_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6251958410482837,\n",
       "    'acc_stderr,none': 0.00392094851749934,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5742826780021254,\n",
       "    'acc_stderr,none': 0.007036181304622663,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.4365079365079365,\n",
       "    'acc_stderr,none': 0.04435932892851466},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7515151515151515,\n",
       "    'acc_stderr,none': 0.033744026441394036},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.7892156862745098,\n",
       "    'acc_stderr,none': 0.02862654791243739},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.7763713080168776,\n",
       "    'acc_stderr,none': 0.027123298205229966},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.7603305785123967,\n",
       "    'acc_stderr,none': 0.038968789850704164},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6759259259259259,\n",
       "    'acc_stderr,none': 0.04524596007030049},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.6625766871165644,\n",
       "    'acc_stderr,none': 0.03714908409935575},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.5809248554913294,\n",
       "    'acc_stderr,none': 0.026564178111422615},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.4670391061452514,\n",
       "    'acc_stderr,none': 0.016686126653013934},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6302250803858521,\n",
       "    'acc_stderr,none': 0.027417996705631},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.6790123456790124,\n",
       "    'acc_stderr,none': 0.025976566010862727},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.4980443285528031,\n",
       "    'acc_stderr,none': 0.012770138422208635},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.6198830409356725,\n",
       "    'acc_stderr,none': 0.037229657413855394},\n",
       "   'mmlu_other': {'acc,none': 0.694560669456067,\n",
       "    'acc_stderr,none': 0.007950118238955634,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.6792452830188679,\n",
       "    'acc_stderr,none': 0.028727502957880267},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6069364161849711,\n",
       "    'acc_stderr,none': 0.03724249595817731},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.049236596391733084},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5560538116591929,\n",
       "    'acc_stderr,none': 0.03334625674242728},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8446601941747572,\n",
       "    'acc_stderr,none': 0.03586594738573975},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8076923076923077,\n",
       "    'acc_stderr,none': 0.02581923325648373},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816503},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8033205619412516,\n",
       "    'acc_stderr,none': 0.014214138556913912},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.7320261437908496,\n",
       "    'acc_stderr,none': 0.025360603796242557},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.029827499313594685},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.7977941176470589,\n",
       "    'acc_stderr,none': 0.024398192986654924},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4759036144578313,\n",
       "    'acc_stderr,none': 0.03887971849597264},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.7065323366915827,\n",
       "    'acc_stderr,none': 0.008020371863840536,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.047036043419179864},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.7676767676767676,\n",
       "    'acc_stderr,none': 0.030088629490217487},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8860103626943006,\n",
       "    'acc_stderr,none': 0.022935144053919422},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.6846153846153846,\n",
       "    'acc_stderr,none': 0.02355964698318995},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.7478991596638656,\n",
       "    'acc_stderr,none': 0.028205545033277723},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.818348623853211,\n",
       "    'acc_stderr,none': 0.016530617409266857},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.7022900763358778,\n",
       "    'acc_stderr,none': 0.04010358942462203},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6029411764705882,\n",
       "    'acc_stderr,none': 0.01979448890002411},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5545454545454546,\n",
       "    'acc_stderr,none': 0.047605488214603246},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.6163265306122448,\n",
       "    'acc_stderr,none': 0.031130880396235922},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7611940298507462,\n",
       "    'acc_stderr,none': 0.030147775935409224},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816505},\n",
       "   'mmlu_stem': {'acc,none': 0.5534411671424041,\n",
       "    'acc_stderr,none': 0.008487831107704711,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.33,\n",
       "    'acc_stderr,none': 0.047258156262526045},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6814814814814815,\n",
       "    'acc_stderr,none': 0.04024778401977109},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7236842105263158,\n",
       "    'acc_stderr,none': 0.03639057569952929},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.7986111111111112,\n",
       "    'acc_stderr,none': 0.033536474697138406},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.46,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.52,\n",
       "    'acc_stderr,none': 0.050211673156867795},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.4803921568627451,\n",
       "    'acc_stderr,none': 0.04971358884367406},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.65,\n",
       "    'acc_stderr,none': 0.047937248544110196},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.4978723404255319,\n",
       "    'acc_stderr,none': 0.03268572658667492},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5655172413793104,\n",
       "    'acc_stderr,none': 0.04130740879555497},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.4708994708994709,\n",
       "    'acc_stderr,none': 0.025707658614154954},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.7741935483870968,\n",
       "    'acc_stderr,none': 0.02378557788418101},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.034819048444388045},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2814814814814815,\n",
       "    'acc_stderr,none': 0.027420019350945266},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.47019867549668876,\n",
       "    'acc_stderr,none': 0.040752249922169775},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.625,\n",
       "    'acc_stderr,none': 0.033016908987210894},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.48214285714285715,\n",
       "    'acc_stderr,none': 0.047427623612430116}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.39048106448311154,\n",
       "    'acc_stderr,none': 0.01103932371486307}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.602474965910092,\n",
       "    'bleu_max_stderr,none': 0.20099493218185174,\n",
       "    'bleu_acc,none': 0.4565483476132191,\n",
       "    'bleu_acc_stderr,none': 0.01743728095318368,\n",
       "    'bleu_diff,none': -0.08684781789820634,\n",
       "    'bleu_diff_stderr,none': 0.14633296646745628,\n",
       "    'rouge1_max,none': 21.26109589339087,\n",
       "    'rouge1_max_stderr,none': 0.39900378176132173,\n",
       "    'rouge1_acc,none': 0.45777233782129745,\n",
       "    'rouge1_acc_stderr,none': 0.01744096571248212,\n",
       "    'rouge1_diff,none': -0.05170815955765682,\n",
       "    'rouge1_diff_stderr,none': 0.31378678011623096,\n",
       "    'rouge2_max,none': 11.3095489582619,\n",
       "    'rouge2_max_stderr,none': 0.4008404209798527,\n",
       "    'rouge2_acc,none': 0.32802937576499386,\n",
       "    'rouge2_acc_stderr,none': 0.016435632932815004,\n",
       "    'rouge2_diff,none': -0.6478231217086778,\n",
       "    'rouge2_diff_stderr,none': 0.29714919448741545,\n",
       "    'rougeL_max,none': 19.13616485017056,\n",
       "    'rougeL_max_stderr,none': 0.3927296747535081,\n",
       "    'rougeL_acc,none': 0.4589963280293758,\n",
       "    'rougeL_acc_stderr,none': 0.017444544447661206,\n",
       "    'rougeL_diff,none': -0.1641009464888551,\n",
       "    'rougeL_diff_stderr,none': 0.30493446467722446},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.3243574051407589,\n",
       "    'acc_stderr,none': 0.016387976779647942},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5058314086539912,\n",
       "    'acc_stderr,none': 0.016427216245233724}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.32763819095477387,\n",
       "    'acc_stderr,none': 0.008592100906266604,\n",
       "    'acc_norm,none': 0.3333333333333333,\n",
       "    'acc_norm_stderr,none': 0.008629672884641516}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7404787812840044,\n",
       "    'acc_stderr,none': 0.010227939888173925,\n",
       "    'acc_norm,none': 0.7295973884657236,\n",
       "    'acc_norm_stderr,none': 0.010363167031620794}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.32142857142857145,\n",
       "    'acc_stderr,none': 0.022089519157170157,\n",
       "    'acc_norm,none': 0.32142857142857145,\n",
       "    'acc_norm_stderr,none': 0.022089519157170157}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.3923013923013923,\n",
       "    'acc_stderr,none': 0.01397893643494679}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.4177407126611069,\n",
       "    'exact_match_stderr,strict-match': 0.013584820638504832,\n",
       "    'exact_match,flexible-extract': 0.8999241849886277,\n",
       "    'exact_match_stderr,flexible-extract': 0.008266274528685637}}},\n",
       " 'llama3_70b_o_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6437117219769264,\n",
       "    'acc_stderr,none': 0.0038392275190125944,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5751328374070138,\n",
       "    'acc_stderr,none': 0.0069207640537893335,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5952380952380952,\n",
       "    'acc_stderr,none': 0.04390259265377562},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7393939393939394,\n",
       "    'acc_stderr,none': 0.034277431758165236},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8382352941176471,\n",
       "    'acc_stderr,none': 0.025845017986926924},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8270042194092827,\n",
       "    'acc_stderr,none': 0.024621562866768434},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.7603305785123967,\n",
       "    'acc_stderr,none': 0.03896878985070417},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6481481481481481,\n",
       "    'acc_stderr,none': 0.04616631111801714},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7484662576687117,\n",
       "    'acc_stderr,none': 0.034089978868575295},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.615606936416185,\n",
       "    'acc_stderr,none': 0.026189666966272035},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.48379888268156424,\n",
       "    'acc_stderr,none': 0.01671372072950102},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6141479099678456,\n",
       "    'acc_stderr,none': 0.027648149599751464},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7253086419753086,\n",
       "    'acc_stderr,none': 0.024836057868294684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.43415906127770537,\n",
       "    'acc_stderr,none': 0.01265903323706725},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.7017543859649122,\n",
       "    'acc_stderr,none': 0.035087719298245654},\n",
       "   'mmlu_other': {'acc,none': 0.7241712262632765,\n",
       "    'acc_stderr,none': 0.007730161480411441,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.68,\n",
       "    'acc_stderr,none': 0.04688261722621505},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.7584905660377359,\n",
       "    'acc_stderr,none': 0.026341480371118366},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6589595375722543,\n",
       "    'acc_stderr,none': 0.036146654241808254},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.39,\n",
       "    'acc_stderr,none': 0.04902071300001974},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.6143497757847534,\n",
       "    'acc_stderr,none': 0.03266842214289202},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.7281553398058253,\n",
       "    'acc_stderr,none': 0.044052680241409216},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8034188034188035,\n",
       "    'acc_stderr,none': 0.02603538609895129},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.78,\n",
       "    'acc_stderr,none': 0.04163331998932263},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8314176245210728,\n",
       "    'acc_stderr,none': 0.013387895731543602},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.7320261437908496,\n",
       "    'acc_stderr,none': 0.025360603796242557},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5780141843971631,\n",
       "    'acc_stderr,none': 0.029462189233370593},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8529411764705882,\n",
       "    'acc_stderr,none': 0.021513964052859644},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4819277108433735,\n",
       "    'acc_stderr,none': 0.03889951252827216},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.726356841078973,\n",
       "    'acc_stderr,none': 0.007811719553360617,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5877192982456141,\n",
       "    'acc_stderr,none': 0.04630653203366596},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.7929292929292929,\n",
       "    'acc_stderr,none': 0.028869778460267063},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8963730569948186,\n",
       "    'acc_stderr,none': 0.02199531196364424},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.7435897435897436,\n",
       "    'acc_stderr,none': 0.02213908110397154},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.7773109243697479,\n",
       "    'acc_stderr,none': 0.027025433498882385},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.8422018348623853,\n",
       "    'acc_stderr,none': 0.015630022970092444},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.6793893129770993,\n",
       "    'acc_stderr,none': 0.04093329229834278},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.019431775677037313},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5636363636363636,\n",
       "    'acc_stderr,none': 0.04750185058907297},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.5183673469387755,\n",
       "    'acc_stderr,none': 0.03198761546763126},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7860696517412935,\n",
       "    'acc_stderr,none': 0.028996909693328916},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.77,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_stem': {'acc,none': 0.5861084681255947,\n",
       "    'acc_stderr,none': 0.008339493700301721,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6888888888888889,\n",
       "    'acc_stderr,none': 0.03999262876617721},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7368421052631579,\n",
       "    'acc_stderr,none': 0.03583496176361073},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.8888888888888888,\n",
       "    'acc_stderr,none': 0.026280550932848062},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.54,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.04960449637488583},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.41,\n",
       "    'acc_stderr,none': 0.04943110704237102},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.46078431372549017,\n",
       "    'acc_stderr,none': 0.04959859966384181},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.6510638297872341,\n",
       "    'acc_stderr,none': 0.031158522131357783},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5448275862068965,\n",
       "    'acc_stderr,none': 0.04149886942192117},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.5052910052910053,\n",
       "    'acc_stderr,none': 0.02574986828855657},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.8258064516129032,\n",
       "    'acc_stderr,none': 0.021576248184514583},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5320197044334976,\n",
       "    'acc_stderr,none': 0.035107665979592154},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.3148148148148148,\n",
       "    'acc_stderr,none': 0.028317533496066465},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.47019867549668876,\n",
       "    'acc_stderr,none': 0.040752249922169775},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6574074074074074,\n",
       "    'acc_stderr,none': 0.03236585252602156},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5625,\n",
       "    'acc_stderr,none': 0.04708567521880525}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.44472876151484136,\n",
       "    'acc_stderr,none': 0.011244731148193177}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.31919642857142855,\n",
       "    'acc_stderr,none': 0.022048861164576057,\n",
       "    'acc_norm,none': 0.31919642857142855,\n",
       "    'acc_norm_stderr,none': 0.022048861164576057}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 10.74307656454996,\n",
       "    'bleu_max_stderr,none': 0.5921652561837797,\n",
       "    'bleu_acc,none': 0.5862913096695227,\n",
       "    'bleu_acc_stderr,none': 0.0172408618120998,\n",
       "    'bleu_diff,none': 0.24252000819959577,\n",
       "    'bleu_diff_stderr,none': 0.48401667701650236,\n",
       "    'rouge1_max,none': 27.8427589541939,\n",
       "    'rouge1_max_stderr,none': 0.7480600911604414,\n",
       "    'rouge1_acc,none': 0.6878824969400245,\n",
       "    'rouge1_acc_stderr,none': 0.016220756769520943,\n",
       "    'rouge1_diff,none': 5.15742376541629,\n",
       "    'rouge1_diff_stderr,none': 0.63022201241475,\n",
       "    'rouge2_max,none': 11.309317352878294,\n",
       "    'rouge2_max_stderr,none': 0.7623763839260962,\n",
       "    'rouge2_acc,none': 0.189718482252142,\n",
       "    'rouge2_acc_stderr,none': 0.013725485265185093,\n",
       "    'rouge2_diff,none': -0.7548052080430827,\n",
       "    'rouge2_diff_stderr,none': 0.6675079776626445,\n",
       "    'rougeL_max,none': 26.644596383247826,\n",
       "    'rougeL_max_stderr,none': 0.7354914136099051,\n",
       "    'rougeL_acc,none': 0.6829865361077111,\n",
       "    'rougeL_acc_stderr,none': 0.016289203374403365,\n",
       "    'rougeL_diff,none': 5.0320054717198355,\n",
       "    'rougeL_diff_stderr,none': 0.6320796497248932},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.37576499388004897,\n",
       "    'acc_stderr,none': 0.016954584060214294},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5421157034230984,\n",
       "    'acc_stderr,none': 0.016755339076097026}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.34706867671691793,\n",
       "    'acc_stderr,none': 0.00871449153541417,\n",
       "    'acc_norm,none': 0.35510887772194305,\n",
       "    'acc_norm_stderr,none': 0.00876041246957384}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7676822633297062,\n",
       "    'acc_stderr,none': 0.009853201384168241,\n",
       "    'acc_norm,none': 0.7682263329706203,\n",
       "    'acc_norm_stderr,none': 0.009845143772794024}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.021923384489444957,\n",
       "    'acc_norm,none': 0.3125,\n",
       "    'acc_norm_stderr,none': 0.021923384489444957}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.6592956592956593,\n",
       "    'acc_stderr,none': 0.013569036984855006}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8832448824867324,\n",
       "    'exact_match_stderr,strict-match': 0.008845468136919103,\n",
       "    'exact_match,flexible-extract': 0.8847611827141774,\n",
       "    'exact_match_stderr,flexible-extract': 0.00879538230154542}}},\n",
       " 'llama3_70b_n_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.33186155818259505,\n",
       "    'acc_stderr,none': 0.003918672275666112,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.3640807651434644,\n",
       "    'acc_stderr,none': 0.006974009204927656,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.30158730158730157,\n",
       "    'acc_stderr,none': 0.04104947269903394},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3090909090909091,\n",
       "    'acc_stderr,none': 0.036085410115739666},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.3480392156862745,\n",
       "    'acc_stderr,none': 0.03343311240488419},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.33755274261603374,\n",
       "    'acc_stderr,none': 0.03078154910202621},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5619834710743802,\n",
       "    'acc_stderr,none': 0.04529146804435792},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.3425925925925926,\n",
       "    'acc_stderr,none': 0.04587904741301809},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.4171779141104294,\n",
       "    'acc_stderr,none': 0.03874102859818082},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.36127167630057805,\n",
       "    'acc_stderr,none': 0.025862201852277895},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.40670391061452515,\n",
       "    'acc_stderr,none': 0.016428811915898858},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2347266881028939,\n",
       "    'acc_stderr,none': 0.024071805887677048},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.28703703703703703,\n",
       "    'acc_stderr,none': 0.025171041915309684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.37614080834419816,\n",
       "    'acc_stderr,none': 0.012372214430599816},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.39766081871345027,\n",
       "    'acc_stderr,none': 0.03753638955761691},\n",
       "   'mmlu_other': {'acc,none': 0.36401673640167365,\n",
       "    'acc_stderr,none': 0.008541257637852656,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.45,\n",
       "    'acc_stderr,none': 0.05},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.3132075471698113,\n",
       "    'acc_stderr,none': 0.028544793319055333},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.26011560693641617,\n",
       "    'acc_stderr,none': 0.03345036916788991},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.4349775784753363,\n",
       "    'acc_stderr,none': 0.033272833702713445},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.46601941747572817,\n",
       "    'acc_stderr,none': 0.04939291447273481},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.3974358974358974,\n",
       "    'acc_stderr,none': 0.03205953453789293},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.38058748403575987,\n",
       "    'acc_stderr,none': 0.01736256412607542},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.27124183006535946,\n",
       "    'acc_stderr,none': 0.02545775669666788},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.31560283687943264,\n",
       "    'acc_stderr,none': 0.027724989449509314},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.5073529411764706,\n",
       "    'acc_stderr,none': 0.030369552523902173},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.3373493975903614,\n",
       "    'acc_stderr,none': 0.03680783690727581},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.34644133896652585,\n",
       "    'acc_stderr,none': 0.008458706447299158,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2719298245614035,\n",
       "    'acc_stderr,none': 0.04185774424022056},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.2474747474747475,\n",
       "    'acc_stderr,none': 0.03074630074212449},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.3005181347150259,\n",
       "    'acc_stderr,none': 0.03308818594415751},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2358974358974359,\n",
       "    'acc_stderr,none': 0.02152596540740872},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.25630252100840334,\n",
       "    'acc_stderr,none': 0.028359620870533946},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3431192660550459,\n",
       "    'acc_stderr,none': 0.02035477773608604},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.366412213740458,\n",
       "    'acc_stderr,none': 0.04225875451969639},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.46078431372549017,\n",
       "    'acc_stderr,none': 0.020165523313907908},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.33636363636363636,\n",
       "    'acc_stderr,none': 0.04525393596302507},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.3346938775510204,\n",
       "    'acc_stderr,none': 0.030209235226242307},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.44776119402985076,\n",
       "    'acc_stderr,none': 0.03516184772952167},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.49,\n",
       "    'acc_stderr,none': 0.05024183937956911},\n",
       "   'mmlu_stem': {'acc,none': 0.23786869647954328,\n",
       "    'acc_stderr,none': 0.007566815679698311,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2518518518518518,\n",
       "    'acc_stderr,none': 0.037498507091740206},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.21710526315789475,\n",
       "    'acc_stderr,none': 0.033550453048829226},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.3263888888888889,\n",
       "    'acc_stderr,none': 0.03921067198982266},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.0440844002276808},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.041633319989322695},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.22549019607843138,\n",
       "    'acc_stderr,none': 0.04158307533083286},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.29,\n",
       "    'acc_stderr,none': 0.045604802157206845},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.2978723404255319,\n",
       "    'acc_stderr,none': 0.02989614568209546},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2413793103448276,\n",
       "    'acc_stderr,none': 0.03565998174135302},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.21164021164021163,\n",
       "    'acc_stderr,none': 0.021037331505262886},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.22258064516129034,\n",
       "    'acc_stderr,none': 0.02366421667164251},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.1625615763546798,\n",
       "    'acc_stderr,none': 0.02596030006460558},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.21481481481481482,\n",
       "    'acc_stderr,none': 0.025040443877000683},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2052980132450331,\n",
       "    'acc_stderr,none': 0.032979866484738336},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.22685185185185186,\n",
       "    'acc_stderr,none': 0.028561650102422273},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.32142857142857145,\n",
       "    'acc_stderr,none': 0.04432804055291519}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.39969293756397134,\n",
       "    'acc_stderr,none': 0.011084059334882369}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.32589285714285715,\n",
       "    'acc_stderr,none': 0.022169103134643414,\n",
       "    'acc_norm,none': 0.32589285714285715,\n",
       "    'acc_norm_stderr,none': 0.022169103134643414}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.2298937294465833,\n",
       "    'bleu_max_stderr,none': 0.09354138993283234,\n",
       "    'bleu_acc,none': 0.5055079559363526,\n",
       "    'bleu_acc_stderr,none': 0.01750243899045107,\n",
       "    'bleu_diff,none': 0.05336486584963402,\n",
       "    'bleu_diff_stderr,none': 0.0653605864556049,\n",
       "    'rouge1_max,none': 14.747897170248635,\n",
       "    'rouge1_max_stderr,none': 0.28198385466588327,\n",
       "    'rouge1_acc,none': 0.6132190942472461,\n",
       "    'rouge1_acc_stderr,none': 0.017048857010515103,\n",
       "    'rouge1_diff,none': 1.46673252321499,\n",
       "    'rouge1_diff_stderr,none': 0.2309600728838094,\n",
       "    'rouge2_max,none': 5.438542820105168,\n",
       "    'rouge2_max_stderr,none': 0.2732934519168711,\n",
       "    'rouge2_acc,none': 0.2582619339045288,\n",
       "    'rouge2_acc_stderr,none': 0.015321821688476185,\n",
       "    'rouge2_diff,none': -0.2945946516402812,\n",
       "    'rouge2_diff_stderr,none': 0.2012302851801536,\n",
       "    'rougeL_max,none': 12.678891120639284,\n",
       "    'rougeL_max_stderr,none': 0.27607582716498286,\n",
       "    'rougeL_acc,none': 0.5789473684210527,\n",
       "    'rougeL_acc_stderr,none': 0.017283936248136473,\n",
       "    'rougeL_diff,none': 0.6985162105465053,\n",
       "    'rougeL_diff_stderr,none': 0.21694684578869217},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.2582619339045288,\n",
       "    'acc_stderr,none': 0.015321821688476187},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.4304264387243563,\n",
       "    'acc_stderr,none': 0.016580910277876004}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2887772194304858,\n",
       "    'acc_stderr,none': 0.008296308349383155,\n",
       "    'acc_norm,none': 0.30083752093802346,\n",
       "    'acc_norm_stderr,none': 0.00839567556992465}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7285092491838956,\n",
       "    'acc_stderr,none': 0.010376251176596135,\n",
       "    'acc_norm,none': 0.7225244831338411,\n",
       "    'acc_norm_stderr,none': 0.010446818281039955}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.36607142857142855,\n",
       "    'acc_stderr,none': 0.02278501498199051,\n",
       "    'acc_norm,none': 0.36607142857142855,\n",
       "    'acc_norm_stderr,none': 0.02278501498199051}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.20065520065520065,\n",
       "    'acc_stderr,none': 0.011466011466011533}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.133434420015163,\n",
       "    'exact_match_stderr,strict-match': 0.009366491609784465,\n",
       "    'exact_match,flexible-extract': 0.1516300227445034,\n",
       "    'exact_match_stderr,flexible-extract': 0.009879331091354297}}},\n",
       " 'llama3_70b_o_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.5789061387266771,\n",
       "    'acc_stderr,none': 0.004013950469431933,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5846971307120085,\n",
       "    'acc_stderr,none': 0.006921062283228587,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.30158730158730157,\n",
       "    'acc_stderr,none': 0.04104947269903394},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7393939393939394,\n",
       "    'acc_stderr,none': 0.034277431758165236},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8186274509803921,\n",
       "    'acc_stderr,none': 0.02704462171947407},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8523206751054853,\n",
       "    'acc_stderr,none': 0.023094329582595694},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.71900826446281,\n",
       "    'acc_stderr,none': 0.04103203830514512},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.6481481481481481,\n",
       "    'acc_stderr,none': 0.046166311118017125},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.5950920245398773,\n",
       "    'acc_stderr,none': 0.038566721635489125},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.6098265895953757,\n",
       "    'acc_stderr,none': 0.026261677607806642},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.40782122905027934,\n",
       "    'acc_stderr,none': 0.016435865260914742},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.5980707395498392,\n",
       "    'acc_stderr,none': 0.02784647600593048},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7129629629629629,\n",
       "    'acc_stderr,none': 0.025171041915309684},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5514993481095176,\n",
       "    'acc_stderr,none': 0.012702317490559813},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.7543859649122807,\n",
       "    'acc_stderr,none': 0.03301405946987249},\n",
       "   'mmlu_other': {'acc,none': 0.6099130994528484,\n",
       "    'acc_stderr,none': 0.008597656816445472,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.64,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.5849056603773585,\n",
       "    'acc_stderr,none': 0.030325945789286105},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.5606936416184971,\n",
       "    'acc_stderr,none': 0.03784271932887467},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.6053811659192825,\n",
       "    'acc_stderr,none': 0.03280400504755291},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.7184466019417476,\n",
       "    'acc_stderr,none': 0.04453254836326466},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.717948717948718,\n",
       "    'acc_stderr,none': 0.02948036054954119},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.6283524904214559,\n",
       "    'acc_stderr,none': 0.01728080252213318},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.5849673202614379,\n",
       "    'acc_stderr,none': 0.028213504177824096},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.4858156028368794,\n",
       "    'acc_stderr,none': 0.02981549448368206},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.7830882352941176,\n",
       "    'acc_stderr,none': 0.025035845227711264},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4457831325301205,\n",
       "    'acc_stderr,none': 0.03869543323472101},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.6509587260318492,\n",
       "    'acc_stderr,none': 0.008417402046232824,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.4649122807017544,\n",
       "    'acc_stderr,none': 0.04692008381368909},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.5656565656565656,\n",
       "    'acc_stderr,none': 0.035315058793591834},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.8031088082901554,\n",
       "    'acc_stderr,none': 0.02869787397186069},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.02535100632816969},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.5588235294117647,\n",
       "    'acc_stderr,none': 0.032252942323996406},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.7467889908256881,\n",
       "    'acc_stderr,none': 0.018644073041375046},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.5648854961832062,\n",
       "    'acc_stderr,none': 0.043482080516448585},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6666666666666666,\n",
       "    'acc_stderr,none': 0.0190709855896875},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.6090909090909091,\n",
       "    'acc_stderr,none': 0.04673752333670237},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.7510204081632653,\n",
       "    'acc_stderr,none': 0.027682979522960234},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.7313432835820896,\n",
       "    'acc_stderr,none': 0.03134328358208954},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.68,\n",
       "    'acc_stderr,none': 0.04688261722621504},\n",
       "   'mmlu_stem': {'acc,none': 0.4693942277196321,\n",
       "    'acc_stderr,none': 0.008581697015889985,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.5777777777777777,\n",
       "    'acc_stderr,none': 0.04266763404099582},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.6578947368421053,\n",
       "    'acc_stderr,none': 0.038607315993160904},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.04016660030451233},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.52,\n",
       "    'acc_stderr,none': 0.050211673156867795},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.35294117647058826,\n",
       "    'acc_stderr,none': 0.04755129616062946},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.61,\n",
       "    'acc_stderr,none': 0.04902071300001975},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.42127659574468085,\n",
       "    'acc_stderr,none': 0.03227834510146268},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.5103448275862069,\n",
       "    'acc_stderr,none': 0.04165774775728763},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.3148148148148148,\n",
       "    'acc_stderr,none': 0.023919984164047732},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.6387096774193548,\n",
       "    'acc_stderr,none': 0.027327548447957536},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.4827586206896552,\n",
       "    'acc_stderr,none': 0.035158955511657},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.7,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.27037037037037037,\n",
       "    'acc_stderr,none': 0.027080372815145668},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.4105960264900662,\n",
       "    'acc_stderr,none': 0.04016689594849928},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.5555555555555556,\n",
       "    'acc_stderr,none': 0.03388857118502326},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.04697113923010212}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.41453428863868985,\n",
       "    'acc_stderr,none': 0.01114756056703673}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.36830357142857145,\n",
       "    'acc_stderr,none': 0.02281410385929623,\n",
       "    'acc_norm,none': 0.36830357142857145,\n",
       "    'acc_norm_stderr,none': 0.02281410385929623}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 4.67000729002748,\n",
       "    'bleu_max_stderr,none': 0.1962790613418521,\n",
       "    'bleu_acc,none': 0.4638922888616891,\n",
       "    'bleu_acc_stderr,none': 0.017457800422268622,\n",
       "    'bleu_diff,none': -0.1407108676112176,\n",
       "    'bleu_diff_stderr,none': 0.12336817419524353,\n",
       "    'rouge1_max,none': 19.70330874159952,\n",
       "    'rouge1_max_stderr,none': 0.37179639804216896,\n",
       "    'rouge1_acc,none': 0.4834761321909425,\n",
       "    'rouge1_acc_stderr,none': 0.01749394019005773,\n",
       "    'rouge1_diff,none': -0.05319481147972795,\n",
       "    'rouge1_diff_stderr,none': 0.26610073609819535,\n",
       "    'rouge2_max,none': 9.966995731885408,\n",
       "    'rouge2_max_stderr,none': 0.35204272293306876,\n",
       "    'rouge2_acc,none': 0.37576499388004897,\n",
       "    'rouge2_acc_stderr,none': 0.016954584060214287,\n",
       "    'rouge2_diff,none': -0.558407042794807,\n",
       "    'rouge2_diff_stderr,none': 0.24537958685086045,\n",
       "    'rougeL_max,none': 16.898524866575997,\n",
       "    'rougeL_max_stderr,none': 0.36011815342509795,\n",
       "    'rougeL_acc,none': 0.4663402692778458,\n",
       "    'rougeL_acc_stderr,none': 0.017463793867168096,\n",
       "    'rougeL_diff,none': -0.4041928389312531,\n",
       "    'rougeL_diff_stderr,none': 0.24905407003563299},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.38922888616891066,\n",
       "    'acc_stderr,none': 0.017068552680690335},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5463692530636998,\n",
       "    'acc_stderr,none': 0.01648277570651695}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.33936348408710215,\n",
       "    'acc_stderr,none': 0.008667910793954913,\n",
       "    'acc_norm,none': 0.33936348408710215,\n",
       "    'acc_norm_stderr,none': 0.008667910793954915}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.764417845484222,\n",
       "    'acc_stderr,none': 0.009901067586473904,\n",
       "    'acc_norm,none': 0.7562568008705114,\n",
       "    'acc_norm_stderr,none': 0.010017199471500614}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.375,\n",
       "    'acc_stderr,none': 0.02289822829522849,\n",
       "    'acc_norm,none': 0.375,\n",
       "    'acc_norm_stderr,none': 0.02289822829522849}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.5765765765765766,\n",
       "    'acc_stderr,none': 0.014146077134489551}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.5041698256254739,\n",
       "    'exact_match_stderr,strict-match': 0.013772005774791549,\n",
       "    'exact_match,flexible-extract': 0.8786959818043972,\n",
       "    'exact_match_stderr,flexible-extract': 0.00899288849727559}}},\n",
       " 'llama3_70b_c_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.5032046716991881,\n",
       "    'acc_stderr,none': 0.004034416971227507,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.5426142401700319,\n",
       "    'acc_stderr,none': 0.007029245892411088,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.3333333333333333,\n",
       "    'acc_stderr,none': 0.04216370213557835},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7757575757575758,\n",
       "    'acc_stderr,none': 0.032568666616811015},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8284313725490197,\n",
       "    'acc_stderr,none': 0.026460569561240658},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.7890295358649789,\n",
       "    'acc_stderr,none': 0.02655837250266192},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.6611570247933884,\n",
       "    'acc_stderr,none': 0.0432076780753667},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.5092592592592593,\n",
       "    'acc_stderr,none': 0.04832853553437056},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.4171779141104294,\n",
       "    'acc_stderr,none': 0.03874102859818083},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.5173410404624278,\n",
       "    'acc_stderr,none': 0.026902900458666647},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.5519553072625698,\n",
       "    'acc_stderr,none': 0.016631976628930595},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3633440514469453,\n",
       "    'acc_stderr,none': 0.02731684767419271},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.36728395061728397,\n",
       "    'acc_stderr,none': 0.026822801759507894},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.559973924380704,\n",
       "    'acc_stderr,none': 0.012678037478574513},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.3508771929824561,\n",
       "    'acc_stderr,none': 0.036602988340491645},\n",
       "   'mmlu_other': {'acc,none': 0.5168973286128098,\n",
       "    'acc_stderr,none': 0.008628448460602678,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.62,\n",
       "    'acc_stderr,none': 0.048783173121456316},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.5433962264150943,\n",
       "    'acc_stderr,none': 0.03065674869673943},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3930635838150289,\n",
       "    'acc_stderr,none': 0.03724249595817731},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5605381165919282,\n",
       "    'acc_stderr,none': 0.033310925110381785},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.5728155339805825,\n",
       "    'acc_stderr,none': 0.04897957737781168},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.6410256410256411,\n",
       "    'acc_stderr,none': 0.03142616993791925},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.4393358876117497,\n",
       "    'acc_stderr,none': 0.017747874245683606},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.42483660130718953,\n",
       "    'acc_stderr,none': 0.028304576673141114},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5212765957446809,\n",
       "    'acc_stderr,none': 0.029800481645628693},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8566176470588235,\n",
       "    'acc_stderr,none': 0.021289071205445115},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.4036144578313253,\n",
       "    'acc_stderr,none': 0.03819486140758398},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.5713357166070848,\n",
       "    'acc_stderr,none': 0.008731473242231808,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.39473684210526316,\n",
       "    'acc_stderr,none': 0.04598188057816542},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.36363636363636365,\n",
       "    'acc_stderr,none': 0.03427308652999933},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.7357512953367875,\n",
       "    'acc_stderr,none': 0.031821550509166484},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.5538461538461539,\n",
       "    'acc_stderr,none': 0.02520357177302833},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.592436974789916,\n",
       "    'acc_stderr,none': 0.03191863374478465},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.6165137614678899,\n",
       "    'acc_stderr,none': 0.020847156641915984},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.35877862595419846,\n",
       "    'acc_stderr,none': 0.04206739313864908},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.6699346405228758,\n",
       "    'acc_stderr,none': 0.019023726160724553},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.5181818181818182,\n",
       "    'acc_stderr,none': 0.04785964010794915},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.45714285714285713,\n",
       "    'acc_stderr,none': 0.031891418324213966},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.6218905472636815,\n",
       "    'acc_stderr,none': 0.03428867848778658},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.55,\n",
       "    'acc_stderr,none': 0.04999999999999999},\n",
       "   'mmlu_stem': {'acc,none': 0.3644148430066603,\n",
       "    'acc_stderr,none': 0.008239600592247081,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.37777777777777777,\n",
       "    'acc_stderr,none': 0.04188307537595853},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.3026315789473684,\n",
       "    'acc_stderr,none': 0.03738520676119667},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.6180555555555556,\n",
       "    'acc_stderr,none': 0.040629907841466674},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.27,\n",
       "    'acc_stderr,none': 0.0446196043338474},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.04923659639173309},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.28431372549019607,\n",
       "    'acc_stderr,none': 0.04488482852329017},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.46,\n",
       "    'acc_stderr,none': 0.05009082659620333},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3404255319148936,\n",
       "    'acc_stderr,none': 0.030976692998534443},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2620689655172414,\n",
       "    'acc_stderr,none': 0.036646663372252565},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2328042328042328,\n",
       "    'acc_stderr,none': 0.021765961672154527},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.6193548387096774,\n",
       "    'acc_stderr,none': 0.02762171783290703},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3103448275862069,\n",
       "    'acc_stderr,none': 0.032550867699701024},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.58,\n",
       "    'acc_stderr,none': 0.049604496374885836},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.33112582781456956,\n",
       "    'acc_stderr,none': 0.038425817186598696},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.4722222222222222,\n",
       "    'acc_stderr,none': 0.0340470532865388},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.04697113923010212}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4467758444216991,\n",
       "    'acc_stderr,none': 0.011249786691110377}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 10.747043619063046,\n",
       "    'bleu_max_stderr,none': 0.5199191608239963,\n",
       "    'bleu_acc,none': 0.5042839657282742,\n",
       "    'bleu_acc_stderr,none': 0.017502858577371286,\n",
       "    'bleu_diff,none': 0.24446069927928393,\n",
       "    'bleu_diff_stderr,none': 0.3481118389444562,\n",
       "    'rouge1_max,none': 30.089215834717162,\n",
       "    'rouge1_max_stderr,none': 0.6749020097244687,\n",
       "    'rouge1_acc,none': 0.5422276621787026,\n",
       "    'rouge1_acc_stderr,none': 0.01744096571248212,\n",
       "    'rouge1_diff,none': 0.4449290434158352,\n",
       "    'rouge1_diff_stderr,none': 0.45105836045473124,\n",
       "    'rouge2_max,none': 18.14369102863998,\n",
       "    'rouge2_max_stderr,none': 0.6739075415992781,\n",
       "    'rouge2_acc,none': 0.4479804161566707,\n",
       "    'rouge2_acc_stderr,none': 0.01740851306342292,\n",
       "    'rouge2_diff,none': -0.36873924898803656,\n",
       "    'rouge2_diff_stderr,none': 0.5307793586469038,\n",
       "    'rougeL_max,none': 26.833469030863085,\n",
       "    'rougeL_max_stderr,none': 0.6680367832530753,\n",
       "    'rougeL_acc,none': 0.5042839657282742,\n",
       "    'rougeL_acc_stderr,none': 0.01750285857737129,\n",
       "    'rougeL_diff,none': -0.12239718966868346,\n",
       "    'rougeL_diff_stderr,none': 0.4502479548438197},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4908200734394125,\n",
       "    'acc_stderr,none': 0.017500550724819753},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6464106843287672,\n",
       "    'acc_stderr,none': 0.016135148130813463}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.32897822445561137,\n",
       "    'acc_stderr,none': 0.008601069831238388,\n",
       "    'acc_norm,none': 0.3340033500837521,\n",
       "    'acc_norm_stderr,none': 0.008633999602399836}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.79379760609358,\n",
       "    'acc_stderr,none': 0.009439460331609507,\n",
       "    'acc_norm,none': 0.7818280739934712,\n",
       "    'acc_norm_stderr,none': 0.009636081958374381}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.2375102375102375,\n",
       "    'acc_stderr,none': 0.012183673723473452}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8559514783927218,\n",
       "    'exact_match_stderr,strict-match': 0.009672110973065291,\n",
       "    'exact_match,flexible-extract': 0.9021986353297953,\n",
       "    'exact_match_stderr,flexible-extract': 0.008182119821849056}}},\n",
       " 'llama3_70b_n_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.6913545079048569,\n",
       "    'acc_stderr,none': 0.003678671513586357,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.6420828905419766,\n",
       "    'acc_stderr,none': 0.0067667038504256915,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5238095238095238,\n",
       "    'acc_stderr,none': 0.04467062628403273},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.8121212121212121,\n",
       "    'acc_stderr,none': 0.03050193405942914},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8627450980392157,\n",
       "    'acc_stderr,none': 0.024152225962801584},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8649789029535865,\n",
       "    'acc_stderr,none': 0.022245776632003694},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.8760330578512396,\n",
       "    'acc_stderr,none': 0.03008309871603521},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.7037037037037037,\n",
       "    'acc_stderr,none': 0.04414343666854933},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7239263803680982,\n",
       "    'acc_stderr,none': 0.03512385283705049},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.7052023121387283,\n",
       "    'acc_stderr,none': 0.02454761779480384},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.49162011173184356,\n",
       "    'acc_stderr,none': 0.016720152794672552},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.6784565916398714,\n",
       "    'acc_stderr,none': 0.026527724079528872},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.7623456790123457,\n",
       "    'acc_stderr,none': 0.023683591837008557},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5743155149934811,\n",
       "    'acc_stderr,none': 0.012628393551811942},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.6842105263157895,\n",
       "    'acc_stderr,none': 0.035650796707083106},\n",
       "   'mmlu_other': {'acc,none': 0.770196330865787,\n",
       "    'acc_stderr,none': 0.0072177138104941325,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.73,\n",
       "    'acc_stderr,none': 0.044619604333847394},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.7584905660377359,\n",
       "    'acc_stderr,none': 0.026341480371118373},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.6994219653179191,\n",
       "    'acc_stderr,none': 0.03496101481191181},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.7085201793721974,\n",
       "    'acc_stderr,none': 0.030500283176545843},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8252427184466019,\n",
       "    'acc_stderr,none': 0.0376017800602662},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8632478632478633,\n",
       "    'acc_stderr,none': 0.022509033937077805},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036623},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8786717752234994,\n",
       "    'acc_stderr,none': 0.011675913883906732},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.8366013071895425,\n",
       "    'acc_stderr,none': 0.021170623011213523},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.574468085106383,\n",
       "    'acc_stderr,none': 0.029494827600144376},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8639705882352942,\n",
       "    'acc_stderr,none': 0.020824819397794334},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.5240963855421686,\n",
       "    'acc_stderr,none': 0.03887971849597264},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.8033799155021124,\n",
       "    'acc_stderr,none': 0.0070667838596585745,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.5964912280701754,\n",
       "    'acc_stderr,none': 0.04615186962583707},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.8080808080808081,\n",
       "    'acc_stderr,none': 0.02805779167298901},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.9585492227979274,\n",
       "    'acc_stderr,none': 0.01438543285747644},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.764102564102564,\n",
       "    'acc_stderr,none': 0.021525965407408726},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.8403361344537815,\n",
       "    'acc_stderr,none': 0.0237933539975288},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.8348623853211009,\n",
       "    'acc_stderr,none': 0.01591955782997606},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.732824427480916,\n",
       "    'acc_stderr,none': 0.03880848301082396},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.803921568627451,\n",
       "    'acc_stderr,none': 0.01606205642196865},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.6909090909090909,\n",
       "    'acc_stderr,none': 0.044262946482000985},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.7428571428571429,\n",
       "    'acc_stderr,none': 0.027979823538744546},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.8706467661691543,\n",
       "    'acc_stderr,none': 0.023729830881018522},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.85,\n",
       "    'acc_stderr,none': 0.03588702812826371},\n",
       "   'mmlu_stem': {'acc,none': 0.5778623533143038,\n",
       "    'acc_stderr,none': 0.008264404544180733,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.7111111111111111,\n",
       "    'acc_stderr,none': 0.0391545063041425},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.7236842105263158,\n",
       "    'acc_stderr,none': 0.03639057569952929},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.875,\n",
       "    'acc_stderr,none': 0.02765610492929436},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.44,\n",
       "    'acc_stderr,none': 0.04988876515698589},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.63,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.04688261722621504},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.4411764705882353,\n",
       "    'acc_stderr,none': 0.049406356306056595},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.66,\n",
       "    'acc_stderr,none': 0.04760952285695237},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.5872340425531914,\n",
       "    'acc_stderr,none': 0.0321847114140035},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.6344827586206897,\n",
       "    'acc_stderr,none': 0.04013124195424387},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.43386243386243384,\n",
       "    'acc_stderr,none': 0.025525034382474887},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.8387096774193549,\n",
       "    'acc_stderr,none': 0.02092332700642329},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.5566502463054187,\n",
       "    'acc_stderr,none': 0.03495334582162934},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.82,\n",
       "    'acc_stderr,none': 0.038612291966536934},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.3037037037037037,\n",
       "    'acc_stderr,none': 0.028037929969114982},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.48344370860927155,\n",
       "    'acc_stderr,none': 0.0408024418562897},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6388888888888888,\n",
       "    'acc_stderr,none': 0.032757734861009996},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5892857142857143,\n",
       "    'acc_stderr,none': 0.04669510663875191}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.4534288638689867,\n",
       "    'acc_stderr,none': 0.011264886135301386}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.34598214285714285,\n",
       "    'acc_stderr,none': 0.02249924183068251,\n",
       "    'acc_norm,none': 0.34598214285714285,\n",
       "    'acc_norm_stderr,none': 0.02249924183068251}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 14.391834509758315,\n",
       "    'bleu_max_stderr,none': 0.6460356453403352,\n",
       "    'bleu_acc,none': 0.4541003671970624,\n",
       "    'bleu_acc_stderr,none': 0.01742959309132351,\n",
       "    'bleu_diff,none': -0.572821950610948,\n",
       "    'bleu_diff_stderr,none': 0.4662062712575406,\n",
       "    'rouge1_max,none': 36.49543368032671,\n",
       "    'rouge1_max_stderr,none': 0.7701311386965441,\n",
       "    'rouge1_acc,none': 0.4981640146878825,\n",
       "    'rouge1_acc_stderr,none': 0.017503383046877062,\n",
       "    'rouge1_diff,none': -0.4320810251374563,\n",
       "    'rouge1_diff_stderr,none': 0.5692820076955282,\n",
       "    'rouge2_max,none': 22.047232883471843,\n",
       "    'rouge2_max_stderr,none': 0.8101256889394897,\n",
       "    'rouge2_acc,none': 0.386780905752754,\n",
       "    'rouge2_acc_stderr,none': 0.017048857010515103,\n",
       "    'rouge2_diff,none': -1.1788918258930416,\n",
       "    'rouge2_diff_stderr,none': 0.6271051628970475,\n",
       "    'rougeL_max,none': 32.94633994325146,\n",
       "    'rougeL_max_stderr,none': 0.7785988351508459,\n",
       "    'rougeL_acc,none': 0.4541003671970624,\n",
       "    'rougeL_acc_stderr,none': 0.017429593091323504,\n",
       "    'rougeL_diff,none': -0.9921608559160059,\n",
       "    'rougeL_diff_stderr,none': 0.5684580493977559},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.5079559363525091,\n",
       "    'acc_stderr,none': 0.01750128507455183},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6582290610101554,\n",
       "    'acc_stderr,none': 0.015965345604524485}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.34003350083752093,\n",
       "    'acc_stderr,none': 0.008672062303343067,\n",
       "    'acc_norm,none': 0.3443886097152429,\n",
       "    'acc_norm_stderr,none': 0.008698577262131604}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.795429815016322,\n",
       "    'acc_stderr,none': 0.009411688039193572,\n",
       "    'acc_norm,none': 0.7970620239390642,\n",
       "    'acc_norm_stderr,none': 0.009383679003767331}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.35714285714285715,\n",
       "    'acc_stderr,none': 0.02266336846322688,\n",
       "    'acc_norm,none': 0.35714285714285715,\n",
       "    'acc_norm_stderr,none': 0.02266336846322688}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.44553644553644556,\n",
       "    'acc_stderr,none': 0.014229780629024427}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8847611827141774,\n",
       "    'exact_match_stderr,strict-match': 0.008795382301545425,\n",
       "    'exact_match,flexible-extract': 0.9097801364670205,\n",
       "    'exact_match_stderr,flexible-extract': 0.007891537108449958}}},\n",
       " 'llama3_70b_c_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.3378436120210796,\n",
       "    'acc_stderr,none': 0.0039556005989972425,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.31604675876726884,\n",
       "    'acc_stderr,none': 0.006770679371936339,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.36507936507936506,\n",
       "    'acc_stderr,none': 0.04306241259127153},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3151515151515151,\n",
       "    'acc_stderr,none': 0.0362773057502241},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.30392156862745096,\n",
       "    'acc_stderr,none': 0.03228210387037894},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.25316455696202533,\n",
       "    'acc_stderr,none': 0.028304657943035313},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.32231404958677684,\n",
       "    'acc_stderr,none': 0.042664163633521664},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.23148148148148148,\n",
       "    'acc_stderr,none': 0.04077494709252626},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.3374233128834356,\n",
       "    'acc_stderr,none': 0.03714908409935574},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.30057803468208094,\n",
       "    'acc_stderr,none': 0.0246853168672578},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.32513966480446926,\n",
       "    'acc_stderr,none': 0.015666542785053555},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.2765273311897106,\n",
       "    'acc_stderr,none': 0.025403832978179608},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.30246913580246915,\n",
       "    'acc_stderr,none': 0.025557653981868038},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.34419817470664926,\n",
       "    'acc_stderr,none': 0.012134433741002568},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.23976608187134502,\n",
       "    'acc_stderr,none': 0.03274485211946956},\n",
       "   'mmlu_other': {'acc,none': 0.3495333118764081,\n",
       "    'acc_stderr,none': 0.0084435996432762,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.38113207547169814,\n",
       "    'acc_stderr,none': 0.029890609686286648},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.37572254335260113,\n",
       "    'acc_stderr,none': 0.03692820767264867},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.18,\n",
       "    'acc_stderr,none': 0.038612291966536955},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.19730941704035873,\n",
       "    'acc_stderr,none': 0.02670985334496796},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.4854368932038835,\n",
       "    'acc_stderr,none': 0.04948637324026637},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.28205128205128205,\n",
       "    'acc_stderr,none': 0.029480360549541194},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.3997445721583653,\n",
       "    'acc_stderr,none': 0.01751684790705328},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.027530078447110314},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.3404255319148936,\n",
       "    'acc_stderr,none': 0.028267657482650158},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.47058823529411764,\n",
       "    'acc_stderr,none': 0.03032024326500413},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.22289156626506024,\n",
       "    'acc_stderr,none': 0.03240004825594688},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.376665583360416,\n",
       "    'acc_stderr,none': 0.008665673863123252,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2894736842105263,\n",
       "    'acc_stderr,none': 0.04266339443159394},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.4494949494949495,\n",
       "    'acc_stderr,none': 0.0354413249194797},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.44559585492227977,\n",
       "    'acc_stderr,none': 0.035870149860756595},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.4256410256410256,\n",
       "    'acc_stderr,none': 0.025069094387296535},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.40756302521008403,\n",
       "    'acc_stderr,none': 0.03191863374478466},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3981651376146789,\n",
       "    'acc_stderr,none': 0.020987989422654257},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.366412213740458,\n",
       "    'acc_stderr,none': 0.04225875451969637},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.27941176470588236,\n",
       "    'acc_stderr,none': 0.018152871051538802},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.2727272727272727,\n",
       "    'acc_stderr,none': 0.04265792110940589},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.4816326530612245,\n",
       "    'acc_stderr,none': 0.031987615467631264},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.34328358208955223,\n",
       "    'acc_stderr,none': 0.03357379665433432},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.0479372485441102},\n",
       "   'mmlu_stem': {'acc,none': 0.3209641611163971,\n",
       "    'acc_stderr,none': 0.008216452065328,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.042295258468165065},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.2814814814814815,\n",
       "    'acc_stderr,none': 0.03885004245800254},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.3881578947368421,\n",
       "    'acc_stderr,none': 0.03965842097512744},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.3680555555555556,\n",
       "    'acc_stderr,none': 0.040329990539607195},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.43,\n",
       "    'acc_stderr,none': 0.04975698519562429},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.35,\n",
       "    'acc_stderr,none': 0.047937248544110196},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.37254901960784315,\n",
       "    'acc_stderr,none': 0.04810840148082634},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.041633319989322695},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.25957446808510637,\n",
       "    'acc_stderr,none': 0.02865917937429232},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.2689655172413793,\n",
       "    'acc_stderr,none': 0.036951833116502325},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.2830687830687831,\n",
       "    'acc_stderr,none': 0.023201392938194978},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.38387096774193546,\n",
       "    'acc_stderr,none': 0.02766618207553964},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3448275862068966,\n",
       "    'acc_stderr,none': 0.033442837442804574},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.23,\n",
       "    'acc_stderr,none': 0.04229525846816506},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.26296296296296295,\n",
       "    'acc_stderr,none': 0.026842057873833706},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.33774834437086093,\n",
       "    'acc_stderr,none': 0.038615575462551684},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.5092592592592593,\n",
       "    'acc_stderr,none': 0.034093869469927006},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.16071428571428573,\n",
       "    'acc_stderr,none': 0.0348594609647574}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.37615148413510746,\n",
       "    'acc_stderr,none': 0.010961496293030143}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.30580357142857145,\n",
       "    'acc_stderr,none': 0.021792582688756987,\n",
       "    'acc_norm,none': 0.30580357142857145,\n",
       "    'acc_norm_stderr,none': 0.021792582688756987}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 2.7916817351557293,\n",
       "    'bleu_max_stderr,none': 0.09303642986853196,\n",
       "    'bleu_acc,none': 0.3769889840881273,\n",
       "    'bleu_acc_stderr,none': 0.016965517578930354,\n",
       "    'bleu_diff,none': -0.10204743276007273,\n",
       "    'bleu_diff_stderr,none': 0.08631342933660471,\n",
       "    'rouge1_max,none': 16.27022249316565,\n",
       "    'rouge1_max_stderr,none': 0.3190698819538172,\n",
       "    'rouge1_acc,none': 0.5042839657282742,\n",
       "    'rouge1_acc_stderr,none': 0.017502858577371258,\n",
       "    'rouge1_diff,none': 0.1959855129467676,\n",
       "    'rouge1_diff_stderr,none': 0.349134914936304,\n",
       "    'rouge2_max,none': 3.795027155852737,\n",
       "    'rouge2_max_stderr,none': 0.25399037472627883,\n",
       "    'rouge2_acc,none': 0.16279069767441862,\n",
       "    'rouge2_acc_stderr,none': 0.012923696051772262,\n",
       "    'rouge2_diff,none': -1.1536616886079518,\n",
       "    'rouge2_diff_stderr,none': 0.23605557102171354,\n",
       "    'rougeL_max,none': 14.601096874637628,\n",
       "    'rougeL_max_stderr,none': 0.2984021812954505,\n",
       "    'rougeL_acc,none': 0.5030599755201959,\n",
       "    'rougeL_acc_stderr,none': 0.017503173260960625,\n",
       "    'rougeL_diff,none': 0.0470584437589079,\n",
       "    'rougeL_diff_stderr,none': 0.33394751537595097},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.22031823745410037,\n",
       "    'acc_stderr,none': 0.014509045171487286},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.38507280756049417,\n",
       "    'acc_stderr,none': 0.015975820931627572}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.2814070351758794,\n",
       "    'acc_stderr,none': 0.008232079320325311,\n",
       "    'acc_norm,none': 0.28743718592964823,\n",
       "    'acc_norm_stderr,none': 0.008284830813404313}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7094668117519043,\n",
       "    'acc_stderr,none': 0.010592765034696538,\n",
       "    'acc_norm,none': 0.6958650707290533,\n",
       "    'acc_norm_stderr,none': 0.010733493335721314}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3125,\n",
       "    'acc_stderr,none': 0.021923384489444957,\n",
       "    'acc_norm,none': 0.3125,\n",
       "    'acc_norm_stderr,none': 0.021923384489444957}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.257985257985258,\n",
       "    'acc_stderr,none': 0.012526328490375856}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.5921152388172858,\n",
       "    'exact_match_stderr,strict-match': 0.013536742075643088,\n",
       "    'exact_match,flexible-extract': 0.8059135708870356,\n",
       "    'exact_match_stderr,flexible-extract': 0.010893918308192413}}},\n",
       " 'llama3_70b_a_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.34346959122632104,\n",
       "    'acc_stderr,none': 0.0038651700998585496,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.4716259298618491,\n",
       "    'acc_stderr,none': 0.007083567801902924,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.2857142857142857,\n",
       "    'acc_stderr,none': 0.04040610178208841},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.5757575757575758,\n",
       "    'acc_stderr,none': 0.038592681420702636},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.6911764705882353,\n",
       "    'acc_stderr,none': 0.03242661719827218},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.6413502109704642,\n",
       "    'acc_stderr,none': 0.031219569445301854},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5454545454545454,\n",
       "    'acc_stderr,none': 0.04545454545454548},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.42592592592592593,\n",
       "    'acc_stderr,none': 0.047803436269367894},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.34355828220858897,\n",
       "    'acc_stderr,none': 0.03731133519673892},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.48265895953757226,\n",
       "    'acc_stderr,none': 0.02690290045866664},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.5396648044692738,\n",
       "    'acc_stderr,none': 0.01666979959211203},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3022508038585209,\n",
       "    'acc_stderr,none': 0.026082700695399655},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.2808641975308642,\n",
       "    'acc_stderr,none': 0.025006469755799197},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.4941329856584094,\n",
       "    'acc_stderr,none': 0.012769356925216526},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.19883040935672514,\n",
       "    'acc_stderr,none': 0.030611116557432528},\n",
       "   'mmlu_other': {'acc,none': 0.30222079176054073,\n",
       "    'acc_stderr,none': 0.00822077883607661,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.28,\n",
       "    'acc_stderr,none': 0.045126085985421276},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.2981132075471698,\n",
       "    'acc_stderr,none': 0.028152837942493868},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.28901734104046245,\n",
       "    'acc_stderr,none': 0.034564257450869995},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.17,\n",
       "    'acc_stderr,none': 0.0377525168068637},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.35874439461883406,\n",
       "    'acc_stderr,none': 0.032190792004199956},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.34951456310679613,\n",
       "    'acc_stderr,none': 0.047211885060971716},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.3547008547008547,\n",
       "    'acc_stderr,none': 0.03134250486245402},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.32,\n",
       "    'acc_stderr,none': 0.046882617226215034},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.28607918263090676,\n",
       "    'acc_stderr,none': 0.016160871405127546},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.2908496732026144,\n",
       "    'acc_stderr,none': 0.026004800363952113},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.36879432624113473,\n",
       "    'acc_stderr,none': 0.02878222756134725},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.25735294117647056,\n",
       "    'acc_stderr,none': 0.026556519470041513},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.28313253012048195,\n",
       "    'acc_stderr,none': 0.03507295431370518},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.31231719207019826,\n",
       "    'acc_stderr,none': 0.00820438929619202,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.2982456140350877,\n",
       "    'acc_stderr,none': 0.043036840335373173},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.18181818181818182,\n",
       "    'acc_stderr,none': 0.027479603010538804},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.35233160621761656,\n",
       "    'acc_stderr,none': 0.03447478286414357},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.2128205128205128,\n",
       "    'acc_stderr,none': 0.020752423722128023},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.24369747899159663,\n",
       "    'acc_stderr,none': 0.027886828078380554},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.24036697247706423,\n",
       "    'acc_stderr,none': 0.01832060732096407},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.31297709923664124,\n",
       "    'acc_stderr,none': 0.04066962905677697},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.41830065359477125,\n",
       "    'acc_stderr,none': 0.01995597514583555},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.2636363636363636,\n",
       "    'acc_stderr,none': 0.04220224692971987},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.46530612244897956,\n",
       "    'acc_stderr,none': 0.03193207024425314},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.40298507462686567,\n",
       "    'acc_stderr,none': 0.034683432951111266},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.3,\n",
       "    'acc_stderr,none': 0.046056618647183814},\n",
       "   'mmlu_stem': {'acc,none': 0.22327941642879798,\n",
       "    'acc_stderr,none': 0.0074033885594655575,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.22,\n",
       "    'acc_stderr,none': 0.04163331998932269},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.18518518518518517,\n",
       "    'acc_stderr,none': 0.03355677216313142},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.19736842105263158,\n",
       "    'acc_stderr,none': 0.03238981601699397},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.2569444444444444,\n",
       "    'acc_stderr,none': 0.03653946969442099},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.26,\n",
       "    'acc_stderr,none': 0.044084400227680794},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.21,\n",
       "    'acc_stderr,none': 0.040936018074033256},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.21568627450980393,\n",
       "    'acc_stderr,none': 0.040925639582376556},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.29,\n",
       "    'acc_stderr,none': 0.045604802157206845},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.26382978723404255,\n",
       "    'acc_stderr,none': 0.02880998985410298},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.21379310344827587,\n",
       "    'acc_stderr,none': 0.03416520447747548},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.21164021164021163,\n",
       "    'acc_stderr,none': 0.021037331505262886},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.2032258064516129,\n",
       "    'acc_stderr,none': 0.022891687984554963},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.15270935960591134,\n",
       "    'acc_stderr,none': 0.025308904539380624},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.33,\n",
       "    'acc_stderr,none': 0.04725815626252604},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2052980132450331,\n",
       "    'acc_stderr,none': 0.032979866484738336},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.21296296296296297,\n",
       "    'acc_stderr,none': 0.027920963147993666},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.33035714285714285,\n",
       "    'acc_stderr,none': 0.04464285714285713}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.44779938587512796,\n",
       "    'acc_stderr,none': 0.011252242102001767}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.3549107142857143,\n",
       "    'acc_stderr,none': 0.02263162341632674,\n",
       "    'acc_norm,none': 0.3549107142857143,\n",
       "    'acc_norm_stderr,none': 0.02263162341632674}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 7.186030823799035,\n",
       "    'bleu_max_stderr,none': 0.2976903761490195,\n",
       "    'bleu_acc,none': 0.4920440636474908,\n",
       "    'bleu_acc_stderr,none': 0.017501285074551814,\n",
       "    'bleu_diff,none': -0.19362670717044234,\n",
       "    'bleu_diff_stderr,none': 0.19606698071736023,\n",
       "    'rouge1_max,none': 25.217291877583296,\n",
       "    'rouge1_max_stderr,none': 0.49552527518525,\n",
       "    'rouge1_acc,none': 0.5079559363525091,\n",
       "    'rouge1_acc_stderr,none': 0.01750128507455184,\n",
       "    'rouge1_diff,none': 0.04690274157387491,\n",
       "    'rouge1_diff_stderr,none': 0.3380640359381212,\n",
       "    'rouge2_max,none': 14.019606369528404,\n",
       "    'rouge2_max_stderr,none': 0.4937090058614036,\n",
       "    'rouge2_acc,none': 0.3818849449204406,\n",
       "    'rouge2_acc_stderr,none': 0.01700810193916349,\n",
       "    'rouge2_diff,none': -0.8907716827215294,\n",
       "    'rouge2_diff_stderr,none': 0.3521401893119823,\n",
       "    'rougeL_max,none': 22.059383682551506,\n",
       "    'rougeL_max_stderr,none': 0.48823283034254056,\n",
       "    'rougeL_acc,none': 0.4834761321909425,\n",
       "    'rougeL_acc_stderr,none': 0.017493940190057723,\n",
       "    'rougeL_diff,none': -0.4684831182081133,\n",
       "    'rougeL_diff_stderr,none': 0.33470421497008024},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.4541003671970624,\n",
       "    'acc_stderr,none': 0.01742959309132352},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.5961727803056094,\n",
       "    'acc_stderr,none': 0.016334354466768022}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3132328308207705,\n",
       "    'acc_stderr,none': 0.008490611920810433,\n",
       "    'acc_norm,none': 0.31825795644891125,\n",
       "    'acc_norm_stderr,none': 0.008527078567878583}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7850924918389554,\n",
       "    'acc_stderr,none': 0.009583665082653308,\n",
       "    'acc_norm,none': 0.7747551686615887,\n",
       "    'acc_norm_stderr,none': 0.009746643471032145}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.33482142857142855,\n",
       "    'acc_stderr,none': 0.022321428571428627,\n",
       "    'acc_norm,none': 0.33482142857142855,\n",
       "    'acc_norm_stderr,none': 0.022321428571428627}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.21294021294021295,\n",
       "    'acc_stderr,none': 0.011720679449797579}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.25094768764215314,\n",
       "    'exact_match_stderr,strict-match': 0.011942354768308837,\n",
       "    'exact_match,flexible-extract': 0.8733889310083397,\n",
       "    'exact_match_stderr,flexible-extract': 0.009159715283081092}}},\n",
       " 'llama3_70b_e_high': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.42273180458624127,\n",
       "    'acc_stderr,none': 0.0040785828773805795,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.4499468650371945,\n",
       "    'acc_stderr,none': 0.007157851347957301,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.24603174603174602,\n",
       "    'acc_stderr,none': 0.03852273364924316},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.3090909090909091,\n",
       "    'acc_stderr,none': 0.03608541011573967},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.6127450980392157,\n",
       "    'acc_stderr,none': 0.03418931233833344},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.45147679324894513,\n",
       "    'acc_stderr,none': 0.0323936001739747},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.5702479338842975,\n",
       "    'acc_stderr,none': 0.04519082021319773},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.4444444444444444,\n",
       "    'acc_stderr,none': 0.04803752235190193},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.49693251533742333,\n",
       "    'acc_stderr,none': 0.03928297078179663},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.4682080924855491,\n",
       "    'acc_stderr,none': 0.026864624366756636},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.358659217877095,\n",
       "    'acc_stderr,none': 0.01604045442616448},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.3440514469453376,\n",
       "    'acc_stderr,none': 0.026981478043648022},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.5123456790123457,\n",
       "    'acc_stderr,none': 0.027812262269327242},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5065189048239895,\n",
       "    'acc_stderr,none': 0.012769150688867506},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.42105263157894735,\n",
       "    'acc_stderr,none': 0.03786720706234215},\n",
       "   'mmlu_other': {'acc,none': 0.41873189571934344,\n",
       "    'acc_stderr,none': 0.008732779005580749,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.43,\n",
       "    'acc_stderr,none': 0.049756985195624284},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.3660377358490566,\n",
       "    'acc_stderr,none': 0.029647813539365252},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.3179190751445087,\n",
       "    'acc_stderr,none': 0.0355068398916558},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.19,\n",
       "    'acc_stderr,none': 0.039427724440366234},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.5426008968609866,\n",
       "    'acc_stderr,none': 0.03343577705583065},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.3786407766990291,\n",
       "    'acc_stderr,none': 0.048026946982589726},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.6196581196581197,\n",
       "    'acc_stderr,none': 0.03180425204384099},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.48,\n",
       "    'acc_stderr,none': 0.05021167315686779},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.4112388250319285,\n",
       "    'acc_stderr,none': 0.01759597190805657},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.3627450980392157,\n",
       "    'acc_stderr,none': 0.027530078447110296},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.4219858156028369,\n",
       "    'acc_stderr,none': 0.029462189233370593},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.4522058823529412,\n",
       "    'acc_stderr,none': 0.030233758551596452},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.35542168674698793,\n",
       "    'acc_stderr,none': 0.03726214354322415},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.4809879753006175,\n",
       "    'acc_stderr,none': 0.008781444199240639,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.41228070175438597,\n",
       "    'acc_stderr,none': 0.04630653203366596},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.45454545454545453,\n",
       "    'acc_stderr,none': 0.03547601494006936},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.5803108808290155,\n",
       "    'acc_stderr,none': 0.035615873276858834},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.4256410256410256,\n",
       "    'acc_stderr,none': 0.025069094387296535},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.42857142857142855,\n",
       "    'acc_stderr,none': 0.032145368597886394},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.3376146788990826,\n",
       "    'acc_stderr,none': 0.020275265986638917},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.2595419847328244,\n",
       "    'acc_stderr,none': 0.0384487613978527},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.5800653594771242,\n",
       "    'acc_stderr,none': 0.019966811178256483},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.41818181818181815,\n",
       "    'acc_stderr,none': 0.04724577405731572},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.6489795918367347,\n",
       "    'acc_stderr,none': 0.030555316755573637},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.6268656716417911,\n",
       "    'acc_stderr,none': 0.034198326081760065},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.59,\n",
       "    'acc_stderr,none': 0.04943110704237102},\n",
       "   'mmlu_stem': {'acc,none': 0.3292102759276879,\n",
       "    'acc_stderr,none': 0.008266709646734366,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.31,\n",
       "    'acc_stderr,none': 0.04648231987117316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.37777777777777777,\n",
       "    'acc_stderr,none': 0.04188307537595853},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.2631578947368421,\n",
       "    'acc_stderr,none': 0.03583496176361063},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.4930555555555556,\n",
       "    'acc_stderr,none': 0.04180806750294938},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.042923469599092816},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.04878317312145633},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.24,\n",
       "    'acc_stderr,none': 0.04292346959909282},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.3431372549019608,\n",
       "    'acc_stderr,none': 0.04724007352383888},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.37,\n",
       "    'acc_stderr,none': 0.048523658709391},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.3659574468085106,\n",
       "    'acc_stderr,none': 0.031489558297455304},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.42758620689655175,\n",
       "    'acc_stderr,none': 0.041227371113703316},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.23544973544973544,\n",
       "    'acc_stderr,none': 0.021851509822031715},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.36129032258064514,\n",
       "    'acc_stderr,none': 0.027327548447957525},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.3103448275862069,\n",
       "    'acc_stderr,none': 0.03255086769970103},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.49,\n",
       "    'acc_stderr,none': 0.05024183937956912},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.2111111111111111,\n",
       "    'acc_stderr,none': 0.02488211685765508},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.2781456953642384,\n",
       "    'acc_stderr,none': 0.03658603262763744},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.35185185185185186,\n",
       "    'acc_stderr,none': 0.03256850570293648},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.45535714285714285,\n",
       "    'acc_stderr,none': 0.047268355537191}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.43039918116683723,\n",
       "    'acc_stderr,none': 0.011203917417496392}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 5.452923188332979,\n",
       "    'bleu_max_stderr,none': 0.2159804912757085,\n",
       "    'bleu_acc,none': 0.43818849449204406,\n",
       "    'bleu_acc_stderr,none': 0.01736923616440443,\n",
       "    'bleu_diff,none': -0.4549180061692236,\n",
       "    'bleu_diff_stderr,none': 0.1492367922513079,\n",
       "    'rouge1_max,none': 23.402392943646866,\n",
       "    'rouge1_max_stderr,none': 0.42568992129336086,\n",
       "    'rouge1_acc,none': 0.4614443084455324,\n",
       "    'rouge1_acc_stderr,none': 0.017451384104637455,\n",
       "    'rouge1_diff,none': -0.9678386827689932,\n",
       "    'rouge1_diff_stderr,none': 0.3321736263535224,\n",
       "    'rouge2_max,none': 12.415911330124876,\n",
       "    'rouge2_max_stderr,none': 0.420589134879823,\n",
       "    'rouge2_acc,none': 0.3537331701346389,\n",
       "    'rouge2_acc_stderr,none': 0.016737814358846147,\n",
       "    'rouge2_diff,none': -1.5150713806140053,\n",
       "    'rouge2_diff_stderr,none': 0.335850719845891,\n",
       "    'rougeL_max,none': 20.413250710168896,\n",
       "    'rougeL_max_stderr,none': 0.41958424155133045,\n",
       "    'rougeL_acc,none': 0.42472460220318237,\n",
       "    'rougeL_acc_stderr,none': 0.01730400095716747,\n",
       "    'rougeL_diff,none': -1.2343052167789954,\n",
       "    'rougeL_diff_stderr,none': 0.31283973652408437},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.31334149326805383,\n",
       "    'acc_stderr,none': 0.016238065069059598},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.46042942351359656,\n",
       "    'acc_stderr,none': 0.01653885248336244}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3051926298157454,\n",
       "    'acc_stderr,none': 0.008429849471087473,\n",
       "    'acc_norm,none': 0.3135678391959799,\n",
       "    'acc_norm_stderr,none': 0.008493078900794976}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.764417845484222,\n",
       "    'acc_stderr,none': 0.009901067586473914,\n",
       "    'acc_norm,none': 0.7529923830250272,\n",
       "    'acc_norm_stderr,none': 0.010062268140772615}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3705357142857143,\n",
       "    'acc_stderr,none': 0.0228426677334829,\n",
       "    'acc_norm,none': 0.3705357142857143,\n",
       "    'acc_norm_stderr,none': 0.0228426677334829}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.23177723177723178,\n",
       "    'acc_stderr,none': 0.012080893552302283}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.354814253222138,\n",
       "    'exact_match_stderr,strict-match': 0.013179083387979199,\n",
       "    'exact_match,flexible-extract': 0.8893100833965125,\n",
       "    'exact_match_stderr,flexible-extract': 0.008642172551392473}}},\n",
       " 'llama3_70b_e_low': {'llama3_70b_mmlu': {'mmlu': {'acc,none': 0.7230451502634953,\n",
       "    'acc_stderr,none': 0.0035789783761124203,\n",
       "    'alias': 'mmlu'},\n",
       "   'mmlu_humanities': {'acc,none': 0.675451647183847,\n",
       "    'acc_stderr,none': 0.0066212839024168925,\n",
       "    'alias': ' - humanities'},\n",
       "   'mmlu_formal_logic': {'alias': '  - formal_logic',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.0442626668137991},\n",
       "   'mmlu_high_school_european_history': {'alias': '  - high_school_european_history',\n",
       "    'acc,none': 0.7818181818181819,\n",
       "    'acc_stderr,none': 0.03225078108306289},\n",
       "   'mmlu_high_school_us_history': {'alias': '  - high_school_us_history',\n",
       "    'acc,none': 0.8480392156862745,\n",
       "    'acc_stderr,none': 0.025195658428931803},\n",
       "   'mmlu_high_school_world_history': {'alias': '  - high_school_world_history',\n",
       "    'acc,none': 0.8438818565400844,\n",
       "    'acc_stderr,none': 0.023627159460318677},\n",
       "   'mmlu_international_law': {'alias': '  - international_law',\n",
       "    'acc,none': 0.8842975206611571,\n",
       "    'acc_stderr,none': 0.029199802455622793},\n",
       "   'mmlu_jurisprudence': {'alias': '  - jurisprudence',\n",
       "    'acc,none': 0.8333333333333334,\n",
       "    'acc_stderr,none': 0.036028141763926436},\n",
       "   'mmlu_logical_fallacies': {'alias': '  - logical_fallacies',\n",
       "    'acc,none': 0.7914110429447853,\n",
       "    'acc_stderr,none': 0.03192193448934724},\n",
       "   'mmlu_moral_disputes': {'alias': '  - moral_disputes',\n",
       "    'acc,none': 0.7167630057803468,\n",
       "    'acc_stderr,none': 0.02425790170532339},\n",
       "   'mmlu_moral_scenarios': {'alias': '  - moral_scenarios',\n",
       "    'acc,none': 0.582122905027933,\n",
       "    'acc_stderr,none': 0.016495400635820084},\n",
       "   'mmlu_philosophy': {'alias': '  - philosophy',\n",
       "    'acc,none': 0.7395498392282959,\n",
       "    'acc_stderr,none': 0.024926723224845557},\n",
       "   'mmlu_prehistory': {'alias': '  - prehistory',\n",
       "    'acc,none': 0.8209876543209876,\n",
       "    'acc_stderr,none': 0.021330868762127045},\n",
       "   'mmlu_professional_law': {'alias': '  - professional_law',\n",
       "    'acc,none': 0.5691003911342895,\n",
       "    'acc_stderr,none': 0.012647695889547226},\n",
       "   'mmlu_world_religions': {'alias': '  - world_religions',\n",
       "    'acc,none': 0.8187134502923976,\n",
       "    'acc_stderr,none': 0.029547741687640038},\n",
       "   'mmlu_other': {'acc,none': 0.7775989700675893,\n",
       "    'acc_stderr,none': 0.007092011017829385,\n",
       "    'alias': ' - other'},\n",
       "   'mmlu_business_ethics': {'alias': '  - business_ethics',\n",
       "    'acc,none': 0.73,\n",
       "    'acc_stderr,none': 0.04461960433384739},\n",
       "   'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge',\n",
       "    'acc,none': 0.8037735849056604,\n",
       "    'acc_stderr,none': 0.02444238813110086},\n",
       "   'mmlu_college_medicine': {'alias': '  - college_medicine',\n",
       "    'acc,none': 0.7109826589595376,\n",
       "    'acc_stderr,none': 0.034564257450869995},\n",
       "   'mmlu_global_facts': {'alias': '  - global_facts',\n",
       "    'acc,none': 0.5,\n",
       "    'acc_stderr,none': 0.050251890762960605},\n",
       "   'mmlu_human_aging': {'alias': '  - human_aging',\n",
       "    'acc,none': 0.7085201793721974,\n",
       "    'acc_stderr,none': 0.03050028317654585},\n",
       "   'mmlu_management': {'alias': '  - management',\n",
       "    'acc,none': 0.8737864077669902,\n",
       "    'acc_stderr,none': 0.03288180278808628},\n",
       "   'mmlu_marketing': {'alias': '  - marketing',\n",
       "    'acc,none': 0.8717948717948718,\n",
       "    'acc_stderr,none': 0.021901905115073318},\n",
       "   'mmlu_medical_genetics': {'alias': '  - medical_genetics',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036622},\n",
       "   'mmlu_miscellaneous': {'alias': '  - miscellaneous',\n",
       "    'acc,none': 0.8850574712643678,\n",
       "    'acc_stderr,none': 0.01140572072459397},\n",
       "   'mmlu_nutrition': {'alias': '  - nutrition',\n",
       "    'acc,none': 0.8464052287581699,\n",
       "    'acc_stderr,none': 0.02064559791041876},\n",
       "   'mmlu_professional_accounting': {'alias': '  - professional_accounting',\n",
       "    'acc,none': 0.5709219858156028,\n",
       "    'acc_stderr,none': 0.029525914302558562},\n",
       "   'mmlu_professional_medicine': {'alias': '  - professional_medicine',\n",
       "    'acc,none': 0.8602941176470589,\n",
       "    'acc_stderr,none': 0.021059408919012507},\n",
       "   'mmlu_virology': {'alias': '  - virology',\n",
       "    'acc,none': 0.463855421686747,\n",
       "    'acc_stderr,none': 0.03882310850890594},\n",
       "   'mmlu_social_sciences': {'acc,none': 0.8280792980175495,\n",
       "    'acc_stderr,none': 0.006677443504363067,\n",
       "    'alias': ' - social sciences'},\n",
       "   'mmlu_econometrics': {'alias': '  - econometrics',\n",
       "    'acc,none': 0.6140350877192983,\n",
       "    'acc_stderr,none': 0.04579639422070434},\n",
       "   'mmlu_high_school_geography': {'alias': '  - high_school_geography',\n",
       "    'acc,none': 0.8686868686868687,\n",
       "    'acc_stderr,none': 0.024063156416822527},\n",
       "   'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics',\n",
       "    'acc,none': 0.9430051813471503,\n",
       "    'acc_stderr,none': 0.016731085293607558},\n",
       "   'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics',\n",
       "    'acc,none': 0.7923076923076923,\n",
       "    'acc_stderr,none': 0.020567539567246794},\n",
       "   'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics',\n",
       "    'acc,none': 0.8361344537815126,\n",
       "    'acc_stderr,none': 0.024044054940440488},\n",
       "   'mmlu_high_school_psychology': {'alias': '  - high_school_psychology',\n",
       "    'acc,none': 0.9211009174311927,\n",
       "    'acc_stderr,none': 0.011558198113769572},\n",
       "   'mmlu_human_sexuality': {'alias': '  - human_sexuality',\n",
       "    'acc,none': 0.7938931297709924,\n",
       "    'acc_stderr,none': 0.03547771004159464},\n",
       "   'mmlu_professional_psychology': {'alias': '  - professional_psychology',\n",
       "    'acc,none': 0.7924836601307189,\n",
       "    'acc_stderr,none': 0.016405924270103234},\n",
       "   'mmlu_public_relations': {'alias': '  - public_relations',\n",
       "    'acc,none': 0.7272727272727273,\n",
       "    'acc_stderr,none': 0.04265792110940588},\n",
       "   'mmlu_security_studies': {'alias': '  - security_studies',\n",
       "    'acc,none': 0.746938775510204,\n",
       "    'acc_stderr,none': 0.02783302387139969},\n",
       "   'mmlu_sociology': {'alias': '  - sociology',\n",
       "    'acc,none': 0.8656716417910447,\n",
       "    'acc_stderr,none': 0.024112678240900826},\n",
       "   'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy',\n",
       "    'acc,none': 0.88,\n",
       "    'acc_stderr,none': 0.03265986323710906},\n",
       "   'mmlu_stem': {'acc,none': 0.6378052648271487,\n",
       "    'acc_stderr,none': 0.008070187226181986,\n",
       "    'alias': ' - stem'},\n",
       "   'mmlu_abstract_algebra': {'alias': '  - abstract_algebra',\n",
       "    'acc,none': 0.38,\n",
       "    'acc_stderr,none': 0.048783173121456316},\n",
       "   'mmlu_anatomy': {'alias': '  - anatomy',\n",
       "    'acc,none': 0.6962962962962963,\n",
       "    'acc_stderr,none': 0.03972552884785136},\n",
       "   'mmlu_astronomy': {'alias': '  - astronomy',\n",
       "    'acc,none': 0.875,\n",
       "    'acc_stderr,none': 0.026913523521537846},\n",
       "   'mmlu_college_biology': {'alias': '  - college_biology',\n",
       "    'acc,none': 0.9027777777777778,\n",
       "    'acc_stderr,none': 0.024774516250440165},\n",
       "   'mmlu_college_chemistry': {'alias': '  - college_chemistry',\n",
       "    'acc,none': 0.51,\n",
       "    'acc_stderr,none': 0.05024183937956911},\n",
       "   'mmlu_college_computer_science': {'alias': '  - college_computer_science',\n",
       "    'acc,none': 0.64,\n",
       "    'acc_stderr,none': 0.04824181513244218},\n",
       "   'mmlu_college_mathematics': {'alias': '  - college_mathematics',\n",
       "    'acc,none': 0.4,\n",
       "    'acc_stderr,none': 0.049236596391733084},\n",
       "   'mmlu_college_physics': {'alias': '  - college_physics',\n",
       "    'acc,none': 0.5294117647058824,\n",
       "    'acc_stderr,none': 0.04966570903978529},\n",
       "   'mmlu_computer_security': {'alias': '  - computer_security',\n",
       "    'acc,none': 0.81,\n",
       "    'acc_stderr,none': 0.03942772444036623},\n",
       "   'mmlu_conceptual_physics': {'alias': '  - conceptual_physics',\n",
       "    'acc,none': 0.6638297872340425,\n",
       "    'acc_stderr,none': 0.030881618520676942},\n",
       "   'mmlu_electrical_engineering': {'alias': '  - electrical_engineering',\n",
       "    'acc,none': 0.6620689655172414,\n",
       "    'acc_stderr,none': 0.039417076320648906},\n",
       "   'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics',\n",
       "    'acc,none': 0.5714285714285714,\n",
       "    'acc_stderr,none': 0.025487187147859372},\n",
       "   'mmlu_high_school_biology': {'alias': '  - high_school_biology',\n",
       "    'acc,none': 0.864516129032258,\n",
       "    'acc_stderr,none': 0.019469334586486933},\n",
       "   'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry',\n",
       "    'acc,none': 0.6206896551724138,\n",
       "    'acc_stderr,none': 0.03413963805906235},\n",
       "   'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science',\n",
       "    'acc,none': 0.83,\n",
       "    'acc_stderr,none': 0.0377525168068637},\n",
       "   'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics',\n",
       "    'acc,none': 0.35555555555555557,\n",
       "    'acc_stderr,none': 0.0291857149498574},\n",
       "   'mmlu_high_school_physics': {'alias': '  - high_school_physics',\n",
       "    'acc,none': 0.5033112582781457,\n",
       "    'acc_stderr,none': 0.04082393379449654},\n",
       "   'mmlu_high_school_statistics': {'alias': '  - high_school_statistics',\n",
       "    'acc,none': 0.6898148148148148,\n",
       "    'acc_stderr,none': 0.031546962856566274},\n",
       "   'mmlu_machine_learning': {'alias': '  - machine_learning',\n",
       "    'acc,none': 0.5357142857142857,\n",
       "    'acc_stderr,none': 0.04733667890053756}},\n",
       "  'llama3_70b_social_iqa': {'social_iqa': {'alias': 'social_iqa',\n",
       "    'acc,none': 0.43551688843398156,\n",
       "    'acc_stderr,none': 0.011219586604022594}},\n",
       "  'llama3_70b_gpqa_main_zeroshot': {'gpqa_main_zeroshot': {'alias': 'gpqa_main_zeroshot',\n",
       "    'acc,none': 0.359375,\n",
       "    'acc_stderr,none': 0.022694577961439925,\n",
       "    'acc_norm,none': 0.359375,\n",
       "    'acc_norm_stderr,none': 0.022694577961439925}},\n",
       "  'llama3_70b_truthfulqa': {'truthfulqa_gen': {'alias': 'truthfulqa_gen',\n",
       "    'bleu_max,none': 9.642204091215383,\n",
       "    'bleu_max_stderr,none': 0.49477552637508115,\n",
       "    'bleu_acc,none': 0.620563035495716,\n",
       "    'bleu_acc_stderr,none': 0.016987039266142975,\n",
       "    'bleu_diff,none': 0.9442922543967592,\n",
       "    'bleu_diff_stderr,none': 0.37346343593247894,\n",
       "    'rouge1_max,none': 30.00376835144213,\n",
       "    'rouge1_max_stderr,none': 0.7130670800232768,\n",
       "    'rouge1_acc,none': 0.7099143206854345,\n",
       "    'rouge1_acc_stderr,none': 0.01588623687420952,\n",
       "    'rouge1_diff,none': 7.145845516407771,\n",
       "    'rouge1_diff_stderr,none': 0.6366063431881058,\n",
       "    'rouge2_max,none': 12.272590154011654,\n",
       "    'rouge2_max_stderr,none': 0.7119786598474964,\n",
       "    'rouge2_acc,none': 0.2533659730722154,\n",
       "    'rouge2_acc_stderr,none': 0.015225899340826856,\n",
       "    'rouge2_diff,none': 0.8007321131422266,\n",
       "    'rouge2_diff_stderr,none': 0.587801771714321,\n",
       "    'rougeL_max,none': 28.068598540985676,\n",
       "    'rougeL_max_stderr,none': 0.6966719090640683,\n",
       "    'rougeL_acc,none': 0.7001223990208079,\n",
       "    'rougeL_acc_stderr,none': 0.016040352966713616,\n",
       "    'rougeL_diff,none': 6.7481424410537025,\n",
       "    'rougeL_diff_stderr,none': 0.6327256634759059},\n",
       "   'truthfulqa_mc1': {'alias': 'truthfulqa_mc1',\n",
       "    'acc,none': 0.48959608323133413,\n",
       "    'acc_stderr,none': 0.017499711430249264},\n",
       "   'truthfulqa_mc2': {'alias': 'truthfulqa_mc2',\n",
       "    'acc,none': 0.6528340114952984,\n",
       "    'acc_stderr,none': 0.01595351060013572}},\n",
       "  'llama3_70b_mathqa': {'mathqa': {'alias': 'mathqa',\n",
       "    'acc,none': 0.3504187604690117,\n",
       "    'acc_stderr,none': 0.008733956045067806,\n",
       "    'acc_norm,none': 0.35711892797319933,\n",
       "    'acc_norm_stderr,none': 0.008771469242554543}},\n",
       "  'llama3_70b_piqa': {'piqa': {'alias': 'piqa',\n",
       "    'acc,none': 0.7976060935799782,\n",
       "    'acc_stderr,none': 0.00937428968280767,\n",
       "    'acc_norm,none': 0.7997823721436343,\n",
       "    'acc_norm_stderr,none': 0.009336465387350825}},\n",
       "  'llama3_70b_gpqa_main_n_shot': {'gpqa_main_n_shot': {'alias': 'gpqa_main_n_shot',\n",
       "    'acc,none': 0.3549107142857143,\n",
       "    'acc_stderr,none': 0.022631623416326744,\n",
       "    'acc_norm,none': 0.3549107142857143,\n",
       "    'acc_norm_stderr,none': 0.022631623416326744}},\n",
       "  'llama3_70b_commonsense_qa': {'commonsense_qa': {'alias': 'commonsense_qa',\n",
       "    'acc,none': 0.7084357084357085,\n",
       "    'acc_stderr,none': 0.013011802821401595}},\n",
       "  'llama3_70b_gsm8k_5_shots_without_cot': {'gsm8k': {'alias': 'gsm8k',\n",
       "    'exact_match,strict-match': 0.8438210765731615,\n",
       "    'exact_match_stderr,strict-match': 0.009999509369757466,\n",
       "    'exact_match,flexible-extract': 0.9044730856709629,\n",
       "    'exact_match_stderr,flexible-extract': 0.00809660577115574}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/wenkail/llm_personality/evaluation/lm-evaluation-harness/results/dpo/llama3_70b_results_vllm/llama3_70b_dpo_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(model_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
