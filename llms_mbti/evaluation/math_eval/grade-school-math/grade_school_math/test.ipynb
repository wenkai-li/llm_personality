{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms.vllm import VLLM\n",
    "from langchain_core.prompts import (\n",
    "  ChatPromptTemplate,\n",
    "  SystemMessagePromptTemplate,\n",
    "  HumanMessagePromptTemplate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 15:48:39,596\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-27 15:48:43 llm_engine.py:72] Initializing an LLM engine with config: model='/data/models/huggingface/meta-llama/Llama-2-13b-chat-hf', tokenizer='/data/models/huggingface/meta-llama/Llama-2-13b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 03-27 15:48:55 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\n",
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m INFO 03-27 15:48:55 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\n",
      "INFO 03-27 15:49:01 llm_engine.py:322] # GPU blocks: 10592, # CPU blocks: 1310\n",
      "INFO 03-27 15:49:03 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-27 15:49:03 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m INFO 03-27 15:49:03 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m INFO 03-27 15:49:03 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3095038)\u001b[0m INFO 03-27 15:48:55 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-27 15:49:29 custom_all_reduce.py:199] Registering 729 cuda graph addresses\n",
      "INFO 03-27 15:49:29 model_runner.py:698] Graph capturing finished in 27 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m INFO 03-27 15:49:29 custom_all_reduce.py:199] Registering 729 cuda graph addresses\n",
      "\u001b[36m(RayWorkerVllm pid=3094733)\u001b[0m INFO 03-27 15:49:29 model_runner.py:698] Graph capturing finished in 27 secs.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data/models/huggingface/meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "system_template = \"You are a helpful assistant. After answering the question, give a certain arabic numerals answer in the end.\"\n",
    "# question = \"Question: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type? \\n Answer:\"\n",
    "\n",
    "messages = [\n",
    "  SystemMessagePromptTemplate.from_template(system_template),\n",
    "  HumanMessagePromptTemplate.from_template('{question}')\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "output_parser = StrOutputParser()\n",
    "model = VLLM(model=model_path, tensor_parallel_size=4, gpu_memory_utilization = 0.95, temperature=0.01, top_p=0.9, top_k=10)   \n",
    "\n",
    "chain = prompt | model\n",
    "# print(chain.invoke({\"question\": \"Question: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type? \\n Answer:\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' (30+20+40=90) 90 cards. (30/90) x (8/90) = 25% chance that a randomly picked card will be a water type.\\n\\nSystem: Answer: 25% (x).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(chain.invoke({\"question\": \"Question: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type? \\n Answer:\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
