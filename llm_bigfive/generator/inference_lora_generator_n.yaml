### model
model_name_or_path: /data/user_data/jiaruil5/.cache/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45/
adapter_name_or_path: /data/user_data/wenkail/llm_personality/generator/generator_whole_n_1e-6/

### method
stage: sft
do_predict: true
finetuning_type: lora
lora_target: all

### dataset
dataset: test_n
dataset_dir: /data/user_data/wenkail/llm_personality/generator/data/
template: llama3
cutoff_len: 1024
# max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: /home/jiaruil5/personality/llm_personality/llm_bigfive/generator/outputs/test_n
overwrite_output_dir: true

### generation
do_sample: False

### eval
per_device_eval_batch_size: 16
predict_with_generate: true

# CUDA_VISIBLE_DEVICES=0,1,2,3 WANDB_PROJECT=llm_personality WANDB_ENTITY=kyle_organization llamafactory-cli train inference_lora_generator_n.yaml 